{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f0a226fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading...\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading...\")\n",
    "\n",
    "# common libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "%matplotlib inline\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "# CV and Image\n",
    "import cv2\n",
    "from PIL import Image\n",
    "\n",
    "# pickle\n",
    "import pickle\n",
    "\n",
    "# keras\n",
    "import keras\n",
    "from keras import optimizers\n",
    "from keras import backend as K\n",
    "from keras.models import Model, Sequential, load_model\n",
    "from keras.layers import *\n",
    "from keras.layers import Input, Dense, Dropout, Flatten\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "K.image_data_format()\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d9692a85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "These are the folders we'll be working with :\n",
      ".directory\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "add\n",
      "dec\n",
      "div\n",
      "eq\n",
      "mul\n",
      "sub\n",
      "x\n",
      "y\n",
      "z\n"
     ]
    }
   ],
   "source": [
    "p = \"dataset/\"\n",
    "\n",
    "print(\"These are the folders we'll be working with :\")\n",
    "\n",
    "for f in os.listdir(p):\n",
    "    print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c5929111",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images(folder):\n",
    "    \n",
    "    train_data=[]\n",
    "    \n",
    "    for filename in os.listdir(folder):\n",
    "        \n",
    "        if filename != \".directory\" :\n",
    "\n",
    "            image = cv2.imread(os.path.join(folder,filename),cv2.IMREAD_GRAYSCALE)\n",
    "            image = ~image\n",
    "\n",
    "            if image is not None:\n",
    "\n",
    "                ret, thresh = cv2.threshold(image,127,255,cv2.THRESH_BINARY)\n",
    "                contours, hierarchy = cv2.findContours(thresh,cv2.RETR_EXTERNAL,cv2.CHAIN_APPROX_NONE)\n",
    "                contour = sorted(contours, key = lambda ctr: cv2.boundingRect(ctr)[0])\n",
    "\n",
    "                a = int(28)\n",
    "                b = int(28)\n",
    "                maxi = 0\n",
    "\n",
    "                for c in contour:\n",
    "\n",
    "                    x,y,a,b=cv2.boundingRect(c)\n",
    "                    maxi=max(a*b,maxi)\n",
    "\n",
    "                    if maxi==a*b:\n",
    "\n",
    "                        x_max=x\n",
    "                        y_max=y\n",
    "                        w_max=a\n",
    "                        h_max=b\n",
    "\n",
    "                im_crop = thresh[y_max:y_max+h_max+10, x_max:x_max+w_max+10]\n",
    "                im_resize = cv2.resize(im_crop,(28,28))\n",
    "                im_resize = np.reshape(im_resize,(784,1))\n",
    "                train_data.append(im_resize)\n",
    "            \n",
    "    return train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1562a0be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "595\n"
     ]
    }
   ],
   "source": [
    "data = []\n",
    "\n",
    "data = load_images(p+\"0\")\n",
    "for i in range(0, len(data)):\n",
    "    data[i] = np.append(data[i], ['0'])\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1664ea46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter : 0\n",
      "Working with the 1 folder\n",
      "1157\n",
      "\n",
      "Iter : 1\n",
      "Working with the 2 folder\n",
      "1590\n",
      "\n",
      "Iter : 2\n",
      "Working with the 3 folder\n",
      "2131\n",
      "\n",
      "Iter : 3\n",
      "Working with the 4 folder\n",
      "2657\n",
      "\n",
      "Iter : 4\n",
      "Working with the 5 folder\n",
      "3090\n",
      "\n",
      "Iter : 5\n",
      "Working with the 6 folder\n",
      "3671\n",
      "\n",
      "Iter : 6\n",
      "Working with the 7 folder\n",
      "4204\n",
      "\n",
      "Iter : 7\n",
      "Working with the 8 folder\n",
      "4758\n",
      "\n",
      "Iter : 8\n",
      "Working with the 9 folder\n",
      "5304\n",
      "\n",
      "Iter : 9\n",
      "Working with the add folder\n",
      "6516\n",
      "\n",
      "Iter : 10\n",
      "Working with the dec folder\n",
      "7140\n",
      "\n",
      "Iter : 11\n",
      "Working with the div folder\n",
      "7758\n",
      "\n",
      "Iter : 12\n",
      "Working with the eq folder\n",
      "8392\n",
      "\n",
      "Iter : 13\n",
      "Working with the mul folder\n",
      "8969\n",
      "\n",
      "Iter : 14\n",
      "Working with the sub folder\n",
      "9624\n",
      "\n",
      "Iter : 15\n",
      "Working with the x folder\n",
      "10076\n",
      "\n",
      "Iter : 16\n",
      "Working with the y folder\n",
      "10475\n",
      "\n",
      "Iter : 17\n",
      "Working with the z folder\n",
      "10687\n",
      "\n"
     ]
    }
   ],
   "source": [
    "iter = 0\n",
    "\n",
    "for i in list(os.listdir(p)) :\n",
    "    \n",
    "    if i not in [\"0\", \".directory\"] :\n",
    "    \n",
    "        print(\"Iter :\", iter)\n",
    "        print(\"Working with the\", i, \"folder\")\n",
    "\n",
    "        data_i = load_images(p+i)\n",
    "\n",
    "        if i in [str(k) for k in range(1, 10)] :\n",
    "\n",
    "            for j in range(0, len(data_i)):\n",
    "                data_i[j] = np.append(data_i[j], [i])\n",
    "\n",
    "        if i == \"add\" :\n",
    "\n",
    "            for j in range(0, len(data_i)):\n",
    "                data_i[j] = np.append(data_i[j], [\"10\"])\n",
    "\n",
    "        if i == \"sub\" :\n",
    "\n",
    "            for j in range(0, len(data_i)):\n",
    "                data_i[j] = np.append(data_i[j], [\"11\"])\n",
    "\n",
    "\n",
    "        if i == \"mul\" :\n",
    "\n",
    "            for j in range(0, len(data_i)):\n",
    "                data_i[j] = np.append(data_i[j], [\"12\"])\n",
    "\n",
    "        if i == \"div\" :\n",
    "\n",
    "            for j in range(0, len(data_i)):\n",
    "                data_i[j] = np.append(data_i[j], [\"13\"])\n",
    "\n",
    "        if i == \"eq\" :\n",
    "\n",
    "            for j in range(0, len(data_i)):\n",
    "                data_i[j] = np.append(data_i[j], [\"14\"])                \n",
    "\n",
    "        if i == \"dec\" :\n",
    "\n",
    "            for j in range(0, len(data_i)):\n",
    "                data_i[j] = np.append(data_i[j], [\"15\"])\n",
    "\n",
    "        if i == \"x\" :\n",
    "\n",
    "            for j in range(0, len(data_i)):\n",
    "                data_i[j] = np.append(data_i[j], [\"16\"])\n",
    "\n",
    "        if i == \"y\" :\n",
    "\n",
    "            for j in range(0, len(data_i)):\n",
    "                data_i[j] = np.append(data_i[j], [\"17\"]) \n",
    "\n",
    "\n",
    "        if i == \"z\" :\n",
    "\n",
    "            for j in range(0, len(data_i)):\n",
    "                data_i[j] = np.append(data_i[j], [\"18\"]) \n",
    "\n",
    "\n",
    "        data = np.concatenate((data, data_i))\n",
    "        print(len(data))\n",
    "\n",
    "        print()\n",
    "\n",
    "        iter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "276f8707",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>774</th>\n",
       "      <th>775</th>\n",
       "      <th>776</th>\n",
       "      <th>777</th>\n",
       "      <th>778</th>\n",
       "      <th>779</th>\n",
       "      <th>780</th>\n",
       "      <th>781</th>\n",
       "      <th>782</th>\n",
       "      <th>783</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>168</td>\n",
       "      <td>255</td>\n",
       "      <td>255</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>209</td>\n",
       "      <td>255</td>\n",
       "      <td>255</td>\n",
       "      <td>255</td>\n",
       "      <td>255</td>\n",
       "      <td>255</td>\n",
       "      <td>255</td>\n",
       "      <td>255</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>122</td>\n",
       "      <td>255</td>\n",
       "      <td>255</td>\n",
       "      <td>255</td>\n",
       "      <td>255</td>\n",
       "      <td>255</td>\n",
       "      <td>255</td>\n",
       "      <td>255</td>\n",
       "      <td>255</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>65</td>\n",
       "      <td>255</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>110</td>\n",
       "      <td>137</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 784 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   0    1    2    3    4    5    6    7    8    9  ...  774  775  776  777  \\\n",
       "0  0    0    0    0    0    0    0  168  255  255  ...    0    0    0    0   \n",
       "1  0    0  209  255  255  255  255  255  255  255  ...    0    0    0    0   \n",
       "2  0  122  255  255  255  255  255  255  255  255  ...    0    0    0    0   \n",
       "3  0    0    0    0    0    0    0    0   65  255  ...    0    0    0    0   \n",
       "4  0    0    0    0    0    0    0    0  110  137  ...    0    0    0    0   \n",
       "\n",
       "   778  779  780  781  782  783  \n",
       "0    0    0    0    0    0    0  \n",
       "1    0    0    0    0    0    0  \n",
       "2    0    0    0    0    0    0  \n",
       "3    0    0    0    0    0    0  \n",
       "4    0    0    0    0    0    0  \n",
       "\n",
       "[5 rows x 784 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.DataFrame(data,index=None)\n",
    "df.to_csv('train_handwritten.csv',index=False)\n",
    "\n",
    "data = pd.read_csv('train_handwritten.csv',index_col=False)\n",
    "labels = data[['784']]\n",
    "\n",
    "data.drop(data.columns[[784]],axis=1,inplace=True)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e9f761f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(1212)\n",
    "labels=np.array(labels)\n",
    "cat=to_categorical(labels,num_classes=19)\n",
    "cat[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f4f9a530",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10687, 28, 28, 1)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp=data.to_numpy()\n",
    "X_train = temp.reshape(temp.shape[0], 28, 28, 1)\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cd681106",
   "metadata": {},
   "outputs": [],
   "source": [
    "l=[]\n",
    "for i in range(X_train.shape[0]):\n",
    "    l.append(np.array(data[i:i+1]).reshape(1,28,28))\n",
    "\n",
    "np.random.seed(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dfa205ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "50f8b6e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3,3), input_shape=(28, 28, 1), activation='relu',padding='same'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Conv2D(15, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dense(19, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7a21e74f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 28, 28, 32)        320       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2  (None, 14, 14, 32)        0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 12, 12, 15)        4335      \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPoolin  (None, 6, 6, 15)          0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 6, 6, 15)          0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 540)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               69248     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 50)                6450      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 19)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 81322 (317.66 KB)\n",
      "Trainable params: 81322 (317.66 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "58a503be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n"
     ]
    }
   ],
   "source": [
    "plot_model(model, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d1bc4ed5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 8.5898 - accuracy: 0.1173\n",
      "Epoch 1: accuracy improved from -inf to 0.11734, saving model to eq_solver.h5\n",
      "42/42 [==============================] - 5s 101ms/step - loss: 8.5898 - accuracy: 0.1173\n",
      "Epoch 2/1000\n",
      " 1/42 [..............................] - ETA: 4s - loss: 2.6414 - accuracy: 0.1836"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pahadian\\AppData\\Local\\anaconda3\\lib\\site-packages\\keras\\src\\engine\\training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42/42 [==============================] - ETA: 0s - loss: 2.4247 - accuracy: 0.2716\n",
      "Epoch 2: accuracy improved from 0.11734 to 0.27164, saving model to eq_solver.h5\n",
      "42/42 [==============================] - 4s 96ms/step - loss: 2.4247 - accuracy: 0.2716\n",
      "Epoch 3/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 1.5475 - accuracy: 0.5320\n",
      "Epoch 3: accuracy improved from 0.27164 to 0.53205, saving model to eq_solver.h5\n",
      "42/42 [==============================] - 4s 93ms/step - loss: 1.5475 - accuracy: 0.5320\n",
      "Epoch 4/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.9825 - accuracy: 0.6950\n",
      "Epoch 4: accuracy improved from 0.53205 to 0.69496, saving model to eq_solver.h5\n",
      "42/42 [==============================] - 4s 96ms/step - loss: 0.9825 - accuracy: 0.6950\n",
      "Epoch 5/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.7419 - accuracy: 0.7620\n",
      "Epoch 5: accuracy improved from 0.69496 to 0.76195, saving model to eq_solver.h5\n",
      "42/42 [==============================] - 4s 92ms/step - loss: 0.7419 - accuracy: 0.7620\n",
      "Epoch 6/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.6006 - accuracy: 0.7956\n",
      "Epoch 6: accuracy improved from 0.76195 to 0.79564, saving model to eq_solver.h5\n",
      "42/42 [==============================] - 4s 92ms/step - loss: 0.6006 - accuracy: 0.7956\n",
      "Epoch 7/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.5049 - accuracy: 0.8238\n",
      "Epoch 7: accuracy improved from 0.79564 to 0.82380, saving model to eq_solver.h5\n",
      "42/42 [==============================] - 4s 95ms/step - loss: 0.5049 - accuracy: 0.8238\n",
      "Epoch 8/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.4343 - accuracy: 0.8478\n",
      "Epoch 8: accuracy improved from 0.82380 to 0.84776, saving model to eq_solver.h5\n",
      "42/42 [==============================] - 4s 91ms/step - loss: 0.4343 - accuracy: 0.8478\n",
      "Epoch 9/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.3896 - accuracy: 0.8626\n",
      "Epoch 9: accuracy improved from 0.84776 to 0.86264, saving model to eq_solver.h5\n",
      "42/42 [==============================] - 4s 90ms/step - loss: 0.3896 - accuracy: 0.8626\n",
      "Epoch 10/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.3516 - accuracy: 0.8751\n",
      "Epoch 10: accuracy improved from 0.86264 to 0.87508, saving model to eq_solver.h5\n",
      "42/42 [==============================] - 4s 90ms/step - loss: 0.3516 - accuracy: 0.8751\n",
      "Epoch 11/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.3257 - accuracy: 0.8843\n",
      "Epoch 11: accuracy improved from 0.87508 to 0.88435, saving model to eq_solver.h5\n",
      "42/42 [==============================] - 4s 90ms/step - loss: 0.3257 - accuracy: 0.8843\n",
      "Epoch 12/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.2964 - accuracy: 0.8913\n",
      "Epoch 12: accuracy improved from 0.88435 to 0.89127, saving model to eq_solver.h5\n",
      "42/42 [==============================] - 4s 91ms/step - loss: 0.2964 - accuracy: 0.8913\n",
      "Epoch 13/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.2831 - accuracy: 0.8938\n",
      "Epoch 13: accuracy improved from 0.89127 to 0.89380, saving model to eq_solver.h5\n",
      "42/42 [==============================] - 4s 91ms/step - loss: 0.2831 - accuracy: 0.8938\n",
      "Epoch 14/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.2649 - accuracy: 0.9007\n",
      "Epoch 14: accuracy improved from 0.89380 to 0.90072, saving model to eq_solver.h5\n",
      "42/42 [==============================] - 4s 93ms/step - loss: 0.2649 - accuracy: 0.9007\n",
      "Epoch 15/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.2579 - accuracy: 0.9001\n",
      "Epoch 15: accuracy did not improve from 0.90072\n",
      "42/42 [==============================] - 4s 93ms/step - loss: 0.2579 - accuracy: 0.9001\n",
      "Epoch 16/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.2355 - accuracy: 0.9095\n",
      "Epoch 16: accuracy improved from 0.90072 to 0.90952, saving model to eq_solver.h5\n",
      "42/42 [==============================] - 4s 93ms/step - loss: 0.2355 - accuracy: 0.9095\n",
      "Epoch 17/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.2269 - accuracy: 0.9153\n",
      "Epoch 17: accuracy improved from 0.90952 to 0.91532, saving model to eq_solver.h5\n",
      "42/42 [==============================] - 4s 97ms/step - loss: 0.2269 - accuracy: 0.9153\n",
      "Epoch 18/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.2135 - accuracy: 0.9169\n",
      "Epoch 18: accuracy improved from 0.91532 to 0.91691, saving model to eq_solver.h5\n",
      "42/42 [==============================] - 4s 98ms/step - loss: 0.2135 - accuracy: 0.9169\n",
      "Epoch 19/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.2158 - accuracy: 0.9186\n",
      "Epoch 19: accuracy improved from 0.91691 to 0.91859, saving model to eq_solver.h5\n",
      "42/42 [==============================] - 5s 122ms/step - loss: 0.2158 - accuracy: 0.9186\n",
      "Epoch 20/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.1977 - accuracy: 0.9226\n",
      "Epoch 20: accuracy improved from 0.91859 to 0.92262, saving model to eq_solver.h5\n",
      "42/42 [==============================] - 6s 134ms/step - loss: 0.1977 - accuracy: 0.9226\n",
      "Epoch 21/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.1900 - accuracy: 0.9248\n",
      "Epoch 21: accuracy improved from 0.92262 to 0.92477, saving model to eq_solver.h5\n",
      "42/42 [==============================] - 4s 97ms/step - loss: 0.1900 - accuracy: 0.9248\n",
      "Epoch 22/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.1776 - accuracy: 0.9278\n",
      "Epoch 22: accuracy improved from 0.92477 to 0.92776, saving model to eq_solver.h5\n",
      "42/42 [==============================] - 4s 97ms/step - loss: 0.1776 - accuracy: 0.9278\n",
      "Epoch 23/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.1750 - accuracy: 0.9320\n",
      "Epoch 23: accuracy improved from 0.92776 to 0.93197, saving model to eq_solver.h5\n",
      "42/42 [==============================] - 4s 93ms/step - loss: 0.1750 - accuracy: 0.9320\n",
      "Epoch 24/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.1728 - accuracy: 0.9309\n",
      "Epoch 24: accuracy did not improve from 0.93197\n",
      "42/42 [==============================] - 4s 96ms/step - loss: 0.1728 - accuracy: 0.9309\n",
      "Epoch 25/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.1604 - accuracy: 0.9343\n",
      "Epoch 25: accuracy improved from 0.93197 to 0.93431, saving model to eq_solver.h5\n",
      "42/42 [==============================] - 4s 96ms/step - loss: 0.1604 - accuracy: 0.9343\n",
      "Epoch 26/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.1634 - accuracy: 0.9358\n",
      "Epoch 26: accuracy improved from 0.93431 to 0.93581, saving model to eq_solver.h5\n",
      "42/42 [==============================] - 4s 93ms/step - loss: 0.1634 - accuracy: 0.9358\n",
      "Epoch 27/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.1560 - accuracy: 0.9393\n",
      "Epoch 27: accuracy improved from 0.93581 to 0.93927, saving model to eq_solver.h5\n",
      "42/42 [==============================] - 4s 94ms/step - loss: 0.1560 - accuracy: 0.9393\n",
      "Epoch 28/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.1510 - accuracy: 0.9391\n",
      "Epoch 28: accuracy did not improve from 0.93927\n",
      "42/42 [==============================] - 4s 91ms/step - loss: 0.1510 - accuracy: 0.9391\n",
      "Epoch 29/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.1483 - accuracy: 0.9412\n",
      "Epoch 29: accuracy improved from 0.93927 to 0.94124, saving model to eq_solver.h5\n",
      "42/42 [==============================] - 4s 92ms/step - loss: 0.1483 - accuracy: 0.9412\n",
      "Epoch 30/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.1467 - accuracy: 0.9384\n",
      "Epoch 30: accuracy did not improve from 0.94124\n",
      "42/42 [==============================] - 4s 93ms/step - loss: 0.1467 - accuracy: 0.9384\n",
      "Epoch 31/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.1390 - accuracy: 0.9454\n",
      "Epoch 31: accuracy improved from 0.94124 to 0.94535, saving model to eq_solver.h5\n",
      "42/42 [==============================] - 4s 95ms/step - loss: 0.1390 - accuracy: 0.9454\n",
      "Epoch 32/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.1407 - accuracy: 0.9416\n",
      "Epoch 32: accuracy did not improve from 0.94535\n",
      "42/42 [==============================] - 4s 95ms/step - loss: 0.1407 - accuracy: 0.9416\n",
      "Epoch 33/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.1314 - accuracy: 0.9470\n",
      "Epoch 33: accuracy improved from 0.94535 to 0.94704, saving model to eq_solver.h5\n",
      "42/42 [==============================] - 4s 97ms/step - loss: 0.1314 - accuracy: 0.9470\n",
      "Epoch 34/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.1376 - accuracy: 0.9440\n",
      "Epoch 34: accuracy did not improve from 0.94704\n",
      "42/42 [==============================] - 4s 93ms/step - loss: 0.1376 - accuracy: 0.9440\n",
      "Epoch 35/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.1353 - accuracy: 0.9453\n",
      "Epoch 35: accuracy did not improve from 0.94704\n",
      "42/42 [==============================] - 4s 94ms/step - loss: 0.1353 - accuracy: 0.9453\n",
      "Epoch 36/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.1342 - accuracy: 0.9474\n",
      "Epoch 36: accuracy improved from 0.94704 to 0.94741, saving model to eq_solver.h5\n",
      "42/42 [==============================] - 4s 94ms/step - loss: 0.1342 - accuracy: 0.9474\n",
      "Epoch 37/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.1190 - accuracy: 0.9507\n",
      "Epoch 37: accuracy improved from 0.94741 to 0.95069, saving model to eq_solver.h5\n",
      "42/42 [==============================] - 4s 99ms/step - loss: 0.1190 - accuracy: 0.9507\n",
      "Epoch 38/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.1281 - accuracy: 0.9488\n",
      "Epoch 38: accuracy did not improve from 0.95069\n",
      "42/42 [==============================] - 4s 93ms/step - loss: 0.1281 - accuracy: 0.9488\n",
      "Epoch 39/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.1191 - accuracy: 0.9526\n",
      "Epoch 39: accuracy improved from 0.95069 to 0.95256, saving model to eq_solver.h5\n",
      "42/42 [==============================] - 4s 96ms/step - loss: 0.1191 - accuracy: 0.9526\n",
      "Epoch 40/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.1153 - accuracy: 0.9512\n",
      "Epoch 40: accuracy did not improve from 0.95256\n",
      "42/42 [==============================] - 4s 94ms/step - loss: 0.1153 - accuracy: 0.9512\n",
      "Epoch 41/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.1142 - accuracy: 0.9522\n",
      "Epoch 41: accuracy did not improve from 0.95256\n",
      "42/42 [==============================] - 4s 93ms/step - loss: 0.1142 - accuracy: 0.9522\n",
      "Epoch 42/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.1139 - accuracy: 0.9533\n",
      "Epoch 42: accuracy improved from 0.95256 to 0.95331, saving model to eq_solver.h5\n",
      "42/42 [==============================] - 4s 93ms/step - loss: 0.1139 - accuracy: 0.9533\n",
      "Epoch 43/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.1102 - accuracy: 0.9552\n",
      "Epoch 43: accuracy improved from 0.95331 to 0.95518, saving model to eq_solver.h5\n",
      "42/42 [==============================] - 4s 96ms/step - loss: 0.1102 - accuracy: 0.9552\n",
      "Epoch 44/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.1103 - accuracy: 0.9535\n",
      "Epoch 44: accuracy did not improve from 0.95518\n",
      "42/42 [==============================] - 4s 92ms/step - loss: 0.1103 - accuracy: 0.9535\n",
      "Epoch 45/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.1078 - accuracy: 0.9556\n",
      "Epoch 45: accuracy improved from 0.95518 to 0.95555, saving model to eq_solver.h5\n",
      "42/42 [==============================] - 4s 94ms/step - loss: 0.1078 - accuracy: 0.9556\n",
      "Epoch 46/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.1055 - accuracy: 0.9583\n",
      "Epoch 46: accuracy improved from 0.95555 to 0.95827, saving model to eq_solver.h5\n",
      "42/42 [==============================] - 4s 95ms/step - loss: 0.1055 - accuracy: 0.9583\n",
      "Epoch 47/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.1058 - accuracy: 0.9580\n",
      "Epoch 47: accuracy did not improve from 0.95827\n",
      "42/42 [==============================] - 4s 93ms/step - loss: 0.1058 - accuracy: 0.9580\n",
      "Epoch 48/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.1045 - accuracy: 0.9573\n",
      "Epoch 48: accuracy did not improve from 0.95827\n",
      "42/42 [==============================] - 4s 95ms/step - loss: 0.1045 - accuracy: 0.9573\n",
      "Epoch 49/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.1017 - accuracy: 0.9613\n",
      "Epoch 49: accuracy improved from 0.95827 to 0.96126, saving model to eq_solver.h5\n",
      "42/42 [==============================] - 4s 93ms/step - loss: 0.1017 - accuracy: 0.9613\n",
      "Epoch 50/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0997 - accuracy: 0.9594\n",
      "Epoch 50: accuracy did not improve from 0.96126\n",
      "42/42 [==============================] - 4s 94ms/step - loss: 0.0997 - accuracy: 0.9594\n",
      "Epoch 51/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0923 - accuracy: 0.9626\n",
      "Epoch 51: accuracy improved from 0.96126 to 0.96257, saving model to eq_solver.h5\n",
      "42/42 [==============================] - 4s 94ms/step - loss: 0.0923 - accuracy: 0.9626\n",
      "Epoch 52/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0939 - accuracy: 0.9624\n",
      "Epoch 52: accuracy did not improve from 0.96257\n",
      "42/42 [==============================] - 4s 95ms/step - loss: 0.0939 - accuracy: 0.9624\n",
      "Epoch 53/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0965 - accuracy: 0.9593\n",
      "Epoch 53: accuracy did not improve from 0.96257\n",
      "42/42 [==============================] - 4s 95ms/step - loss: 0.0965 - accuracy: 0.9593\n",
      "Epoch 54/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0915 - accuracy: 0.9622\n",
      "Epoch 54: accuracy did not improve from 0.96257\n",
      "42/42 [==============================] - 4s 97ms/step - loss: 0.0915 - accuracy: 0.9622\n",
      "Epoch 55/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0939 - accuracy: 0.9613\n",
      "Epoch 55: accuracy did not improve from 0.96257\n",
      "42/42 [==============================] - 4s 95ms/step - loss: 0.0939 - accuracy: 0.9613\n",
      "Epoch 56/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0903 - accuracy: 0.9641\n",
      "Epoch 56: accuracy improved from 0.96257 to 0.96407, saving model to eq_solver.h5\n",
      "42/42 [==============================] - 4s 94ms/step - loss: 0.0903 - accuracy: 0.9641\n",
      "Epoch 57/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0880 - accuracy: 0.9635\n",
      "Epoch 57: accuracy did not improve from 0.96407\n",
      "42/42 [==============================] - 4s 93ms/step - loss: 0.0880 - accuracy: 0.9635\n",
      "Epoch 58/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0931 - accuracy: 0.9642\n",
      "Epoch 58: accuracy improved from 0.96407 to 0.96416, saving model to eq_solver.h5\n",
      "42/42 [==============================] - 4s 94ms/step - loss: 0.0931 - accuracy: 0.9642\n",
      "Epoch 59/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0901 - accuracy: 0.9644\n",
      "Epoch 59: accuracy improved from 0.96416 to 0.96444, saving model to eq_solver.h5\n",
      "42/42 [==============================] - 4s 96ms/step - loss: 0.0901 - accuracy: 0.9644\n",
      "Epoch 60/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0836 - accuracy: 0.9676\n",
      "Epoch 60: accuracy improved from 0.96444 to 0.96762, saving model to eq_solver.h5\n",
      "42/42 [==============================] - 4s 97ms/step - loss: 0.0836 - accuracy: 0.9676\n",
      "Epoch 61/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0896 - accuracy: 0.9644\n",
      "Epoch 61: accuracy did not improve from 0.96762\n",
      "42/42 [==============================] - 4s 95ms/step - loss: 0.0896 - accuracy: 0.9644\n",
      "Epoch 62/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0864 - accuracy: 0.9650\n",
      "Epoch 62: accuracy did not improve from 0.96762\n",
      "42/42 [==============================] - 4s 99ms/step - loss: 0.0864 - accuracy: 0.9650\n",
      "Epoch 63/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0808 - accuracy: 0.9665\n",
      "Epoch 63: accuracy did not improve from 0.96762\n",
      "42/42 [==============================] - 4s 97ms/step - loss: 0.0808 - accuracy: 0.9665\n",
      "Epoch 64/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0855 - accuracy: 0.9647\n",
      "Epoch 64: accuracy did not improve from 0.96762\n",
      "42/42 [==============================] - 4s 95ms/step - loss: 0.0855 - accuracy: 0.9647\n",
      "Epoch 65/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0856 - accuracy: 0.9661\n",
      "Epoch 65: accuracy did not improve from 0.96762\n",
      "42/42 [==============================] - 4s 95ms/step - loss: 0.0856 - accuracy: 0.9661\n",
      "Epoch 66/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0778 - accuracy: 0.9687\n",
      "Epoch 66: accuracy improved from 0.96762 to 0.96875, saving model to eq_solver.h5\n",
      "42/42 [==============================] - 4s 95ms/step - loss: 0.0778 - accuracy: 0.9687\n",
      "Epoch 67/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0802 - accuracy: 0.9676\n",
      "Epoch 67: accuracy did not improve from 0.96875\n",
      "42/42 [==============================] - 4s 98ms/step - loss: 0.0802 - accuracy: 0.9676\n",
      "Epoch 68/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0764 - accuracy: 0.9706\n",
      "Epoch 68: accuracy improved from 0.96875 to 0.97062, saving model to eq_solver.h5\n",
      "42/42 [==============================] - 4s 95ms/step - loss: 0.0764 - accuracy: 0.9706\n",
      "Epoch 69/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0719 - accuracy: 0.9727\n",
      "Epoch 69: accuracy improved from 0.97062 to 0.97268, saving model to eq_solver.h5\n",
      "42/42 [==============================] - 4s 96ms/step - loss: 0.0719 - accuracy: 0.9727\n",
      "Epoch 70/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0760 - accuracy: 0.9695\n",
      "Epoch 70: accuracy did not improve from 0.97268\n",
      "42/42 [==============================] - 4s 94ms/step - loss: 0.0760 - accuracy: 0.9695\n",
      "Epoch 71/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0776 - accuracy: 0.9689\n",
      "Epoch 71: accuracy did not improve from 0.97268\n",
      "42/42 [==============================] - 4s 93ms/step - loss: 0.0776 - accuracy: 0.9689\n",
      "Epoch 72/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0740 - accuracy: 0.9699\n",
      "Epoch 72: accuracy did not improve from 0.97268\n",
      "42/42 [==============================] - 4s 93ms/step - loss: 0.0740 - accuracy: 0.9699\n",
      "Epoch 73/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0716 - accuracy: 0.9713\n",
      "Epoch 73: accuracy did not improve from 0.97268\n",
      "42/42 [==============================] - 4s 93ms/step - loss: 0.0716 - accuracy: 0.9713\n",
      "Epoch 74/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0745 - accuracy: 0.9694\n",
      "Epoch 74: accuracy did not improve from 0.97268\n",
      "42/42 [==============================] - 4s 94ms/step - loss: 0.0745 - accuracy: 0.9694\n",
      "Epoch 75/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0759 - accuracy: 0.9706\n",
      "Epoch 75: accuracy did not improve from 0.97268\n",
      "42/42 [==============================] - 4s 97ms/step - loss: 0.0759 - accuracy: 0.9706\n",
      "Epoch 76/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0681 - accuracy: 0.9729\n",
      "Epoch 76: accuracy improved from 0.97268 to 0.97286, saving model to eq_solver.h5\n",
      "42/42 [==============================] - 4s 97ms/step - loss: 0.0681 - accuracy: 0.9729\n",
      "Epoch 77/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0707 - accuracy: 0.9735\n",
      "Epoch 77: accuracy improved from 0.97286 to 0.97352, saving model to eq_solver.h5\n",
      "42/42 [==============================] - 4s 98ms/step - loss: 0.0707 - accuracy: 0.9735\n",
      "Epoch 78/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0723 - accuracy: 0.9709\n",
      "Epoch 78: accuracy did not improve from 0.97352\n",
      "42/42 [==============================] - 4s 93ms/step - loss: 0.0723 - accuracy: 0.9709\n",
      "Epoch 79/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0708 - accuracy: 0.9723\n",
      "Epoch 79: accuracy did not improve from 0.97352\n",
      "42/42 [==============================] - 4s 98ms/step - loss: 0.0708 - accuracy: 0.9723\n",
      "Epoch 80/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0679 - accuracy: 0.9749\n",
      "Epoch 80: accuracy improved from 0.97352 to 0.97492, saving model to eq_solver.h5\n",
      "42/42 [==============================] - 4s 97ms/step - loss: 0.0679 - accuracy: 0.9749\n",
      "Epoch 81/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0684 - accuracy: 0.9751\n",
      "Epoch 81: accuracy improved from 0.97492 to 0.97511, saving model to eq_solver.h5\n",
      "42/42 [==============================] - 4s 97ms/step - loss: 0.0684 - accuracy: 0.9751\n",
      "Epoch 82/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0685 - accuracy: 0.9741\n",
      "Epoch 82: accuracy did not improve from 0.97511\n",
      "42/42 [==============================] - 4s 102ms/step - loss: 0.0685 - accuracy: 0.9741\n",
      "Epoch 83/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0728 - accuracy: 0.9720\n",
      "Epoch 83: accuracy did not improve from 0.97511\n",
      "42/42 [==============================] - 4s 97ms/step - loss: 0.0728 - accuracy: 0.9720\n",
      "Epoch 84/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0701 - accuracy: 0.9737\n",
      "Epoch 84: accuracy did not improve from 0.97511\n",
      "42/42 [==============================] - 4s 95ms/step - loss: 0.0701 - accuracy: 0.9737\n",
      "Epoch 85/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0634 - accuracy: 0.9762\n",
      "Epoch 85: accuracy improved from 0.97511 to 0.97623, saving model to eq_solver.h5\n",
      "42/42 [==============================] - 4s 96ms/step - loss: 0.0634 - accuracy: 0.9762\n",
      "Epoch 86/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0590 - accuracy: 0.9746\n",
      "Epoch 86: accuracy did not improve from 0.97623\n",
      "42/42 [==============================] - 4s 97ms/step - loss: 0.0590 - accuracy: 0.9746\n",
      "Epoch 87/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0569 - accuracy: 0.9761\n",
      "Epoch 87: accuracy did not improve from 0.97623\n",
      "42/42 [==============================] - 4s 96ms/step - loss: 0.0569 - accuracy: 0.9761\n",
      "Epoch 88/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0658 - accuracy: 0.9751\n",
      "Epoch 88: accuracy did not improve from 0.97623\n",
      "42/42 [==============================] - 4s 97ms/step - loss: 0.0658 - accuracy: 0.9751\n",
      "Epoch 89/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0536 - accuracy: 0.9790\n",
      "Epoch 89: accuracy improved from 0.97623 to 0.97904, saving model to eq_solver.h5\n",
      "42/42 [==============================] - 4s 99ms/step - loss: 0.0536 - accuracy: 0.9790\n",
      "Epoch 90/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0636 - accuracy: 0.9775\n",
      "Epoch 90: accuracy did not improve from 0.97904\n",
      "42/42 [==============================] - 4s 97ms/step - loss: 0.0636 - accuracy: 0.9775\n",
      "Epoch 91/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0597 - accuracy: 0.9763\n",
      "Epoch 91: accuracy did not improve from 0.97904\n",
      "42/42 [==============================] - 4s 96ms/step - loss: 0.0597 - accuracy: 0.9763\n",
      "Epoch 92/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0554 - accuracy: 0.9785\n",
      "Epoch 92: accuracy did not improve from 0.97904\n",
      "42/42 [==============================] - 4s 93ms/step - loss: 0.0554 - accuracy: 0.9785\n",
      "Epoch 93/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0611 - accuracy: 0.9777\n",
      "Epoch 93: accuracy did not improve from 0.97904\n",
      "42/42 [==============================] - 4s 96ms/step - loss: 0.0611 - accuracy: 0.9777\n",
      "Epoch 94/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0570 - accuracy: 0.9776\n",
      "Epoch 94: accuracy did not improve from 0.97904\n",
      "42/42 [==============================] - 4s 98ms/step - loss: 0.0570 - accuracy: 0.9776\n",
      "Epoch 95/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0481 - accuracy: 0.9812\n",
      "Epoch 95: accuracy improved from 0.97904 to 0.98119, saving model to eq_solver.h5\n",
      "42/42 [==============================] - 4s 98ms/step - loss: 0.0481 - accuracy: 0.9812\n",
      "Epoch 96/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0506 - accuracy: 0.9797\n",
      "Epoch 96: accuracy did not improve from 0.98119\n",
      "42/42 [==============================] - 4s 95ms/step - loss: 0.0506 - accuracy: 0.9797\n",
      "Epoch 97/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0533 - accuracy: 0.9789\n",
      "Epoch 97: accuracy did not improve from 0.98119\n",
      "42/42 [==============================] - 4s 96ms/step - loss: 0.0533 - accuracy: 0.9789\n",
      "Epoch 98/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0573 - accuracy: 0.9782\n",
      "Epoch 98: accuracy did not improve from 0.98119\n",
      "42/42 [==============================] - 4s 96ms/step - loss: 0.0573 - accuracy: 0.9782\n",
      "Epoch 99/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0516 - accuracy: 0.9809\n",
      "Epoch 99: accuracy did not improve from 0.98119\n",
      "42/42 [==============================] - 4s 94ms/step - loss: 0.0516 - accuracy: 0.9809\n",
      "Epoch 100/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0553 - accuracy: 0.9798\n",
      "Epoch 100: accuracy did not improve from 0.98119\n",
      "42/42 [==============================] - 4s 98ms/step - loss: 0.0553 - accuracy: 0.9798\n",
      "Epoch 101/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0519 - accuracy: 0.9835\n",
      "Epoch 101: accuracy improved from 0.98119 to 0.98353, saving model to eq_solver.h5\n",
      "42/42 [==============================] - 4s 96ms/step - loss: 0.0519 - accuracy: 0.9835\n",
      "Epoch 102/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0578 - accuracy: 0.9788\n",
      "Epoch 102: accuracy did not improve from 0.98353\n",
      "42/42 [==============================] - 4s 96ms/step - loss: 0.0578 - accuracy: 0.9788\n",
      "Epoch 103/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0640 - accuracy: 0.9760\n",
      "Epoch 103: accuracy did not improve from 0.98353\n",
      "42/42 [==============================] - 4s 95ms/step - loss: 0.0640 - accuracy: 0.9760\n",
      "Epoch 104/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0466 - accuracy: 0.9826\n",
      "Epoch 104: accuracy did not improve from 0.98353\n",
      "42/42 [==============================] - 4s 97ms/step - loss: 0.0466 - accuracy: 0.9826\n",
      "Epoch 105/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0485 - accuracy: 0.9817\n",
      "Epoch 105: accuracy did not improve from 0.98353\n",
      "42/42 [==============================] - 4s 95ms/step - loss: 0.0485 - accuracy: 0.9817\n",
      "Epoch 106/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0509 - accuracy: 0.9801\n",
      "Epoch 106: accuracy did not improve from 0.98353\n",
      "42/42 [==============================] - 4s 96ms/step - loss: 0.0509 - accuracy: 0.9801\n",
      "Epoch 107/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0495 - accuracy: 0.9803\n",
      "Epoch 107: accuracy did not improve from 0.98353\n",
      "42/42 [==============================] - 4s 95ms/step - loss: 0.0495 - accuracy: 0.9803\n",
      "Epoch 108/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0528 - accuracy: 0.9803\n",
      "Epoch 108: accuracy did not improve from 0.98353\n",
      "42/42 [==============================] - 4s 96ms/step - loss: 0.0528 - accuracy: 0.9803\n",
      "Epoch 109/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0506 - accuracy: 0.9812\n",
      "Epoch 109: accuracy did not improve from 0.98353\n",
      "42/42 [==============================] - 4s 98ms/step - loss: 0.0506 - accuracy: 0.9812\n",
      "Epoch 110/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0463 - accuracy: 0.9827\n",
      "Epoch 110: accuracy did not improve from 0.98353\n",
      "42/42 [==============================] - 4s 96ms/step - loss: 0.0463 - accuracy: 0.9827\n",
      "Epoch 111/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0422 - accuracy: 0.9851\n",
      "Epoch 111: accuracy improved from 0.98353 to 0.98512, saving model to eq_solver.h5\n",
      "42/42 [==============================] - 4s 98ms/step - loss: 0.0422 - accuracy: 0.9851\n",
      "Epoch 112/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0476 - accuracy: 0.9823\n",
      "Epoch 112: accuracy did not improve from 0.98512\n",
      "42/42 [==============================] - 4s 97ms/step - loss: 0.0476 - accuracy: 0.9823\n",
      "Epoch 113/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0443 - accuracy: 0.9833\n",
      "Epoch 113: accuracy did not improve from 0.98512\n",
      "42/42 [==============================] - 4s 96ms/step - loss: 0.0443 - accuracy: 0.9833\n",
      "Epoch 114/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0562 - accuracy: 0.9782\n",
      "Epoch 114: accuracy did not improve from 0.98512\n",
      "42/42 [==============================] - 4s 96ms/step - loss: 0.0562 - accuracy: 0.9782\n",
      "Epoch 115/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0469 - accuracy: 0.9823\n",
      "Epoch 115: accuracy did not improve from 0.98512\n",
      "42/42 [==============================] - 4s 94ms/step - loss: 0.0469 - accuracy: 0.9823\n",
      "Epoch 116/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0448 - accuracy: 0.9836\n",
      "Epoch 116: accuracy did not improve from 0.98512\n",
      "42/42 [==============================] - 4s 98ms/step - loss: 0.0448 - accuracy: 0.9836\n",
      "Epoch 117/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0469 - accuracy: 0.9805\n",
      "Epoch 117: accuracy did not improve from 0.98512\n",
      "42/42 [==============================] - 4s 95ms/step - loss: 0.0469 - accuracy: 0.9805\n",
      "Epoch 118/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0487 - accuracy: 0.9816\n",
      "Epoch 118: accuracy did not improve from 0.98512\n",
      "42/42 [==============================] - 4s 96ms/step - loss: 0.0487 - accuracy: 0.9816\n",
      "Epoch 119/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0553 - accuracy: 0.9801\n",
      "Epoch 119: accuracy did not improve from 0.98512\n",
      "42/42 [==============================] - 4s 100ms/step - loss: 0.0553 - accuracy: 0.9801\n",
      "Epoch 120/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0495 - accuracy: 0.9834\n",
      "Epoch 120: accuracy did not improve from 0.98512\n",
      "42/42 [==============================] - 4s 102ms/step - loss: 0.0495 - accuracy: 0.9834\n",
      "Epoch 121/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0470 - accuracy: 0.9818\n",
      "Epoch 121: accuracy did not improve from 0.98512\n",
      "42/42 [==============================] - 4s 97ms/step - loss: 0.0470 - accuracy: 0.9818\n",
      "Epoch 122/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0397 - accuracy: 0.9855\n",
      "Epoch 122: accuracy improved from 0.98512 to 0.98550, saving model to eq_solver.h5\n",
      "42/42 [==============================] - 4s 96ms/step - loss: 0.0397 - accuracy: 0.9855\n",
      "Epoch 123/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0353 - accuracy: 0.9884\n",
      "Epoch 123: accuracy improved from 0.98550 to 0.98840, saving model to eq_solver.h5\n",
      "42/42 [==============================] - 4s 97ms/step - loss: 0.0353 - accuracy: 0.9884\n",
      "Epoch 124/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0434 - accuracy: 0.9834\n",
      "Epoch 124: accuracy did not improve from 0.98840\n",
      "42/42 [==============================] - 4s 99ms/step - loss: 0.0434 - accuracy: 0.9834\n",
      "Epoch 125/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0396 - accuracy: 0.9857\n",
      "Epoch 125: accuracy did not improve from 0.98840\n",
      "42/42 [==============================] - 4s 95ms/step - loss: 0.0396 - accuracy: 0.9857\n",
      "Epoch 126/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0371 - accuracy: 0.9879\n",
      "Epoch 126: accuracy did not improve from 0.98840\n",
      "42/42 [==============================] - 4s 98ms/step - loss: 0.0371 - accuracy: 0.9879\n",
      "Epoch 127/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0387 - accuracy: 0.9864\n",
      "Epoch 127: accuracy did not improve from 0.98840\n",
      "42/42 [==============================] - 4s 95ms/step - loss: 0.0387 - accuracy: 0.9864\n",
      "Epoch 128/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0395 - accuracy: 0.9864\n",
      "Epoch 128: accuracy did not improve from 0.98840\n",
      "42/42 [==============================] - 4s 93ms/step - loss: 0.0395 - accuracy: 0.9864\n",
      "Epoch 129/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0387 - accuracy: 0.9846\n",
      "Epoch 129: accuracy did not improve from 0.98840\n",
      "42/42 [==============================] - 4s 96ms/step - loss: 0.0387 - accuracy: 0.9846\n",
      "Epoch 130/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0380 - accuracy: 0.9871\n",
      "Epoch 130: accuracy did not improve from 0.98840\n",
      "42/42 [==============================] - 4s 95ms/step - loss: 0.0380 - accuracy: 0.9871\n",
      "Epoch 131/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0428 - accuracy: 0.9857\n",
      "Epoch 131: accuracy did not improve from 0.98840\n",
      "42/42 [==============================] - 4s 98ms/step - loss: 0.0428 - accuracy: 0.9857\n",
      "Epoch 132/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0529 - accuracy: 0.9817\n",
      "Epoch 132: accuracy did not improve from 0.98840\n",
      "42/42 [==============================] - 4s 98ms/step - loss: 0.0529 - accuracy: 0.9817\n",
      "Epoch 133/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0382 - accuracy: 0.9861\n",
      "Epoch 133: accuracy did not improve from 0.98840\n",
      "42/42 [==============================] - 4s 94ms/step - loss: 0.0382 - accuracy: 0.9861\n",
      "Epoch 134/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0399 - accuracy: 0.9859\n",
      "Epoch 134: accuracy did not improve from 0.98840\n",
      "42/42 [==============================] - 4s 97ms/step - loss: 0.0399 - accuracy: 0.9859\n",
      "Epoch 135/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0354 - accuracy: 0.9877\n",
      "Epoch 135: accuracy did not improve from 0.98840\n",
      "42/42 [==============================] - 4s 95ms/step - loss: 0.0354 - accuracy: 0.9877\n",
      "Epoch 136/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0314 - accuracy: 0.9883\n",
      "Epoch 136: accuracy did not improve from 0.98840\n",
      "42/42 [==============================] - 4s 97ms/step - loss: 0.0314 - accuracy: 0.9883\n",
      "Epoch 137/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0385 - accuracy: 0.9867\n",
      "Epoch 137: accuracy did not improve from 0.98840\n",
      "42/42 [==============================] - 4s 95ms/step - loss: 0.0385 - accuracy: 0.9867\n",
      "Epoch 138/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0440 - accuracy: 0.9847\n",
      "Epoch 138: accuracy did not improve from 0.98840\n",
      "42/42 [==============================] - 4s 95ms/step - loss: 0.0440 - accuracy: 0.9847\n",
      "Epoch 139/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0387 - accuracy: 0.9861\n",
      "Epoch 139: accuracy did not improve from 0.98840\n",
      "42/42 [==============================] - 4s 96ms/step - loss: 0.0387 - accuracy: 0.9861\n",
      "Epoch 140/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0338 - accuracy: 0.9882\n",
      "Epoch 140: accuracy did not improve from 0.98840\n",
      "42/42 [==============================] - 4s 97ms/step - loss: 0.0338 - accuracy: 0.9882\n",
      "Epoch 141/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0379 - accuracy: 0.9873\n",
      "Epoch 141: accuracy did not improve from 0.98840\n",
      "42/42 [==============================] - 4s 97ms/step - loss: 0.0379 - accuracy: 0.9873\n",
      "Epoch 142/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0356 - accuracy: 0.9882\n",
      "Epoch 142: accuracy did not improve from 0.98840\n",
      "42/42 [==============================] - 4s 97ms/step - loss: 0.0356 - accuracy: 0.9882\n",
      "Epoch 143/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0336 - accuracy: 0.9872\n",
      "Epoch 143: accuracy did not improve from 0.98840\n",
      "42/42 [==============================] - 4s 95ms/step - loss: 0.0336 - accuracy: 0.9872\n",
      "Epoch 144/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0398 - accuracy: 0.9860\n",
      "Epoch 144: accuracy did not improve from 0.98840\n",
      "42/42 [==============================] - 4s 98ms/step - loss: 0.0398 - accuracy: 0.9860\n",
      "Epoch 145/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0328 - accuracy: 0.9870\n",
      "Epoch 145: accuracy did not improve from 0.98840\n",
      "42/42 [==============================] - 4s 96ms/step - loss: 0.0328 - accuracy: 0.9870\n",
      "Epoch 146/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0348 - accuracy: 0.9876\n",
      "Epoch 146: accuracy did not improve from 0.98840\n",
      "42/42 [==============================] - 4s 96ms/step - loss: 0.0348 - accuracy: 0.9876\n",
      "Epoch 147/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0339 - accuracy: 0.9887\n",
      "Epoch 147: accuracy improved from 0.98840 to 0.98868, saving model to eq_solver.h5\n",
      "42/42 [==============================] - 4s 97ms/step - loss: 0.0339 - accuracy: 0.9887\n",
      "Epoch 148/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0341 - accuracy: 0.9885\n",
      "Epoch 148: accuracy did not improve from 0.98868\n",
      "42/42 [==============================] - 4s 97ms/step - loss: 0.0341 - accuracy: 0.9885\n",
      "Epoch 149/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0320 - accuracy: 0.9870\n",
      "Epoch 149: accuracy did not improve from 0.98868\n",
      "42/42 [==============================] - 4s 102ms/step - loss: 0.0320 - accuracy: 0.9870\n",
      "Epoch 150/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0360 - accuracy: 0.9869\n",
      "Epoch 150: accuracy did not improve from 0.98868\n",
      "42/42 [==============================] - 4s 104ms/step - loss: 0.0360 - accuracy: 0.9869\n",
      "Epoch 151/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0349 - accuracy: 0.9874\n",
      "Epoch 151: accuracy did not improve from 0.98868\n",
      "42/42 [==============================] - 4s 97ms/step - loss: 0.0349 - accuracy: 0.9874\n",
      "Epoch 152/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0349 - accuracy: 0.9869\n",
      "Epoch 152: accuracy did not improve from 0.98868\n",
      "42/42 [==============================] - 4s 98ms/step - loss: 0.0349 - accuracy: 0.9869\n",
      "Epoch 153/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0359 - accuracy: 0.9882\n",
      "Epoch 153: accuracy did not improve from 0.98868\n",
      "42/42 [==============================] - 4s 96ms/step - loss: 0.0359 - accuracy: 0.9882\n",
      "Epoch 154/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0336 - accuracy: 0.9873\n",
      "Epoch 154: accuracy did not improve from 0.98868\n",
      "42/42 [==============================] - 4s 100ms/step - loss: 0.0336 - accuracy: 0.9873\n",
      "Epoch 155/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0325 - accuracy: 0.9895\n",
      "Epoch 155: accuracy improved from 0.98868 to 0.98952, saving model to eq_solver.h5\n",
      "42/42 [==============================] - 4s 96ms/step - loss: 0.0325 - accuracy: 0.9895\n",
      "Epoch 156/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0323 - accuracy: 0.9879\n",
      "Epoch 156: accuracy did not improve from 0.98952\n",
      "42/42 [==============================] - 4s 98ms/step - loss: 0.0323 - accuracy: 0.9879\n",
      "Epoch 157/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0431 - accuracy: 0.9862\n",
      "Epoch 157: accuracy did not improve from 0.98952\n",
      "42/42 [==============================] - 4s 99ms/step - loss: 0.0431 - accuracy: 0.9862\n",
      "Epoch 158/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0352 - accuracy: 0.9873\n",
      "Epoch 158: accuracy did not improve from 0.98952\n",
      "42/42 [==============================] - 4s 97ms/step - loss: 0.0352 - accuracy: 0.9873\n",
      "Epoch 159/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0326 - accuracy: 0.9890\n",
      "Epoch 159: accuracy did not improve from 0.98952\n",
      "42/42 [==============================] - 4s 96ms/step - loss: 0.0326 - accuracy: 0.9890\n",
      "Epoch 160/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0312 - accuracy: 0.9880\n",
      "Epoch 160: accuracy did not improve from 0.98952\n",
      "42/42 [==============================] - 4s 99ms/step - loss: 0.0312 - accuracy: 0.9880\n",
      "Epoch 161/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0351 - accuracy: 0.9882\n",
      "Epoch 161: accuracy did not improve from 0.98952\n",
      "42/42 [==============================] - 4s 94ms/step - loss: 0.0351 - accuracy: 0.9882\n",
      "Epoch 162/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0311 - accuracy: 0.9896\n",
      "Epoch 162: accuracy improved from 0.98952 to 0.98961, saving model to eq_solver.h5\n",
      "42/42 [==============================] - 4s 94ms/step - loss: 0.0311 - accuracy: 0.9896\n",
      "Epoch 163/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0282 - accuracy: 0.9905\n",
      "Epoch 163: accuracy improved from 0.98961 to 0.99055, saving model to eq_solver.h5\n",
      "42/42 [==============================] - 4s 97ms/step - loss: 0.0282 - accuracy: 0.9905\n",
      "Epoch 164/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0330 - accuracy: 0.9899\n",
      "Epoch 164: accuracy did not improve from 0.99055\n",
      "42/42 [==============================] - 4s 97ms/step - loss: 0.0330 - accuracy: 0.9899\n",
      "Epoch 165/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0263 - accuracy: 0.9899\n",
      "Epoch 165: accuracy did not improve from 0.99055\n",
      "42/42 [==============================] - 4s 97ms/step - loss: 0.0263 - accuracy: 0.9899\n",
      "Epoch 166/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0285 - accuracy: 0.9891\n",
      "Epoch 166: accuracy did not improve from 0.99055\n",
      "42/42 [==============================] - 4s 93ms/step - loss: 0.0285 - accuracy: 0.9891\n",
      "Epoch 167/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0322 - accuracy: 0.9887\n",
      "Epoch 167: accuracy did not improve from 0.99055\n",
      "42/42 [==============================] - 4s 96ms/step - loss: 0.0322 - accuracy: 0.9887\n",
      "Epoch 168/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0288 - accuracy: 0.9901\n",
      "Epoch 168: accuracy did not improve from 0.99055\n",
      "42/42 [==============================] - 4s 95ms/step - loss: 0.0288 - accuracy: 0.9901\n",
      "Epoch 169/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0262 - accuracy: 0.9903\n",
      "Epoch 169: accuracy did not improve from 0.99055\n",
      "42/42 [==============================] - 4s 96ms/step - loss: 0.0262 - accuracy: 0.9903\n",
      "Epoch 170/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0251 - accuracy: 0.9908\n",
      "Epoch 170: accuracy improved from 0.99055 to 0.99083, saving model to eq_solver.h5\n",
      "42/42 [==============================] - 4s 96ms/step - loss: 0.0251 - accuracy: 0.9908\n",
      "Epoch 171/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0277 - accuracy: 0.9906\n",
      "Epoch 171: accuracy did not improve from 0.99083\n",
      "42/42 [==============================] - 4s 97ms/step - loss: 0.0277 - accuracy: 0.9906\n",
      "Epoch 172/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0321 - accuracy: 0.9890\n",
      "Epoch 172: accuracy did not improve from 0.99083\n",
      "42/42 [==============================] - 4s 98ms/step - loss: 0.0321 - accuracy: 0.9890\n",
      "Epoch 173/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0287 - accuracy: 0.9901\n",
      "Epoch 173: accuracy did not improve from 0.99083\n",
      "42/42 [==============================] - 4s 96ms/step - loss: 0.0287 - accuracy: 0.9901\n",
      "Epoch 174/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0229 - accuracy: 0.9916\n",
      "Epoch 174: accuracy improved from 0.99083 to 0.99158, saving model to eq_solver.h5\n",
      "42/42 [==============================] - 4s 99ms/step - loss: 0.0229 - accuracy: 0.9916\n",
      "Epoch 175/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0248 - accuracy: 0.9917\n",
      "Epoch 175: accuracy improved from 0.99158 to 0.99167, saving model to eq_solver.h5\n",
      "42/42 [==============================] - 4s 97ms/step - loss: 0.0248 - accuracy: 0.9917\n",
      "Epoch 176/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0257 - accuracy: 0.9894\n",
      "Epoch 176: accuracy did not improve from 0.99167\n",
      "42/42 [==============================] - 4s 97ms/step - loss: 0.0257 - accuracy: 0.9894\n",
      "Epoch 177/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0287 - accuracy: 0.9909\n",
      "Epoch 177: accuracy did not improve from 0.99167\n",
      "42/42 [==============================] - 4s 99ms/step - loss: 0.0287 - accuracy: 0.9909\n",
      "Epoch 178/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0303 - accuracy: 0.9898\n",
      "Epoch 178: accuracy did not improve from 0.99167\n",
      "42/42 [==============================] - 4s 97ms/step - loss: 0.0303 - accuracy: 0.9898\n",
      "Epoch 179/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0290 - accuracy: 0.9892\n",
      "Epoch 179: accuracy did not improve from 0.99167\n",
      "42/42 [==============================] - 4s 98ms/step - loss: 0.0290 - accuracy: 0.9892\n",
      "Epoch 180/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0315 - accuracy: 0.9887\n",
      "Epoch 180: accuracy did not improve from 0.99167\n",
      "42/42 [==============================] - 4s 97ms/step - loss: 0.0315 - accuracy: 0.9887\n",
      "Epoch 181/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0316 - accuracy: 0.9900\n",
      "Epoch 181: accuracy did not improve from 0.99167\n",
      "42/42 [==============================] - 4s 96ms/step - loss: 0.0316 - accuracy: 0.9900\n",
      "Epoch 182/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0342 - accuracy: 0.9889\n",
      "Epoch 182: accuracy did not improve from 0.99167\n",
      "42/42 [==============================] - 4s 95ms/step - loss: 0.0342 - accuracy: 0.9889\n",
      "Epoch 183/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0349 - accuracy: 0.9891\n",
      "Epoch 183: accuracy did not improve from 0.99167\n",
      "42/42 [==============================] - 4s 97ms/step - loss: 0.0349 - accuracy: 0.9891\n",
      "Epoch 184/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0255 - accuracy: 0.9916\n",
      "Epoch 184: accuracy did not improve from 0.99167\n",
      "42/42 [==============================] - 4s 98ms/step - loss: 0.0255 - accuracy: 0.9916\n",
      "Epoch 185/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0243 - accuracy: 0.9911\n",
      "Epoch 185: accuracy did not improve from 0.99167\n",
      "42/42 [==============================] - 4s 98ms/step - loss: 0.0243 - accuracy: 0.9911\n",
      "Epoch 186/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0259 - accuracy: 0.9912\n",
      "Epoch 186: accuracy did not improve from 0.99167\n",
      "42/42 [==============================] - 4s 98ms/step - loss: 0.0259 - accuracy: 0.9912\n",
      "Epoch 187/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0311 - accuracy: 0.9901\n",
      "Epoch 187: accuracy did not improve from 0.99167\n",
      "42/42 [==============================] - 4s 99ms/step - loss: 0.0311 - accuracy: 0.9901\n",
      "Epoch 188/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0255 - accuracy: 0.9916\n",
      "Epoch 188: accuracy did not improve from 0.99167\n",
      "42/42 [==============================] - 4s 98ms/step - loss: 0.0255 - accuracy: 0.9916\n",
      "Epoch 189/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0253 - accuracy: 0.9912\n",
      "Epoch 189: accuracy did not improve from 0.99167\n",
      "42/42 [==============================] - 4s 100ms/step - loss: 0.0253 - accuracy: 0.9912\n",
      "Epoch 190/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0302 - accuracy: 0.9901\n",
      "Epoch 190: accuracy did not improve from 0.99167\n",
      "42/42 [==============================] - 4s 106ms/step - loss: 0.0302 - accuracy: 0.9901\n",
      "Epoch 191/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0270 - accuracy: 0.9920\n",
      "Epoch 191: accuracy improved from 0.99167 to 0.99205, saving model to eq_solver.h5\n",
      "42/42 [==============================] - 4s 100ms/step - loss: 0.0270 - accuracy: 0.9920\n",
      "Epoch 192/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0294 - accuracy: 0.9905\n",
      "Epoch 192: accuracy did not improve from 0.99205\n",
      "42/42 [==============================] - 5s 108ms/step - loss: 0.0294 - accuracy: 0.9905\n",
      "Epoch 193/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0269 - accuracy: 0.9898\n",
      "Epoch 193: accuracy did not improve from 0.99205\n",
      "42/42 [==============================] - 4s 100ms/step - loss: 0.0269 - accuracy: 0.9898\n",
      "Epoch 194/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0275 - accuracy: 0.9899\n",
      "Epoch 194: accuracy did not improve from 0.99205\n",
      "42/42 [==============================] - 4s 97ms/step - loss: 0.0275 - accuracy: 0.9899\n",
      "Epoch 195/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0261 - accuracy: 0.9922\n",
      "Epoch 195: accuracy improved from 0.99205 to 0.99223, saving model to eq_solver.h5\n",
      "42/42 [==============================] - 4s 98ms/step - loss: 0.0261 - accuracy: 0.9922\n",
      "Epoch 196/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0304 - accuracy: 0.9905\n",
      "Epoch 196: accuracy did not improve from 0.99223\n",
      "42/42 [==============================] - 4s 94ms/step - loss: 0.0304 - accuracy: 0.9905\n",
      "Epoch 197/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0258 - accuracy: 0.9916\n",
      "Epoch 197: accuracy did not improve from 0.99223\n",
      "42/42 [==============================] - 4s 96ms/step - loss: 0.0258 - accuracy: 0.9916\n",
      "Epoch 198/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0231 - accuracy: 0.9918\n",
      "Epoch 198: accuracy did not improve from 0.99223\n",
      "42/42 [==============================] - 4s 94ms/step - loss: 0.0231 - accuracy: 0.9918\n",
      "Epoch 199/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0301 - accuracy: 0.9904\n",
      "Epoch 199: accuracy did not improve from 0.99223\n",
      "42/42 [==============================] - 4s 98ms/step - loss: 0.0301 - accuracy: 0.9904\n",
      "Epoch 200/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0278 - accuracy: 0.9910\n",
      "Epoch 200: accuracy did not improve from 0.99223\n",
      "42/42 [==============================] - 4s 96ms/step - loss: 0.0278 - accuracy: 0.9910\n",
      "Epoch 201/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0308 - accuracy: 0.9914\n",
      "Epoch 201: accuracy did not improve from 0.99223\n",
      "42/42 [==============================] - 4s 98ms/step - loss: 0.0308 - accuracy: 0.9914\n",
      "Epoch 202/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0265 - accuracy: 0.9915\n",
      "Epoch 202: accuracy did not improve from 0.99223\n",
      "42/42 [==============================] - 4s 92ms/step - loss: 0.0265 - accuracy: 0.9915\n",
      "Epoch 203/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0277 - accuracy: 0.9902\n",
      "Epoch 203: accuracy did not improve from 0.99223\n",
      "42/42 [==============================] - 4s 101ms/step - loss: 0.0277 - accuracy: 0.9902\n",
      "Epoch 204/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0192 - accuracy: 0.9934\n",
      "Epoch 204: accuracy improved from 0.99223 to 0.99345, saving model to eq_solver.h5\n",
      "42/42 [==============================] - 4s 104ms/step - loss: 0.0192 - accuracy: 0.9934\n",
      "Epoch 205/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0142 - accuracy: 0.9949\n",
      "Epoch 205: accuracy improved from 0.99345 to 0.99495, saving model to eq_solver.h5\n",
      "42/42 [==============================] - 4s 105ms/step - loss: 0.0142 - accuracy: 0.9949\n",
      "Epoch 206/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0163 - accuracy: 0.9949\n",
      "Epoch 206: accuracy did not improve from 0.99495\n",
      "42/42 [==============================] - 5s 109ms/step - loss: 0.0163 - accuracy: 0.9949\n",
      "Epoch 207/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0215 - accuracy: 0.9914\n",
      "Epoch 207: accuracy did not improve from 0.99495\n",
      "42/42 [==============================] - 5s 115ms/step - loss: 0.0215 - accuracy: 0.9914\n",
      "Epoch 208/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0258 - accuracy: 0.9916\n",
      "Epoch 208: accuracy did not improve from 0.99495\n",
      "42/42 [==============================] - 4s 105ms/step - loss: 0.0258 - accuracy: 0.9916\n",
      "Epoch 209/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0290 - accuracy: 0.9916\n",
      "Epoch 209: accuracy did not improve from 0.99495\n",
      "42/42 [==============================] - 5s 118ms/step - loss: 0.0290 - accuracy: 0.9916\n",
      "Epoch 210/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0313 - accuracy: 0.9887\n",
      "Epoch 210: accuracy did not improve from 0.99495\n",
      "42/42 [==============================] - 5s 116ms/step - loss: 0.0313 - accuracy: 0.9887\n",
      "Epoch 211/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0352 - accuracy: 0.9881\n",
      "Epoch 211: accuracy did not improve from 0.99495\n",
      "42/42 [==============================] - 4s 103ms/step - loss: 0.0352 - accuracy: 0.9881\n",
      "Epoch 212/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0241 - accuracy: 0.9915\n",
      "Epoch 212: accuracy did not improve from 0.99495\n",
      "42/42 [==============================] - 4s 106ms/step - loss: 0.0241 - accuracy: 0.9915\n",
      "Epoch 213/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0309 - accuracy: 0.9902\n",
      "Epoch 213: accuracy did not improve from 0.99495\n",
      "42/42 [==============================] - 5s 117ms/step - loss: 0.0309 - accuracy: 0.9902\n",
      "Epoch 214/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0249 - accuracy: 0.9916\n",
      "Epoch 214: accuracy did not improve from 0.99495\n",
      "42/42 [==============================] - 4s 106ms/step - loss: 0.0249 - accuracy: 0.9916\n",
      "Epoch 215/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0225 - accuracy: 0.9927\n",
      "Epoch 215: accuracy did not improve from 0.99495\n",
      "42/42 [==============================] - 5s 107ms/step - loss: 0.0225 - accuracy: 0.9927\n",
      "Epoch 216/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0220 - accuracy: 0.9927\n",
      "Epoch 216: accuracy did not improve from 0.99495\n",
      "42/42 [==============================] - 5s 110ms/step - loss: 0.0220 - accuracy: 0.9927\n",
      "Epoch 217/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0250 - accuracy: 0.9924\n",
      "Epoch 217: accuracy did not improve from 0.99495\n",
      "42/42 [==============================] - 4s 102ms/step - loss: 0.0250 - accuracy: 0.9924\n",
      "Epoch 218/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0210 - accuracy: 0.9929\n",
      "Epoch 218: accuracy did not improve from 0.99495\n",
      "42/42 [==============================] - 4s 96ms/step - loss: 0.0210 - accuracy: 0.9929\n",
      "Epoch 219/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0298 - accuracy: 0.9912\n",
      "Epoch 219: accuracy did not improve from 0.99495\n",
      "42/42 [==============================] - 4s 97ms/step - loss: 0.0298 - accuracy: 0.9912\n",
      "Epoch 220/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0211 - accuracy: 0.9930\n",
      "Epoch 220: accuracy did not improve from 0.99495\n",
      "42/42 [==============================] - 4s 96ms/step - loss: 0.0211 - accuracy: 0.9930\n",
      "Epoch 221/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0217 - accuracy: 0.9926\n",
      "Epoch 221: accuracy did not improve from 0.99495\n",
      "42/42 [==============================] - 4s 98ms/step - loss: 0.0217 - accuracy: 0.9926\n",
      "Epoch 222/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0227 - accuracy: 0.9926\n",
      "Epoch 222: accuracy did not improve from 0.99495\n",
      "42/42 [==============================] - 4s 100ms/step - loss: 0.0227 - accuracy: 0.9926\n",
      "Epoch 223/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0229 - accuracy: 0.9906\n",
      "Epoch 223: accuracy did not improve from 0.99495\n",
      "42/42 [==============================] - 4s 96ms/step - loss: 0.0229 - accuracy: 0.9906\n",
      "Epoch 224/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0229 - accuracy: 0.9925\n",
      "Epoch 224: accuracy did not improve from 0.99495\n",
      "42/42 [==============================] - 4s 99ms/step - loss: 0.0229 - accuracy: 0.9925\n",
      "Epoch 225/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0248 - accuracy: 0.9912\n",
      "Epoch 225: accuracy did not improve from 0.99495\n",
      "42/42 [==============================] - 4s 97ms/step - loss: 0.0248 - accuracy: 0.9912\n",
      "Epoch 226/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0209 - accuracy: 0.9928\n",
      "Epoch 226: accuracy did not improve from 0.99495\n",
      "42/42 [==============================] - 4s 98ms/step - loss: 0.0209 - accuracy: 0.9928\n",
      "Epoch 227/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0235 - accuracy: 0.9919\n",
      "Epoch 227: accuracy did not improve from 0.99495\n",
      "42/42 [==============================] - 4s 100ms/step - loss: 0.0235 - accuracy: 0.9919\n",
      "Epoch 228/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0207 - accuracy: 0.9932\n",
      "Epoch 228: accuracy did not improve from 0.99495\n",
      "42/42 [==============================] - 4s 100ms/step - loss: 0.0207 - accuracy: 0.9932\n",
      "Epoch 229/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0231 - accuracy: 0.9924\n",
      "Epoch 229: accuracy did not improve from 0.99495\n",
      "42/42 [==============================] - 4s 97ms/step - loss: 0.0231 - accuracy: 0.9924\n",
      "Epoch 230/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0216 - accuracy: 0.9924\n",
      "Epoch 230: accuracy did not improve from 0.99495\n",
      "42/42 [==============================] - 4s 99ms/step - loss: 0.0216 - accuracy: 0.9924\n",
      "Epoch 231/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0272 - accuracy: 0.9913\n",
      "Epoch 231: accuracy did not improve from 0.99495\n",
      "42/42 [==============================] - 4s 95ms/step - loss: 0.0272 - accuracy: 0.9913\n",
      "Epoch 232/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0183 - accuracy: 0.9944\n",
      "Epoch 232: accuracy did not improve from 0.99495\n",
      "42/42 [==============================] - 4s 94ms/step - loss: 0.0183 - accuracy: 0.9944\n",
      "Epoch 233/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0277 - accuracy: 0.9916\n",
      "Epoch 233: accuracy did not improve from 0.99495\n",
      "42/42 [==============================] - 4s 96ms/step - loss: 0.0277 - accuracy: 0.9916\n",
      "Epoch 234/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0206 - accuracy: 0.9931\n",
      "Epoch 234: accuracy did not improve from 0.99495\n",
      "42/42 [==============================] - 4s 97ms/step - loss: 0.0206 - accuracy: 0.9931\n",
      "Epoch 235/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0201 - accuracy: 0.9928\n",
      "Epoch 235: accuracy did not improve from 0.99495\n",
      "42/42 [==============================] - 4s 95ms/step - loss: 0.0201 - accuracy: 0.9928\n",
      "Epoch 236/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0158 - accuracy: 0.9949\n",
      "Epoch 236: accuracy did not improve from 0.99495\n",
      "42/42 [==============================] - 4s 95ms/step - loss: 0.0158 - accuracy: 0.9949\n",
      "Epoch 237/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0185 - accuracy: 0.9934\n",
      "Epoch 237: accuracy did not improve from 0.99495\n",
      "42/42 [==============================] - 4s 96ms/step - loss: 0.0185 - accuracy: 0.9934\n",
      "Epoch 238/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0241 - accuracy: 0.9924\n",
      "Epoch 238: accuracy did not improve from 0.99495\n",
      "42/42 [==============================] - 4s 96ms/step - loss: 0.0241 - accuracy: 0.9924\n",
      "Epoch 239/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0240 - accuracy: 0.9923\n",
      "Epoch 239: accuracy did not improve from 0.99495\n",
      "42/42 [==============================] - 4s 94ms/step - loss: 0.0240 - accuracy: 0.9923\n",
      "Epoch 240/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0226 - accuracy: 0.9926\n",
      "Epoch 240: accuracy did not improve from 0.99495\n",
      "42/42 [==============================] - 4s 96ms/step - loss: 0.0226 - accuracy: 0.9926\n",
      "Epoch 241/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0257 - accuracy: 0.9913\n",
      "Epoch 241: accuracy did not improve from 0.99495\n",
      "42/42 [==============================] - 4s 98ms/step - loss: 0.0257 - accuracy: 0.9913\n",
      "Epoch 242/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0197 - accuracy: 0.9928\n",
      "Epoch 242: accuracy did not improve from 0.99495\n",
      "42/42 [==============================] - 5s 109ms/step - loss: 0.0197 - accuracy: 0.9928\n",
      "Epoch 243/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0247 - accuracy: 0.9929\n",
      "Epoch 243: accuracy did not improve from 0.99495\n",
      "42/42 [==============================] - 4s 100ms/step - loss: 0.0247 - accuracy: 0.9929\n",
      "Epoch 244/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0261 - accuracy: 0.9922\n",
      "Epoch 244: accuracy did not improve from 0.99495\n",
      "42/42 [==============================] - 4s 94ms/step - loss: 0.0261 - accuracy: 0.9922\n",
      "Epoch 245/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0213 - accuracy: 0.9938\n",
      "Epoch 245: accuracy did not improve from 0.99495\n",
      "42/42 [==============================] - 4s 95ms/step - loss: 0.0213 - accuracy: 0.9938\n",
      "Epoch 246/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0196 - accuracy: 0.9943\n",
      "Epoch 246: accuracy did not improve from 0.99495\n",
      "42/42 [==============================] - 4s 92ms/step - loss: 0.0196 - accuracy: 0.9943\n",
      "Epoch 247/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0148 - accuracy: 0.9960\n",
      "Epoch 247: accuracy improved from 0.99495 to 0.99598, saving model to eq_solver.h5\n",
      "42/42 [==============================] - 4s 94ms/step - loss: 0.0148 - accuracy: 0.9960\n",
      "Epoch 248/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0157 - accuracy: 0.9943\n",
      "Epoch 248: accuracy did not improve from 0.99598\n",
      "42/42 [==============================] - 4s 92ms/step - loss: 0.0157 - accuracy: 0.9943\n",
      "Epoch 249/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0170 - accuracy: 0.9944\n",
      "Epoch 249: accuracy did not improve from 0.99598\n",
      "42/42 [==============================] - 4s 94ms/step - loss: 0.0170 - accuracy: 0.9944\n",
      "Epoch 250/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0183 - accuracy: 0.9933\n",
      "Epoch 250: accuracy did not improve from 0.99598\n",
      "42/42 [==============================] - 4s 93ms/step - loss: 0.0183 - accuracy: 0.9933\n",
      "Epoch 251/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0174 - accuracy: 0.9948\n",
      "Epoch 251: accuracy did not improve from 0.99598\n",
      "42/42 [==============================] - 4s 91ms/step - loss: 0.0174 - accuracy: 0.9948\n",
      "Epoch 252/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0184 - accuracy: 0.9940\n",
      "Epoch 252: accuracy did not improve from 0.99598\n",
      "42/42 [==============================] - 4s 92ms/step - loss: 0.0184 - accuracy: 0.9940\n",
      "Epoch 253/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0162 - accuracy: 0.9953\n",
      "Epoch 253: accuracy did not improve from 0.99598\n",
      "42/42 [==============================] - 4s 92ms/step - loss: 0.0162 - accuracy: 0.9953\n",
      "Epoch 254/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0227 - accuracy: 0.9927\n",
      "Epoch 254: accuracy did not improve from 0.99598\n",
      "42/42 [==============================] - 4s 92ms/step - loss: 0.0227 - accuracy: 0.9927\n",
      "Epoch 255/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0219 - accuracy: 0.9934\n",
      "Epoch 255: accuracy did not improve from 0.99598\n",
      "42/42 [==============================] - 4s 91ms/step - loss: 0.0219 - accuracy: 0.9934\n",
      "Epoch 256/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0194 - accuracy: 0.9938\n",
      "Epoch 256: accuracy did not improve from 0.99598\n",
      "42/42 [==============================] - 4s 94ms/step - loss: 0.0194 - accuracy: 0.9938\n",
      "Epoch 257/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0134 - accuracy: 0.9959\n",
      "Epoch 257: accuracy did not improve from 0.99598\n",
      "42/42 [==============================] - 4s 95ms/step - loss: 0.0134 - accuracy: 0.9959\n",
      "Epoch 258/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0215 - accuracy: 0.9934\n",
      "Epoch 258: accuracy did not improve from 0.99598\n",
      "42/42 [==============================] - 4s 95ms/step - loss: 0.0215 - accuracy: 0.9934\n",
      "Epoch 259/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0221 - accuracy: 0.9936\n",
      "Epoch 259: accuracy did not improve from 0.99598\n",
      "42/42 [==============================] - 4s 94ms/step - loss: 0.0221 - accuracy: 0.9936\n",
      "Epoch 260/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0197 - accuracy: 0.9934\n",
      "Epoch 260: accuracy did not improve from 0.99598\n",
      "42/42 [==============================] - 4s 92ms/step - loss: 0.0197 - accuracy: 0.9934\n",
      "Epoch 261/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0127 - accuracy: 0.9963\n",
      "Epoch 261: accuracy improved from 0.99598 to 0.99626, saving model to eq_solver.h5\n",
      "42/42 [==============================] - 4s 94ms/step - loss: 0.0127 - accuracy: 0.9963\n",
      "Epoch 262/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0156 - accuracy: 0.9946\n",
      "Epoch 262: accuracy did not improve from 0.99626\n",
      "42/42 [==============================] - 4s 95ms/step - loss: 0.0156 - accuracy: 0.9946\n",
      "Epoch 263/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0127 - accuracy: 0.9953\n",
      "Epoch 263: accuracy did not improve from 0.99626\n",
      "42/42 [==============================] - 4s 92ms/step - loss: 0.0127 - accuracy: 0.9953\n",
      "Epoch 264/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0138 - accuracy: 0.9951\n",
      "Epoch 264: accuracy did not improve from 0.99626\n",
      "42/42 [==============================] - 4s 93ms/step - loss: 0.0138 - accuracy: 0.9951\n",
      "Epoch 265/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0219 - accuracy: 0.9933\n",
      "Epoch 265: accuracy did not improve from 0.99626\n",
      "42/42 [==============================] - 4s 97ms/step - loss: 0.0219 - accuracy: 0.9933\n",
      "Epoch 266/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0164 - accuracy: 0.9948\n",
      "Epoch 266: accuracy did not improve from 0.99626\n",
      "42/42 [==============================] - 4s 94ms/step - loss: 0.0164 - accuracy: 0.9948\n",
      "Epoch 267/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0179 - accuracy: 0.9945\n",
      "Epoch 267: accuracy did not improve from 0.99626\n",
      "42/42 [==============================] - 4s 91ms/step - loss: 0.0179 - accuracy: 0.9945\n",
      "Epoch 268/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0153 - accuracy: 0.9953\n",
      "Epoch 268: accuracy did not improve from 0.99626\n",
      "42/42 [==============================] - 4s 92ms/step - loss: 0.0153 - accuracy: 0.9953\n",
      "Epoch 269/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0189 - accuracy: 0.9944\n",
      "Epoch 269: accuracy did not improve from 0.99626\n",
      "42/42 [==============================] - 4s 93ms/step - loss: 0.0189 - accuracy: 0.9944\n",
      "Epoch 270/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0195 - accuracy: 0.9940\n",
      "Epoch 270: accuracy did not improve from 0.99626\n",
      "42/42 [==============================] - 4s 93ms/step - loss: 0.0195 - accuracy: 0.9940\n",
      "Epoch 271/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0158 - accuracy: 0.9946\n",
      "Epoch 271: accuracy did not improve from 0.99626\n",
      "42/42 [==============================] - 4s 92ms/step - loss: 0.0158 - accuracy: 0.9946\n",
      "Epoch 272/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0144 - accuracy: 0.9958\n",
      "Epoch 272: accuracy did not improve from 0.99626\n",
      "42/42 [==============================] - 4s 91ms/step - loss: 0.0144 - accuracy: 0.9958\n",
      "Epoch 273/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0168 - accuracy: 0.9947\n",
      "Epoch 273: accuracy did not improve from 0.99626\n",
      "42/42 [==============================] - 4s 94ms/step - loss: 0.0168 - accuracy: 0.9947\n",
      "Epoch 274/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0148 - accuracy: 0.9939\n",
      "Epoch 274: accuracy did not improve from 0.99626\n",
      "42/42 [==============================] - 4s 97ms/step - loss: 0.0148 - accuracy: 0.9939\n",
      "Epoch 275/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0167 - accuracy: 0.9939\n",
      "Epoch 275: accuracy did not improve from 0.99626\n",
      "42/42 [==============================] - 4s 92ms/step - loss: 0.0167 - accuracy: 0.9939\n",
      "Epoch 276/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0197 - accuracy: 0.9927\n",
      "Epoch 276: accuracy did not improve from 0.99626\n",
      "42/42 [==============================] - 4s 91ms/step - loss: 0.0197 - accuracy: 0.9927\n",
      "Epoch 277/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0149 - accuracy: 0.9947\n",
      "Epoch 277: accuracy did not improve from 0.99626\n",
      "42/42 [==============================] - 4s 93ms/step - loss: 0.0149 - accuracy: 0.9947\n",
      "Epoch 278/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0175 - accuracy: 0.9947\n",
      "Epoch 278: accuracy did not improve from 0.99626\n",
      "42/42 [==============================] - 4s 96ms/step - loss: 0.0175 - accuracy: 0.9947\n",
      "Epoch 279/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0138 - accuracy: 0.9955\n",
      "Epoch 279: accuracy did not improve from 0.99626\n",
      "42/42 [==============================] - 4s 93ms/step - loss: 0.0138 - accuracy: 0.9955\n",
      "Epoch 280/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0147 - accuracy: 0.9953\n",
      "Epoch 280: accuracy did not improve from 0.99626\n",
      "42/42 [==============================] - 4s 93ms/step - loss: 0.0147 - accuracy: 0.9953\n",
      "Epoch 281/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0247 - accuracy: 0.9930\n",
      "Epoch 281: accuracy did not improve from 0.99626\n",
      "42/42 [==============================] - 4s 93ms/step - loss: 0.0247 - accuracy: 0.9930\n",
      "Epoch 282/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0195 - accuracy: 0.9939\n",
      "Epoch 282: accuracy did not improve from 0.99626\n",
      "42/42 [==============================] - 4s 95ms/step - loss: 0.0195 - accuracy: 0.9939\n",
      "Epoch 283/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0230 - accuracy: 0.9937\n",
      "Epoch 283: accuracy did not improve from 0.99626\n",
      "42/42 [==============================] - 4s 92ms/step - loss: 0.0230 - accuracy: 0.9937\n",
      "Epoch 284/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0207 - accuracy: 0.9947\n",
      "Epoch 284: accuracy did not improve from 0.99626\n",
      "42/42 [==============================] - 4s 93ms/step - loss: 0.0207 - accuracy: 0.9947\n",
      "Epoch 285/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0217 - accuracy: 0.9933\n",
      "Epoch 285: accuracy did not improve from 0.99626\n",
      "42/42 [==============================] - 4s 93ms/step - loss: 0.0217 - accuracy: 0.9933\n",
      "Epoch 286/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0243 - accuracy: 0.9933\n",
      "Epoch 286: accuracy did not improve from 0.99626\n",
      "42/42 [==============================] - 4s 92ms/step - loss: 0.0243 - accuracy: 0.9933\n",
      "Epoch 287/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0134 - accuracy: 0.9952\n",
      "Epoch 287: accuracy did not improve from 0.99626\n",
      "42/42 [==============================] - 4s 94ms/step - loss: 0.0134 - accuracy: 0.9952\n",
      "Epoch 288/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0141 - accuracy: 0.9949\n",
      "Epoch 288: accuracy did not improve from 0.99626\n",
      "42/42 [==============================] - 4s 93ms/step - loss: 0.0141 - accuracy: 0.9949\n",
      "Epoch 289/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0169 - accuracy: 0.9942\n",
      "Epoch 289: accuracy did not improve from 0.99626\n",
      "42/42 [==============================] - 4s 92ms/step - loss: 0.0169 - accuracy: 0.9942\n",
      "Epoch 290/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0156 - accuracy: 0.9945\n",
      "Epoch 290: accuracy did not improve from 0.99626\n",
      "42/42 [==============================] - 4s 93ms/step - loss: 0.0156 - accuracy: 0.9945\n",
      "Epoch 291/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0177 - accuracy: 0.9944\n",
      "Epoch 291: accuracy did not improve from 0.99626\n",
      "42/42 [==============================] - 4s 92ms/step - loss: 0.0177 - accuracy: 0.9944\n",
      "Epoch 292/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0283 - accuracy: 0.9916\n",
      "Epoch 292: accuracy did not improve from 0.99626\n",
      "42/42 [==============================] - 4s 91ms/step - loss: 0.0283 - accuracy: 0.9916\n",
      "Epoch 293/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0214 - accuracy: 0.9929\n",
      "Epoch 293: accuracy did not improve from 0.99626\n",
      "42/42 [==============================] - 4s 92ms/step - loss: 0.0214 - accuracy: 0.9929\n",
      "Epoch 294/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0173 - accuracy: 0.9959\n",
      "Epoch 294: accuracy did not improve from 0.99626\n",
      "42/42 [==============================] - 4s 91ms/step - loss: 0.0173 - accuracy: 0.9959\n",
      "Epoch 295/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0177 - accuracy: 0.9942\n",
      "Epoch 295: accuracy did not improve from 0.99626\n",
      "42/42 [==============================] - 4s 91ms/step - loss: 0.0177 - accuracy: 0.9942\n",
      "Epoch 296/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0161 - accuracy: 0.9945\n",
      "Epoch 296: accuracy did not improve from 0.99626\n",
      "42/42 [==============================] - 4s 93ms/step - loss: 0.0161 - accuracy: 0.9945\n",
      "Epoch 297/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0164 - accuracy: 0.9950\n",
      "Epoch 297: accuracy did not improve from 0.99626\n",
      "42/42 [==============================] - 4s 95ms/step - loss: 0.0164 - accuracy: 0.9950\n",
      "Epoch 298/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0189 - accuracy: 0.9941\n",
      "Epoch 298: accuracy did not improve from 0.99626\n",
      "42/42 [==============================] - 4s 92ms/step - loss: 0.0189 - accuracy: 0.9941\n",
      "Epoch 299/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0212 - accuracy: 0.9929\n",
      "Epoch 299: accuracy did not improve from 0.99626\n",
      "42/42 [==============================] - 4s 92ms/step - loss: 0.0212 - accuracy: 0.9929\n",
      "Epoch 300/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0188 - accuracy: 0.9945\n",
      "Epoch 300: accuracy did not improve from 0.99626\n",
      "42/42 [==============================] - 4s 92ms/step - loss: 0.0188 - accuracy: 0.9945\n",
      "Epoch 301/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0177 - accuracy: 0.9955\n",
      "Epoch 301: accuracy did not improve from 0.99626\n",
      "42/42 [==============================] - 4s 91ms/step - loss: 0.0177 - accuracy: 0.9955\n",
      "Epoch 302/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0201 - accuracy: 0.9947\n",
      "Epoch 302: accuracy did not improve from 0.99626\n",
      "42/42 [==============================] - 4s 92ms/step - loss: 0.0201 - accuracy: 0.9947\n",
      "Epoch 303/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0226 - accuracy: 0.9936\n",
      "Epoch 303: accuracy did not improve from 0.99626\n",
      "42/42 [==============================] - 4s 95ms/step - loss: 0.0226 - accuracy: 0.9936\n",
      "Epoch 304/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0204 - accuracy: 0.9933\n",
      "Epoch 304: accuracy did not improve from 0.99626\n",
      "42/42 [==============================] - 4s 91ms/step - loss: 0.0204 - accuracy: 0.9933\n",
      "Epoch 305/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0240 - accuracy: 0.9926\n",
      "Epoch 305: accuracy did not improve from 0.99626\n",
      "42/42 [==============================] - 4s 92ms/step - loss: 0.0240 - accuracy: 0.9926\n",
      "Epoch 306/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0161 - accuracy: 0.9949\n",
      "Epoch 306: accuracy did not improve from 0.99626\n",
      "42/42 [==============================] - 4s 91ms/step - loss: 0.0161 - accuracy: 0.9949\n",
      "Epoch 307/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0141 - accuracy: 0.9953\n",
      "Epoch 307: accuracy did not improve from 0.99626\n",
      "42/42 [==============================] - 4s 94ms/step - loss: 0.0141 - accuracy: 0.9953\n",
      "Epoch 308/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0143 - accuracy: 0.9948\n",
      "Epoch 308: accuracy did not improve from 0.99626\n",
      "42/42 [==============================] - 4s 93ms/step - loss: 0.0143 - accuracy: 0.9948\n",
      "Epoch 309/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0184 - accuracy: 0.9940\n",
      "Epoch 309: accuracy did not improve from 0.99626\n",
      "42/42 [==============================] - 4s 93ms/step - loss: 0.0184 - accuracy: 0.9940\n",
      "Epoch 310/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0221 - accuracy: 0.9934\n",
      "Epoch 310: accuracy did not improve from 0.99626\n",
      "42/42 [==============================] - 4s 96ms/step - loss: 0.0221 - accuracy: 0.9934\n",
      "Epoch 311/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0166 - accuracy: 0.9942\n",
      "Epoch 311: accuracy did not improve from 0.99626\n",
      "42/42 [==============================] - 4s 92ms/step - loss: 0.0166 - accuracy: 0.9942\n",
      "Epoch 312/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0140 - accuracy: 0.9953\n",
      "Epoch 312: accuracy did not improve from 0.99626\n",
      "42/42 [==============================] - 4s 104ms/step - loss: 0.0140 - accuracy: 0.9953\n",
      "Epoch 313/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0186 - accuracy: 0.9940\n",
      "Epoch 313: accuracy did not improve from 0.99626\n",
      "42/42 [==============================] - 5s 121ms/step - loss: 0.0186 - accuracy: 0.9940\n",
      "Epoch 314/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0142 - accuracy: 0.9955\n",
      "Epoch 314: accuracy did not improve from 0.99626\n",
      "42/42 [==============================] - 5s 112ms/step - loss: 0.0142 - accuracy: 0.9955\n",
      "Epoch 315/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0109 - accuracy: 0.9969\n",
      "Epoch 315: accuracy improved from 0.99626 to 0.99691, saving model to eq_solver.h5\n",
      "42/42 [==============================] - 5s 109ms/step - loss: 0.0109 - accuracy: 0.9969\n",
      "Epoch 316/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0150 - accuracy: 0.9950\n",
      "Epoch 316: accuracy did not improve from 0.99691\n",
      "42/42 [==============================] - 4s 96ms/step - loss: 0.0150 - accuracy: 0.9950\n",
      "Epoch 317/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0152 - accuracy: 0.9951\n",
      "Epoch 317: accuracy did not improve from 0.99691\n",
      "42/42 [==============================] - 4s 97ms/step - loss: 0.0152 - accuracy: 0.9951\n",
      "Epoch 318/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0122 - accuracy: 0.9957\n",
      "Epoch 318: accuracy did not improve from 0.99691\n",
      "42/42 [==============================] - 4s 97ms/step - loss: 0.0122 - accuracy: 0.9957\n",
      "Epoch 319/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0156 - accuracy: 0.9947\n",
      "Epoch 319: accuracy did not improve from 0.99691\n",
      "42/42 [==============================] - 4s 94ms/step - loss: 0.0156 - accuracy: 0.9947\n",
      "Epoch 320/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0129 - accuracy: 0.9960\n",
      "Epoch 320: accuracy did not improve from 0.99691\n",
      "42/42 [==============================] - 4s 96ms/step - loss: 0.0129 - accuracy: 0.9960\n",
      "Epoch 321/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0150 - accuracy: 0.9951\n",
      "Epoch 321: accuracy did not improve from 0.99691\n",
      "42/42 [==============================] - 4s 96ms/step - loss: 0.0150 - accuracy: 0.9951\n",
      "Epoch 322/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0136 - accuracy: 0.9953\n",
      "Epoch 322: accuracy did not improve from 0.99691\n",
      "42/42 [==============================] - 4s 95ms/step - loss: 0.0136 - accuracy: 0.9953\n",
      "Epoch 323/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0126 - accuracy: 0.9961\n",
      "Epoch 323: accuracy did not improve from 0.99691\n",
      "42/42 [==============================] - 4s 96ms/step - loss: 0.0126 - accuracy: 0.9961\n",
      "Epoch 324/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0153 - accuracy: 0.9953\n",
      "Epoch 324: accuracy did not improve from 0.99691\n",
      "42/42 [==============================] - 4s 94ms/step - loss: 0.0153 - accuracy: 0.9953\n",
      "Epoch 325/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0155 - accuracy: 0.9956\n",
      "Epoch 325: accuracy did not improve from 0.99691\n",
      "42/42 [==============================] - 4s 98ms/step - loss: 0.0155 - accuracy: 0.9956\n",
      "Epoch 326/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0163 - accuracy: 0.9952\n",
      "Epoch 326: accuracy did not improve from 0.99691\n",
      "42/42 [==============================] - 4s 97ms/step - loss: 0.0163 - accuracy: 0.9952\n",
      "Epoch 327/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0162 - accuracy: 0.9953\n",
      "Epoch 327: accuracy did not improve from 0.99691\n",
      "42/42 [==============================] - 4s 97ms/step - loss: 0.0162 - accuracy: 0.9953\n",
      "Epoch 328/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0175 - accuracy: 0.9950\n",
      "Epoch 328: accuracy did not improve from 0.99691\n",
      "42/42 [==============================] - 4s 100ms/step - loss: 0.0175 - accuracy: 0.9950\n",
      "Epoch 329/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0188 - accuracy: 0.9946\n",
      "Epoch 329: accuracy did not improve from 0.99691\n",
      "42/42 [==============================] - 4s 97ms/step - loss: 0.0188 - accuracy: 0.9946\n",
      "Epoch 330/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0159 - accuracy: 0.9942\n",
      "Epoch 330: accuracy did not improve from 0.99691\n",
      "42/42 [==============================] - 4s 96ms/step - loss: 0.0159 - accuracy: 0.9942\n",
      "Epoch 331/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0161 - accuracy: 0.9949\n",
      "Epoch 331: accuracy did not improve from 0.99691\n",
      "42/42 [==============================] - 4s 98ms/step - loss: 0.0161 - accuracy: 0.9949\n",
      "Epoch 332/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0133 - accuracy: 0.9958\n",
      "Epoch 332: accuracy did not improve from 0.99691\n",
      "42/42 [==============================] - 4s 96ms/step - loss: 0.0133 - accuracy: 0.9958\n",
      "Epoch 333/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0169 - accuracy: 0.9949\n",
      "Epoch 333: accuracy did not improve from 0.99691\n",
      "42/42 [==============================] - 4s 95ms/step - loss: 0.0169 - accuracy: 0.9949\n",
      "Epoch 334/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0179 - accuracy: 0.9953\n",
      "Epoch 334: accuracy did not improve from 0.99691\n",
      "42/42 [==============================] - 4s 96ms/step - loss: 0.0179 - accuracy: 0.9953\n",
      "Epoch 335/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0155 - accuracy: 0.9952\n",
      "Epoch 335: accuracy did not improve from 0.99691\n",
      "42/42 [==============================] - 4s 97ms/step - loss: 0.0155 - accuracy: 0.9952\n",
      "Epoch 336/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0156 - accuracy: 0.9948\n",
      "Epoch 336: accuracy did not improve from 0.99691\n",
      "42/42 [==============================] - 4s 95ms/step - loss: 0.0156 - accuracy: 0.9948\n",
      "Epoch 337/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0118 - accuracy: 0.9955\n",
      "Epoch 337: accuracy did not improve from 0.99691\n",
      "42/42 [==============================] - 4s 94ms/step - loss: 0.0118 - accuracy: 0.9955\n",
      "Epoch 338/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0229 - accuracy: 0.9942\n",
      "Epoch 338: accuracy did not improve from 0.99691\n",
      "42/42 [==============================] - 4s 95ms/step - loss: 0.0229 - accuracy: 0.9942\n",
      "Epoch 339/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0142 - accuracy: 0.9950\n",
      "Epoch 339: accuracy did not improve from 0.99691\n",
      "42/42 [==============================] - 4s 94ms/step - loss: 0.0142 - accuracy: 0.9950\n",
      "Epoch 340/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0132 - accuracy: 0.9953\n",
      "Epoch 340: accuracy did not improve from 0.99691\n",
      "42/42 [==============================] - 4s 98ms/step - loss: 0.0132 - accuracy: 0.9953\n",
      "Epoch 341/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0209 - accuracy: 0.9932\n",
      "Epoch 341: accuracy did not improve from 0.99691\n",
      "42/42 [==============================] - 4s 97ms/step - loss: 0.0209 - accuracy: 0.9932\n",
      "Epoch 342/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0268 - accuracy: 0.9922\n",
      "Epoch 342: accuracy did not improve from 0.99691\n",
      "42/42 [==============================] - 4s 96ms/step - loss: 0.0268 - accuracy: 0.9922\n",
      "Epoch 343/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0196 - accuracy: 0.9944\n",
      "Epoch 343: accuracy did not improve from 0.99691\n",
      "42/42 [==============================] - 4s 97ms/step - loss: 0.0196 - accuracy: 0.9944\n",
      "Epoch 344/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0242 - accuracy: 0.9932\n",
      "Epoch 344: accuracy did not improve from 0.99691\n",
      "42/42 [==============================] - 4s 97ms/step - loss: 0.0242 - accuracy: 0.9932\n",
      "Epoch 345/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0178 - accuracy: 0.9946\n",
      "Epoch 345: accuracy did not improve from 0.99691\n",
      "42/42 [==============================] - 4s 95ms/step - loss: 0.0178 - accuracy: 0.9946\n",
      "Epoch 346/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0177 - accuracy: 0.9949\n",
      "Epoch 346: accuracy did not improve from 0.99691\n",
      "42/42 [==============================] - 4s 95ms/step - loss: 0.0177 - accuracy: 0.9949\n",
      "Epoch 347/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0082 - accuracy: 0.9975\n",
      "Epoch 347: accuracy improved from 0.99691 to 0.99747, saving model to eq_solver.h5\n",
      "42/42 [==============================] - 4s 96ms/step - loss: 0.0082 - accuracy: 0.9975\n",
      "Epoch 348/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0162 - accuracy: 0.9946\n",
      "Epoch 348: accuracy did not improve from 0.99747\n",
      "42/42 [==============================] - 4s 97ms/step - loss: 0.0162 - accuracy: 0.9946\n",
      "Epoch 349/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0181 - accuracy: 0.9948\n",
      "Epoch 349: accuracy did not improve from 0.99747\n",
      "42/42 [==============================] - 4s 94ms/step - loss: 0.0181 - accuracy: 0.9948\n",
      "Epoch 350/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0163 - accuracy: 0.9948\n",
      "Epoch 350: accuracy did not improve from 0.99747\n",
      "42/42 [==============================] - 4s 97ms/step - loss: 0.0163 - accuracy: 0.9948\n",
      "Epoch 351/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0174 - accuracy: 0.9950\n",
      "Epoch 351: accuracy did not improve from 0.99747\n",
      "42/42 [==============================] - 4s 96ms/step - loss: 0.0174 - accuracy: 0.9950\n",
      "Epoch 352/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0136 - accuracy: 0.9955\n",
      "Epoch 352: accuracy did not improve from 0.99747\n",
      "42/42 [==============================] - 4s 95ms/step - loss: 0.0136 - accuracy: 0.9955\n",
      "Epoch 353/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0174 - accuracy: 0.9943\n",
      "Epoch 353: accuracy did not improve from 0.99747\n",
      "42/42 [==============================] - 4s 95ms/step - loss: 0.0174 - accuracy: 0.9943\n",
      "Epoch 354/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0211 - accuracy: 0.9939\n",
      "Epoch 354: accuracy did not improve from 0.99747\n",
      "42/42 [==============================] - 4s 96ms/step - loss: 0.0211 - accuracy: 0.9939\n",
      "Epoch 355/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0170 - accuracy: 0.9948\n",
      "Epoch 355: accuracy did not improve from 0.99747\n",
      "42/42 [==============================] - 4s 97ms/step - loss: 0.0170 - accuracy: 0.9948\n",
      "Epoch 356/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0135 - accuracy: 0.9955\n",
      "Epoch 356: accuracy did not improve from 0.99747\n",
      "42/42 [==============================] - 4s 95ms/step - loss: 0.0135 - accuracy: 0.9955\n",
      "Epoch 357/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0150 - accuracy: 0.9967\n",
      "Epoch 357: accuracy did not improve from 0.99747\n",
      "42/42 [==============================] - 4s 93ms/step - loss: 0.0150 - accuracy: 0.9967\n",
      "Epoch 358/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0160 - accuracy: 0.9952\n",
      "Epoch 358: accuracy did not improve from 0.99747\n",
      "42/42 [==============================] - 4s 94ms/step - loss: 0.0160 - accuracy: 0.9952\n",
      "Epoch 359/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0144 - accuracy: 0.9958\n",
      "Epoch 359: accuracy did not improve from 0.99747\n",
      "42/42 [==============================] - 4s 94ms/step - loss: 0.0144 - accuracy: 0.9958\n",
      "Epoch 360/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0127 - accuracy: 0.9961\n",
      "Epoch 360: accuracy did not improve from 0.99747\n",
      "42/42 [==============================] - 4s 96ms/step - loss: 0.0127 - accuracy: 0.9961\n",
      "Epoch 361/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0182 - accuracy: 0.9946\n",
      "Epoch 361: accuracy did not improve from 0.99747\n",
      "42/42 [==============================] - 4s 97ms/step - loss: 0.0182 - accuracy: 0.9946\n",
      "Epoch 362/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0144 - accuracy: 0.9960\n",
      "Epoch 362: accuracy did not improve from 0.99747\n",
      "42/42 [==============================] - 4s 97ms/step - loss: 0.0144 - accuracy: 0.9960\n",
      "Epoch 363/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0146 - accuracy: 0.9956\n",
      "Epoch 363: accuracy did not improve from 0.99747\n",
      "42/42 [==============================] - 4s 95ms/step - loss: 0.0146 - accuracy: 0.9956\n",
      "Epoch 364/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0134 - accuracy: 0.9958\n",
      "Epoch 364: accuracy did not improve from 0.99747\n",
      "42/42 [==============================] - 4s 96ms/step - loss: 0.0134 - accuracy: 0.9958\n",
      "Epoch 365/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0131 - accuracy: 0.9960\n",
      "Epoch 365: accuracy did not improve from 0.99747\n",
      "42/42 [==============================] - 4s 94ms/step - loss: 0.0131 - accuracy: 0.9960\n",
      "Epoch 366/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0121 - accuracy: 0.9960\n",
      "Epoch 366: accuracy did not improve from 0.99747\n",
      "42/42 [==============================] - 4s 97ms/step - loss: 0.0121 - accuracy: 0.9960\n",
      "Epoch 367/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0126 - accuracy: 0.9964\n",
      "Epoch 367: accuracy did not improve from 0.99747\n",
      "42/42 [==============================] - 4s 98ms/step - loss: 0.0126 - accuracy: 0.9964\n",
      "Epoch 368/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0136 - accuracy: 0.9953\n",
      "Epoch 368: accuracy did not improve from 0.99747\n",
      "42/42 [==============================] - 4s 95ms/step - loss: 0.0136 - accuracy: 0.9953\n",
      "Epoch 369/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0146 - accuracy: 0.9949\n",
      "Epoch 369: accuracy did not improve from 0.99747\n",
      "42/42 [==============================] - 4s 95ms/step - loss: 0.0146 - accuracy: 0.9949\n",
      "Epoch 370/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0223 - accuracy: 0.9941\n",
      "Epoch 370: accuracy did not improve from 0.99747\n",
      "42/42 [==============================] - 4s 95ms/step - loss: 0.0223 - accuracy: 0.9941\n",
      "Epoch 371/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0186 - accuracy: 0.9942\n",
      "Epoch 371: accuracy did not improve from 0.99747\n",
      "42/42 [==============================] - 4s 96ms/step - loss: 0.0186 - accuracy: 0.9942\n",
      "Epoch 372/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0170 - accuracy: 0.9950\n",
      "Epoch 372: accuracy did not improve from 0.99747\n",
      "42/42 [==============================] - 4s 96ms/step - loss: 0.0170 - accuracy: 0.9950\n",
      "Epoch 373/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0120 - accuracy: 0.9959\n",
      "Epoch 373: accuracy did not improve from 0.99747\n",
      "42/42 [==============================] - 4s 96ms/step - loss: 0.0120 - accuracy: 0.9959\n",
      "Epoch 374/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0134 - accuracy: 0.9953\n",
      "Epoch 374: accuracy did not improve from 0.99747\n",
      "42/42 [==============================] - 4s 95ms/step - loss: 0.0134 - accuracy: 0.9953\n",
      "Epoch 375/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0131 - accuracy: 0.9964\n",
      "Epoch 375: accuracy did not improve from 0.99747\n",
      "42/42 [==============================] - 4s 95ms/step - loss: 0.0131 - accuracy: 0.9964\n",
      "Epoch 376/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0145 - accuracy: 0.9958\n",
      "Epoch 376: accuracy did not improve from 0.99747\n",
      "42/42 [==============================] - 4s 96ms/step - loss: 0.0145 - accuracy: 0.9958\n",
      "Epoch 377/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0133 - accuracy: 0.9960\n",
      "Epoch 377: accuracy did not improve from 0.99747\n",
      "42/42 [==============================] - 4s 96ms/step - loss: 0.0133 - accuracy: 0.9960\n",
      "Epoch 378/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0101 - accuracy: 0.9963\n",
      "Epoch 378: accuracy did not improve from 0.99747\n",
      "42/42 [==============================] - 5s 108ms/step - loss: 0.0101 - accuracy: 0.9963\n",
      "Epoch 379/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0086 - accuracy: 0.9970\n",
      "Epoch 379: accuracy did not improve from 0.99747\n",
      "42/42 [==============================] - 4s 95ms/step - loss: 0.0086 - accuracy: 0.9970\n",
      "Epoch 380/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0109 - accuracy: 0.9966\n",
      "Epoch 380: accuracy did not improve from 0.99747\n",
      "42/42 [==============================] - 4s 95ms/step - loss: 0.0109 - accuracy: 0.9966\n",
      "Epoch 381/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0132 - accuracy: 0.9958\n",
      "Epoch 381: accuracy did not improve from 0.99747\n",
      "42/42 [==============================] - 4s 96ms/step - loss: 0.0132 - accuracy: 0.9958\n",
      "Epoch 382/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0120 - accuracy: 0.9957\n",
      "Epoch 382: accuracy did not improve from 0.99747\n",
      "42/42 [==============================] - 4s 93ms/step - loss: 0.0120 - accuracy: 0.9957\n",
      "Epoch 383/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0130 - accuracy: 0.9960\n",
      "Epoch 383: accuracy did not improve from 0.99747\n",
      "42/42 [==============================] - 4s 95ms/step - loss: 0.0130 - accuracy: 0.9960\n",
      "Epoch 384/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0131 - accuracy: 0.9961\n",
      "Epoch 384: accuracy did not improve from 0.99747\n",
      "42/42 [==============================] - 4s 98ms/step - loss: 0.0131 - accuracy: 0.9961\n",
      "Epoch 385/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0142 - accuracy: 0.9955\n",
      "Epoch 385: accuracy did not improve from 0.99747\n",
      "42/42 [==============================] - 4s 96ms/step - loss: 0.0142 - accuracy: 0.9955\n",
      "Epoch 386/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0110 - accuracy: 0.9964\n",
      "Epoch 386: accuracy did not improve from 0.99747\n",
      "42/42 [==============================] - 4s 95ms/step - loss: 0.0110 - accuracy: 0.9964\n",
      "Epoch 387/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0100 - accuracy: 0.9964\n",
      "Epoch 387: accuracy did not improve from 0.99747\n",
      "42/42 [==============================] - 4s 93ms/step - loss: 0.0100 - accuracy: 0.9964\n",
      "Epoch 388/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0133 - accuracy: 0.9961\n",
      "Epoch 388: accuracy did not improve from 0.99747\n",
      "42/42 [==============================] - 4s 94ms/step - loss: 0.0133 - accuracy: 0.9961\n",
      "Epoch 389/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0122 - accuracy: 0.9957\n",
      "Epoch 389: accuracy did not improve from 0.99747\n",
      "42/42 [==============================] - 4s 94ms/step - loss: 0.0122 - accuracy: 0.9957\n",
      "Epoch 390/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0149 - accuracy: 0.9959\n",
      "Epoch 390: accuracy did not improve from 0.99747\n",
      "42/42 [==============================] - 4s 93ms/step - loss: 0.0149 - accuracy: 0.9959\n",
      "Epoch 391/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0089 - accuracy: 0.9970\n",
      "Epoch 391: accuracy did not improve from 0.99747\n",
      "42/42 [==============================] - 4s 95ms/step - loss: 0.0089 - accuracy: 0.9970\n",
      "Epoch 392/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0117 - accuracy: 0.9964\n",
      "Epoch 392: accuracy did not improve from 0.99747\n",
      "42/42 [==============================] - 4s 95ms/step - loss: 0.0117 - accuracy: 0.9964\n",
      "Epoch 393/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0118 - accuracy: 0.9960\n",
      "Epoch 393: accuracy did not improve from 0.99747\n",
      "42/42 [==============================] - 4s 94ms/step - loss: 0.0118 - accuracy: 0.9960\n",
      "Epoch 394/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0143 - accuracy: 0.9964\n",
      "Epoch 394: accuracy did not improve from 0.99747\n",
      "42/42 [==============================] - 4s 92ms/step - loss: 0.0143 - accuracy: 0.9964\n",
      "Epoch 395/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0105 - accuracy: 0.9968\n",
      "Epoch 395: accuracy did not improve from 0.99747\n",
      "42/42 [==============================] - 4s 95ms/step - loss: 0.0105 - accuracy: 0.9968\n",
      "Epoch 396/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0125 - accuracy: 0.9959\n",
      "Epoch 396: accuracy did not improve from 0.99747\n",
      "42/42 [==============================] - 4s 98ms/step - loss: 0.0125 - accuracy: 0.9959\n",
      "Epoch 397/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0135 - accuracy: 0.9959\n",
      "Epoch 397: accuracy did not improve from 0.99747\n",
      "42/42 [==============================] - 4s 95ms/step - loss: 0.0135 - accuracy: 0.9959\n",
      "Epoch 398/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0169 - accuracy: 0.9949\n",
      "Epoch 398: accuracy did not improve from 0.99747\n",
      "42/42 [==============================] - 4s 94ms/step - loss: 0.0169 - accuracy: 0.9949\n",
      "Epoch 399/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0112 - accuracy: 0.9962\n",
      "Epoch 399: accuracy did not improve from 0.99747\n",
      "42/42 [==============================] - 4s 94ms/step - loss: 0.0112 - accuracy: 0.9962\n",
      "Epoch 400/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0129 - accuracy: 0.9969\n",
      "Epoch 400: accuracy did not improve from 0.99747\n",
      "42/42 [==============================] - 4s 93ms/step - loss: 0.0129 - accuracy: 0.9969\n",
      "Epoch 401/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0136 - accuracy: 0.9959\n",
      "Epoch 401: accuracy did not improve from 0.99747\n",
      "42/42 [==============================] - 4s 96ms/step - loss: 0.0136 - accuracy: 0.9959\n",
      "Epoch 402/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0165 - accuracy: 0.9950\n",
      "Epoch 402: accuracy did not improve from 0.99747\n",
      "42/42 [==============================] - 4s 96ms/step - loss: 0.0165 - accuracy: 0.9950\n",
      "Epoch 403/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0133 - accuracy: 0.9959\n",
      "Epoch 403: accuracy did not improve from 0.99747\n",
      "42/42 [==============================] - 4s 99ms/step - loss: 0.0133 - accuracy: 0.9959\n",
      "Epoch 404/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0152 - accuracy: 0.9954\n",
      "Epoch 404: accuracy did not improve from 0.99747\n",
      "42/42 [==============================] - 4s 96ms/step - loss: 0.0152 - accuracy: 0.9954\n",
      "Epoch 405/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0110 - accuracy: 0.9963\n",
      "Epoch 405: accuracy did not improve from 0.99747\n",
      "42/42 [==============================] - 4s 94ms/step - loss: 0.0110 - accuracy: 0.9963\n",
      "Epoch 406/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0150 - accuracy: 0.9959\n",
      "Epoch 406: accuracy did not improve from 0.99747\n",
      "42/42 [==============================] - 4s 93ms/step - loss: 0.0150 - accuracy: 0.9959\n",
      "Epoch 407/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0091 - accuracy: 0.9974\n",
      "Epoch 407: accuracy did not improve from 0.99747\n",
      "42/42 [==============================] - 4s 96ms/step - loss: 0.0091 - accuracy: 0.9974\n",
      "Epoch 408/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0147 - accuracy: 0.9950\n",
      "Epoch 408: accuracy did not improve from 0.99747\n",
      "42/42 [==============================] - 4s 95ms/step - loss: 0.0147 - accuracy: 0.9950\n",
      "Epoch 409/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0157 - accuracy: 0.9959\n",
      "Epoch 409: accuracy did not improve from 0.99747\n",
      "42/42 [==============================] - 4s 95ms/step - loss: 0.0157 - accuracy: 0.9959\n",
      "Epoch 410/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0118 - accuracy: 0.9961\n",
      "Epoch 410: accuracy did not improve from 0.99747\n",
      "42/42 [==============================] - 4s 94ms/step - loss: 0.0118 - accuracy: 0.9961\n",
      "Epoch 411/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0154 - accuracy: 0.9959\n",
      "Epoch 411: accuracy did not improve from 0.99747\n",
      "42/42 [==============================] - 4s 94ms/step - loss: 0.0154 - accuracy: 0.9959\n",
      "Epoch 412/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0150 - accuracy: 0.9957\n",
      "Epoch 412: accuracy did not improve from 0.99747\n",
      "42/42 [==============================] - 4s 97ms/step - loss: 0.0150 - accuracy: 0.9957\n",
      "Epoch 413/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0189 - accuracy: 0.9949\n",
      "Epoch 413: accuracy did not improve from 0.99747\n",
      "42/42 [==============================] - 4s 96ms/step - loss: 0.0189 - accuracy: 0.9949\n",
      "Epoch 414/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0154 - accuracy: 0.9952\n",
      "Epoch 414: accuracy did not improve from 0.99747\n",
      "42/42 [==============================] - 4s 94ms/step - loss: 0.0154 - accuracy: 0.9952\n",
      "Epoch 415/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0104 - accuracy: 0.9966\n",
      "Epoch 415: accuracy did not improve from 0.99747\n",
      "42/42 [==============================] - 4s 99ms/step - loss: 0.0104 - accuracy: 0.9966\n",
      "Epoch 416/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0124 - accuracy: 0.9966\n",
      "Epoch 416: accuracy did not improve from 0.99747\n",
      "42/42 [==============================] - 4s 94ms/step - loss: 0.0124 - accuracy: 0.9966\n",
      "Epoch 417/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0132 - accuracy: 0.9952\n",
      "Epoch 417: accuracy did not improve from 0.99747\n",
      "42/42 [==============================] - 4s 94ms/step - loss: 0.0132 - accuracy: 0.9952\n",
      "Epoch 418/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0110 - accuracy: 0.9962\n",
      "Epoch 418: accuracy did not improve from 0.99747\n",
      "42/42 [==============================] - 4s 94ms/step - loss: 0.0110 - accuracy: 0.9962\n",
      "Epoch 419/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0152 - accuracy: 0.9954\n",
      "Epoch 419: accuracy did not improve from 0.99747\n",
      "42/42 [==============================] - 4s 97ms/step - loss: 0.0152 - accuracy: 0.9954\n",
      "Epoch 420/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0126 - accuracy: 0.9957\n",
      "Epoch 420: accuracy did not improve from 0.99747\n",
      "42/42 [==============================] - 4s 95ms/step - loss: 0.0126 - accuracy: 0.9957\n",
      "Epoch 421/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0113 - accuracy: 0.9960\n",
      "Epoch 421: accuracy did not improve from 0.99747\n",
      "42/42 [==============================] - 4s 94ms/step - loss: 0.0113 - accuracy: 0.9960\n",
      "Epoch 422/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0133 - accuracy: 0.9962\n",
      "Epoch 422: accuracy did not improve from 0.99747\n",
      "42/42 [==============================] - 4s 96ms/step - loss: 0.0133 - accuracy: 0.9962\n",
      "Epoch 423/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0134 - accuracy: 0.9955\n",
      "Epoch 423: accuracy did not improve from 0.99747\n",
      "42/42 [==============================] - 4s 95ms/step - loss: 0.0134 - accuracy: 0.9955\n",
      "Epoch 424/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0166 - accuracy: 0.9952\n",
      "Epoch 424: accuracy did not improve from 0.99747\n",
      "42/42 [==============================] - 4s 94ms/step - loss: 0.0166 - accuracy: 0.9952\n",
      "Epoch 425/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0111 - accuracy: 0.9971\n",
      "Epoch 425: accuracy did not improve from 0.99747\n",
      "42/42 [==============================] - 4s 94ms/step - loss: 0.0111 - accuracy: 0.9971\n",
      "Epoch 426/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0081 - accuracy: 0.9977\n",
      "Epoch 426: accuracy improved from 0.99747 to 0.99766, saving model to eq_solver.h5\n",
      "42/42 [==============================] - 4s 94ms/step - loss: 0.0081 - accuracy: 0.9977\n",
      "Epoch 427/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0112 - accuracy: 0.9964\n",
      "Epoch 427: accuracy did not improve from 0.99766\n",
      "42/42 [==============================] - 4s 97ms/step - loss: 0.0112 - accuracy: 0.9964\n",
      "Epoch 428/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0100 - accuracy: 0.9967\n",
      "Epoch 428: accuracy did not improve from 0.99766\n",
      "42/42 [==============================] - 4s 95ms/step - loss: 0.0100 - accuracy: 0.9967\n",
      "Epoch 429/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0152 - accuracy: 0.9955\n",
      "Epoch 429: accuracy did not improve from 0.99766\n",
      "42/42 [==============================] - 4s 96ms/step - loss: 0.0152 - accuracy: 0.9955\n",
      "Epoch 430/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0083 - accuracy: 0.9977\n",
      "Epoch 430: accuracy did not improve from 0.99766\n",
      "42/42 [==============================] - 4s 94ms/step - loss: 0.0083 - accuracy: 0.9977\n",
      "Epoch 431/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0186 - accuracy: 0.9949\n",
      "Epoch 431: accuracy did not improve from 0.99766\n",
      "42/42 [==============================] - 4s 95ms/step - loss: 0.0186 - accuracy: 0.9949\n",
      "Epoch 432/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0130 - accuracy: 0.9963\n",
      "Epoch 432: accuracy did not improve from 0.99766\n",
      "42/42 [==============================] - 4s 94ms/step - loss: 0.0130 - accuracy: 0.9963\n",
      "Epoch 433/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0174 - accuracy: 0.9961\n",
      "Epoch 433: accuracy did not improve from 0.99766\n",
      "42/42 [==============================] - 4s 98ms/step - loss: 0.0174 - accuracy: 0.9961\n",
      "Epoch 434/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0125 - accuracy: 0.9959\n",
      "Epoch 434: accuracy did not improve from 0.99766\n",
      "42/42 [==============================] - 4s 99ms/step - loss: 0.0125 - accuracy: 0.9959\n",
      "Epoch 435/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0100 - accuracy: 0.9967\n",
      "Epoch 435: accuracy did not improve from 0.99766\n",
      "42/42 [==============================] - 4s 97ms/step - loss: 0.0100 - accuracy: 0.9967\n",
      "Epoch 436/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0125 - accuracy: 0.9967\n",
      "Epoch 436: accuracy did not improve from 0.99766\n",
      "42/42 [==============================] - 4s 96ms/step - loss: 0.0125 - accuracy: 0.9967\n",
      "Epoch 437/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0182 - accuracy: 0.9959\n",
      "Epoch 437: accuracy did not improve from 0.99766\n",
      "42/42 [==============================] - 4s 95ms/step - loss: 0.0182 - accuracy: 0.9959\n",
      "Epoch 438/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0106 - accuracy: 0.9960\n",
      "Epoch 438: accuracy did not improve from 0.99766\n",
      "42/42 [==============================] - 4s 98ms/step - loss: 0.0106 - accuracy: 0.9960\n",
      "Epoch 439/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0179 - accuracy: 0.9956\n",
      "Epoch 439: accuracy did not improve from 0.99766\n",
      "42/42 [==============================] - 4s 94ms/step - loss: 0.0179 - accuracy: 0.9956\n",
      "Epoch 440/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0168 - accuracy: 0.9954\n",
      "Epoch 440: accuracy did not improve from 0.99766\n",
      "42/42 [==============================] - 4s 97ms/step - loss: 0.0168 - accuracy: 0.9954\n",
      "Epoch 441/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0142 - accuracy: 0.9955\n",
      "Epoch 441: accuracy did not improve from 0.99766\n",
      "42/42 [==============================] - 4s 96ms/step - loss: 0.0142 - accuracy: 0.9955\n",
      "Epoch 442/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0125 - accuracy: 0.9965\n",
      "Epoch 442: accuracy did not improve from 0.99766\n",
      "42/42 [==============================] - 4s 99ms/step - loss: 0.0125 - accuracy: 0.9965\n",
      "Epoch 443/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0134 - accuracy: 0.9968\n",
      "Epoch 443: accuracy did not improve from 0.99766\n",
      "42/42 [==============================] - 4s 96ms/step - loss: 0.0134 - accuracy: 0.9968\n",
      "Epoch 444/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0135 - accuracy: 0.9951\n",
      "Epoch 444: accuracy did not improve from 0.99766\n",
      "42/42 [==============================] - 4s 94ms/step - loss: 0.0135 - accuracy: 0.9951\n",
      "Epoch 445/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0138 - accuracy: 0.9957\n",
      "Epoch 445: accuracy did not improve from 0.99766\n",
      "42/42 [==============================] - 4s 96ms/step - loss: 0.0138 - accuracy: 0.9957\n",
      "Epoch 446/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0122 - accuracy: 0.9964\n",
      "Epoch 446: accuracy did not improve from 0.99766\n",
      "42/42 [==============================] - 4s 99ms/step - loss: 0.0122 - accuracy: 0.9964\n",
      "Epoch 447/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0156 - accuracy: 0.9968\n",
      "Epoch 447: accuracy did not improve from 0.99766\n",
      "42/42 [==============================] - 4s 96ms/step - loss: 0.0156 - accuracy: 0.9968\n",
      "Epoch 448/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0101 - accuracy: 0.9971\n",
      "Epoch 448: accuracy did not improve from 0.99766\n",
      "42/42 [==============================] - 4s 95ms/step - loss: 0.0101 - accuracy: 0.9971\n",
      "Epoch 449/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0116 - accuracy: 0.9965\n",
      "Epoch 449: accuracy did not improve from 0.99766\n",
      "42/42 [==============================] - 4s 98ms/step - loss: 0.0116 - accuracy: 0.9965\n",
      "Epoch 450/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0140 - accuracy: 0.9964\n",
      "Epoch 450: accuracy did not improve from 0.99766\n",
      "42/42 [==============================] - 4s 95ms/step - loss: 0.0140 - accuracy: 0.9964\n",
      "Epoch 451/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0135 - accuracy: 0.9962\n",
      "Epoch 451: accuracy did not improve from 0.99766\n",
      "42/42 [==============================] - 4s 100ms/step - loss: 0.0135 - accuracy: 0.9962\n",
      "Epoch 452/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0147 - accuracy: 0.9958\n",
      "Epoch 452: accuracy did not improve from 0.99766\n",
      "42/42 [==============================] - 4s 104ms/step - loss: 0.0147 - accuracy: 0.9958\n",
      "Epoch 453/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0145 - accuracy: 0.9964\n",
      "Epoch 453: accuracy did not improve from 0.99766\n",
      "42/42 [==============================] - 4s 102ms/step - loss: 0.0145 - accuracy: 0.9964\n",
      "Epoch 454/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0122 - accuracy: 0.9964\n",
      "Epoch 454: accuracy did not improve from 0.99766\n",
      "42/42 [==============================] - 4s 100ms/step - loss: 0.0122 - accuracy: 0.9964\n",
      "Epoch 455/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0174 - accuracy: 0.9951\n",
      "Epoch 455: accuracy did not improve from 0.99766\n",
      "42/42 [==============================] - 4s 98ms/step - loss: 0.0174 - accuracy: 0.9951\n",
      "Epoch 456/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0111 - accuracy: 0.9966\n",
      "Epoch 456: accuracy did not improve from 0.99766\n",
      "42/42 [==============================] - 4s 95ms/step - loss: 0.0111 - accuracy: 0.9966\n",
      "Epoch 457/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0088 - accuracy: 0.9966\n",
      "Epoch 457: accuracy did not improve from 0.99766\n",
      "42/42 [==============================] - 4s 97ms/step - loss: 0.0088 - accuracy: 0.9966\n",
      "Epoch 458/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0112 - accuracy: 0.9965\n",
      "Epoch 458: accuracy did not improve from 0.99766\n",
      "42/42 [==============================] - 4s 93ms/step - loss: 0.0112 - accuracy: 0.9965\n",
      "Epoch 459/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0128 - accuracy: 0.9960\n",
      "Epoch 459: accuracy did not improve from 0.99766\n",
      "42/42 [==============================] - 4s 97ms/step - loss: 0.0128 - accuracy: 0.9960\n",
      "Epoch 460/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0128 - accuracy: 0.9960\n",
      "Epoch 460: accuracy did not improve from 0.99766\n",
      "42/42 [==============================] - 4s 98ms/step - loss: 0.0128 - accuracy: 0.9960\n",
      "Epoch 461/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0142 - accuracy: 0.9959\n",
      "Epoch 461: accuracy did not improve from 0.99766\n",
      "42/42 [==============================] - 4s 94ms/step - loss: 0.0142 - accuracy: 0.9959\n",
      "Epoch 462/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0126 - accuracy: 0.9964\n",
      "Epoch 462: accuracy did not improve from 0.99766\n",
      "42/42 [==============================] - 4s 95ms/step - loss: 0.0126 - accuracy: 0.9964\n",
      "Epoch 463/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0123 - accuracy: 0.9966\n",
      "Epoch 463: accuracy did not improve from 0.99766\n",
      "42/42 [==============================] - 4s 96ms/step - loss: 0.0123 - accuracy: 0.9966\n",
      "Epoch 464/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0100 - accuracy: 0.9970\n",
      "Epoch 464: accuracy did not improve from 0.99766\n",
      "42/42 [==============================] - 4s 95ms/step - loss: 0.0100 - accuracy: 0.9970\n",
      "Epoch 465/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0098 - accuracy: 0.9964\n",
      "Epoch 465: accuracy did not improve from 0.99766\n",
      "42/42 [==============================] - 4s 96ms/step - loss: 0.0098 - accuracy: 0.9964\n",
      "Epoch 466/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0100 - accuracy: 0.9968\n",
      "Epoch 466: accuracy did not improve from 0.99766\n",
      "42/42 [==============================] - 4s 97ms/step - loss: 0.0100 - accuracy: 0.9968\n",
      "Epoch 467/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0087 - accuracy: 0.9969\n",
      "Epoch 467: accuracy did not improve from 0.99766\n",
      "42/42 [==============================] - 4s 96ms/step - loss: 0.0087 - accuracy: 0.9969\n",
      "Epoch 468/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0140 - accuracy: 0.9953\n",
      "Epoch 468: accuracy did not improve from 0.99766\n",
      "42/42 [==============================] - 4s 96ms/step - loss: 0.0140 - accuracy: 0.9953\n",
      "Epoch 469/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0119 - accuracy: 0.9967\n",
      "Epoch 469: accuracy did not improve from 0.99766\n",
      "42/42 [==============================] - 4s 95ms/step - loss: 0.0119 - accuracy: 0.9967\n",
      "Epoch 470/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0082 - accuracy: 0.9976\n",
      "Epoch 470: accuracy did not improve from 0.99766\n",
      "42/42 [==============================] - 4s 96ms/step - loss: 0.0082 - accuracy: 0.9976\n",
      "Epoch 471/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0098 - accuracy: 0.9967\n",
      "Epoch 471: accuracy did not improve from 0.99766\n",
      "42/42 [==============================] - 4s 97ms/step - loss: 0.0098 - accuracy: 0.9967\n",
      "Epoch 472/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0079 - accuracy: 0.9976\n",
      "Epoch 472: accuracy did not improve from 0.99766\n",
      "42/42 [==============================] - 4s 97ms/step - loss: 0.0079 - accuracy: 0.9976\n",
      "Epoch 473/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0116 - accuracy: 0.9969\n",
      "Epoch 473: accuracy did not improve from 0.99766\n",
      "42/42 [==============================] - 4s 100ms/step - loss: 0.0116 - accuracy: 0.9969\n",
      "Epoch 474/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0110 - accuracy: 0.9963\n",
      "Epoch 474: accuracy did not improve from 0.99766\n",
      "42/42 [==============================] - 4s 100ms/step - loss: 0.0110 - accuracy: 0.9963\n",
      "Epoch 475/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0096 - accuracy: 0.9968\n",
      "Epoch 475: accuracy did not improve from 0.99766\n",
      "42/42 [==============================] - 4s 99ms/step - loss: 0.0096 - accuracy: 0.9968\n",
      "Epoch 476/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0115 - accuracy: 0.9965\n",
      "Epoch 476: accuracy did not improve from 0.99766\n",
      "42/42 [==============================] - 4s 103ms/step - loss: 0.0115 - accuracy: 0.9965\n",
      "Epoch 477/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0155 - accuracy: 0.9950\n",
      "Epoch 477: accuracy did not improve from 0.99766\n",
      "42/42 [==============================] - 4s 99ms/step - loss: 0.0155 - accuracy: 0.9950\n",
      "Epoch 478/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0133 - accuracy: 0.9961\n",
      "Epoch 478: accuracy did not improve from 0.99766\n",
      "42/42 [==============================] - 4s 100ms/step - loss: 0.0133 - accuracy: 0.9961\n",
      "Epoch 479/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0166 - accuracy: 0.9958\n",
      "Epoch 479: accuracy did not improve from 0.99766\n",
      "42/42 [==============================] - 4s 100ms/step - loss: 0.0166 - accuracy: 0.9958\n",
      "Epoch 480/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0203 - accuracy: 0.9953\n",
      "Epoch 480: accuracy did not improve from 0.99766\n",
      "42/42 [==============================] - 4s 99ms/step - loss: 0.0203 - accuracy: 0.9953\n",
      "Epoch 481/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0110 - accuracy: 0.9966\n",
      "Epoch 481: accuracy did not improve from 0.99766\n",
      "42/42 [==============================] - 4s 103ms/step - loss: 0.0110 - accuracy: 0.9966\n",
      "Epoch 482/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0103 - accuracy: 0.9974\n",
      "Epoch 482: accuracy did not improve from 0.99766\n",
      "42/42 [==============================] - 4s 100ms/step - loss: 0.0103 - accuracy: 0.9974\n",
      "Epoch 483/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0100 - accuracy: 0.9971\n",
      "Epoch 483: accuracy did not improve from 0.99766\n",
      "42/42 [==============================] - 4s 102ms/step - loss: 0.0100 - accuracy: 0.9971\n",
      "Epoch 484/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0111 - accuracy: 0.9965\n",
      "Epoch 484: accuracy did not improve from 0.99766\n",
      "42/42 [==============================] - 4s 102ms/step - loss: 0.0111 - accuracy: 0.9965\n",
      "Epoch 485/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0095 - accuracy: 0.9964\n",
      "Epoch 485: accuracy did not improve from 0.99766\n",
      "42/42 [==============================] - 4s 100ms/step - loss: 0.0095 - accuracy: 0.9964\n",
      "Epoch 486/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0077 - accuracy: 0.9977\n",
      "Epoch 486: accuracy did not improve from 0.99766\n",
      "42/42 [==============================] - 4s 100ms/step - loss: 0.0077 - accuracy: 0.9977\n",
      "Epoch 487/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0092 - accuracy: 0.9965\n",
      "Epoch 487: accuracy did not improve from 0.99766\n",
      "42/42 [==============================] - 4s 101ms/step - loss: 0.0092 - accuracy: 0.9965\n",
      "Epoch 488/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0112 - accuracy: 0.9965\n",
      "Epoch 488: accuracy did not improve from 0.99766\n",
      "42/42 [==============================] - 4s 101ms/step - loss: 0.0112 - accuracy: 0.9965\n",
      "Epoch 489/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0113 - accuracy: 0.9971\n",
      "Epoch 489: accuracy did not improve from 0.99766\n",
      "42/42 [==============================] - 4s 101ms/step - loss: 0.0113 - accuracy: 0.9971\n",
      "Epoch 490/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0092 - accuracy: 0.9971\n",
      "Epoch 490: accuracy did not improve from 0.99766\n",
      "42/42 [==============================] - 4s 100ms/step - loss: 0.0092 - accuracy: 0.9971\n",
      "Epoch 491/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0149 - accuracy: 0.9956\n",
      "Epoch 491: accuracy did not improve from 0.99766\n",
      "42/42 [==============================] - 4s 99ms/step - loss: 0.0149 - accuracy: 0.9956\n",
      "Epoch 492/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0130 - accuracy: 0.9955\n",
      "Epoch 492: accuracy did not improve from 0.99766\n",
      "42/42 [==============================] - 4s 99ms/step - loss: 0.0130 - accuracy: 0.9955\n",
      "Epoch 493/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0138 - accuracy: 0.9957\n",
      "Epoch 493: accuracy did not improve from 0.99766\n",
      "42/42 [==============================] - 4s 99ms/step - loss: 0.0138 - accuracy: 0.9957\n",
      "Epoch 494/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0113 - accuracy: 0.9969\n",
      "Epoch 494: accuracy did not improve from 0.99766\n",
      "42/42 [==============================] - 4s 101ms/step - loss: 0.0113 - accuracy: 0.9969\n",
      "Epoch 495/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0137 - accuracy: 0.9961\n",
      "Epoch 495: accuracy did not improve from 0.99766\n",
      "42/42 [==============================] - 4s 105ms/step - loss: 0.0137 - accuracy: 0.9961\n",
      "Epoch 496/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0104 - accuracy: 0.9967\n",
      "Epoch 496: accuracy did not improve from 0.99766\n",
      "42/42 [==============================] - 4s 104ms/step - loss: 0.0104 - accuracy: 0.9967\n",
      "Epoch 497/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0140 - accuracy: 0.9959\n",
      "Epoch 497: accuracy did not improve from 0.99766\n",
      "42/42 [==============================] - 4s 104ms/step - loss: 0.0140 - accuracy: 0.9959\n",
      "Epoch 498/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0127 - accuracy: 0.9964\n",
      "Epoch 498: accuracy did not improve from 0.99766\n",
      "42/42 [==============================] - 4s 102ms/step - loss: 0.0127 - accuracy: 0.9964\n",
      "Epoch 499/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0102 - accuracy: 0.9963\n",
      "Epoch 499: accuracy did not improve from 0.99766\n",
      "42/42 [==============================] - 4s 97ms/step - loss: 0.0102 - accuracy: 0.9963\n",
      "Epoch 500/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0089 - accuracy: 0.9969\n",
      "Epoch 500: accuracy did not improve from 0.99766\n",
      "42/42 [==============================] - 4s 97ms/step - loss: 0.0089 - accuracy: 0.9969\n",
      "Epoch 501/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0117 - accuracy: 0.9965\n",
      "Epoch 501: accuracy did not improve from 0.99766\n",
      "42/42 [==============================] - 4s 100ms/step - loss: 0.0117 - accuracy: 0.9965\n",
      "Epoch 502/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0137 - accuracy: 0.9966\n",
      "Epoch 502: accuracy did not improve from 0.99766\n",
      "42/42 [==============================] - 4s 100ms/step - loss: 0.0137 - accuracy: 0.9966\n",
      "Epoch 503/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0144 - accuracy: 0.9964\n",
      "Epoch 503: accuracy did not improve from 0.99766\n",
      "42/42 [==============================] - 4s 99ms/step - loss: 0.0144 - accuracy: 0.9964\n",
      "Epoch 504/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0129 - accuracy: 0.9972\n",
      "Epoch 504: accuracy did not improve from 0.99766\n",
      "42/42 [==============================] - 4s 99ms/step - loss: 0.0129 - accuracy: 0.9972\n",
      "Epoch 505/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0150 - accuracy: 0.9954\n",
      "Epoch 505: accuracy did not improve from 0.99766\n",
      "42/42 [==============================] - 4s 98ms/step - loss: 0.0150 - accuracy: 0.9954\n",
      "Epoch 506/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0151 - accuracy: 0.9955\n",
      "Epoch 506: accuracy did not improve from 0.99766\n",
      "42/42 [==============================] - 4s 101ms/step - loss: 0.0151 - accuracy: 0.9955\n",
      "Epoch 507/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0101 - accuracy: 0.9966\n",
      "Epoch 507: accuracy did not improve from 0.99766\n",
      "42/42 [==============================] - 4s 99ms/step - loss: 0.0101 - accuracy: 0.9966\n",
      "Epoch 508/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0093 - accuracy: 0.9976\n",
      "Epoch 508: accuracy did not improve from 0.99766\n",
      "42/42 [==============================] - 4s 97ms/step - loss: 0.0093 - accuracy: 0.9976\n",
      "Epoch 509/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0092 - accuracy: 0.9975\n",
      "Epoch 509: accuracy did not improve from 0.99766\n",
      "42/42 [==============================] - 4s 99ms/step - loss: 0.0092 - accuracy: 0.9975\n",
      "Epoch 510/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0110 - accuracy: 0.9967\n",
      "Epoch 510: accuracy did not improve from 0.99766\n",
      "42/42 [==============================] - 4s 96ms/step - loss: 0.0110 - accuracy: 0.9967\n",
      "Epoch 511/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0101 - accuracy: 0.9964\n",
      "Epoch 511: accuracy did not improve from 0.99766\n",
      "42/42 [==============================] - 4s 95ms/step - loss: 0.0101 - accuracy: 0.9964\n",
      "Epoch 512/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0156 - accuracy: 0.9960\n",
      "Epoch 512: accuracy did not improve from 0.99766\n",
      "42/42 [==============================] - 4s 96ms/step - loss: 0.0156 - accuracy: 0.9960\n",
      "Epoch 513/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0143 - accuracy: 0.9969\n",
      "Epoch 513: accuracy did not improve from 0.99766\n",
      "42/42 [==============================] - 4s 96ms/step - loss: 0.0143 - accuracy: 0.9969\n",
      "Epoch 514/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0157 - accuracy: 0.9954\n",
      "Epoch 514: accuracy did not improve from 0.99766\n",
      "42/42 [==============================] - 4s 96ms/step - loss: 0.0157 - accuracy: 0.9954\n",
      "Epoch 515/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0159 - accuracy: 0.9960\n",
      "Epoch 515: accuracy did not improve from 0.99766\n",
      "42/42 [==============================] - 4s 95ms/step - loss: 0.0159 - accuracy: 0.9960\n",
      "Epoch 516/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0124 - accuracy: 0.9962\n",
      "Epoch 516: accuracy did not improve from 0.99766\n",
      "42/42 [==============================] - 4s 97ms/step - loss: 0.0124 - accuracy: 0.9962\n",
      "Epoch 517/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0129 - accuracy: 0.9964\n",
      "Epoch 517: accuracy did not improve from 0.99766\n",
      "42/42 [==============================] - 4s 96ms/step - loss: 0.0129 - accuracy: 0.9964\n",
      "Epoch 518/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0101 - accuracy: 0.9964\n",
      "Epoch 518: accuracy did not improve from 0.99766\n",
      "42/42 [==============================] - 4s 98ms/step - loss: 0.0101 - accuracy: 0.9964\n",
      "Epoch 519/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0107 - accuracy: 0.9973\n",
      "Epoch 519: accuracy did not improve from 0.99766\n",
      "42/42 [==============================] - 4s 98ms/step - loss: 0.0107 - accuracy: 0.9973\n",
      "Epoch 520/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0117 - accuracy: 0.9969\n",
      "Epoch 520: accuracy did not improve from 0.99766\n",
      "42/42 [==============================] - 4s 96ms/step - loss: 0.0117 - accuracy: 0.9969\n",
      "Epoch 521/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0132 - accuracy: 0.9964\n",
      "Epoch 521: accuracy did not improve from 0.99766\n",
      "42/42 [==============================] - 4s 96ms/step - loss: 0.0132 - accuracy: 0.9964\n",
      "Epoch 522/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0074 - accuracy: 0.9972\n",
      "Epoch 522: accuracy did not improve from 0.99766\n",
      "42/42 [==============================] - 4s 98ms/step - loss: 0.0074 - accuracy: 0.9972\n",
      "Epoch 523/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0086 - accuracy: 0.9972\n",
      "Epoch 523: accuracy did not improve from 0.99766\n",
      "42/42 [==============================] - 4s 94ms/step - loss: 0.0086 - accuracy: 0.9972\n",
      "Epoch 524/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0091 - accuracy: 0.9966\n",
      "Epoch 524: accuracy did not improve from 0.99766\n",
      "42/42 [==============================] - 4s 96ms/step - loss: 0.0091 - accuracy: 0.9966\n",
      "Epoch 525/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0115 - accuracy: 0.9966\n",
      "Epoch 525: accuracy did not improve from 0.99766\n",
      "42/42 [==============================] - 4s 99ms/step - loss: 0.0115 - accuracy: 0.9966\n",
      "Epoch 526/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0130 - accuracy: 0.9965\n",
      "Epoch 526: accuracy did not improve from 0.99766\n",
      "42/42 [==============================] - 4s 95ms/step - loss: 0.0130 - accuracy: 0.9965\n",
      "Epoch 527/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0141 - accuracy: 0.9963\n",
      "Epoch 527: accuracy did not improve from 0.99766\n",
      "42/42 [==============================] - 4s 96ms/step - loss: 0.0141 - accuracy: 0.9963\n",
      "Epoch 528/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0109 - accuracy: 0.9968\n",
      "Epoch 528: accuracy did not improve from 0.99766\n",
      "42/42 [==============================] - 4s 94ms/step - loss: 0.0109 - accuracy: 0.9968\n",
      "Epoch 529/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0118 - accuracy: 0.9949\n",
      "Epoch 529: accuracy did not improve from 0.99766\n",
      "42/42 [==============================] - 4s 95ms/step - loss: 0.0118 - accuracy: 0.9949\n",
      "Epoch 530/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0094 - accuracy: 0.9970\n",
      "Epoch 530: accuracy did not improve from 0.99766\n",
      "42/42 [==============================] - 4s 94ms/step - loss: 0.0094 - accuracy: 0.9970\n",
      "Epoch 531/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0100 - accuracy: 0.9966\n",
      "Epoch 531: accuracy did not improve from 0.99766\n",
      "42/42 [==============================] - 4s 96ms/step - loss: 0.0100 - accuracy: 0.9966\n",
      "Epoch 532/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0098 - accuracy: 0.9974\n",
      "Epoch 532: accuracy did not improve from 0.99766\n",
      "42/42 [==============================] - 4s 98ms/step - loss: 0.0098 - accuracy: 0.9974\n",
      "Epoch 533/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0135 - accuracy: 0.9956\n",
      "Epoch 533: accuracy did not improve from 0.99766\n",
      "42/42 [==============================] - 4s 97ms/step - loss: 0.0135 - accuracy: 0.9956\n",
      "Epoch 534/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0100 - accuracy: 0.9977\n",
      "Epoch 534: accuracy did not improve from 0.99766\n",
      "42/42 [==============================] - 4s 95ms/step - loss: 0.0100 - accuracy: 0.9977\n",
      "Epoch 535/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0132 - accuracy: 0.9959\n",
      "Epoch 535: accuracy did not improve from 0.99766\n",
      "42/42 [==============================] - 4s 98ms/step - loss: 0.0132 - accuracy: 0.9959\n",
      "Epoch 536/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0117 - accuracy: 0.9964\n",
      "Epoch 536: accuracy did not improve from 0.99766\n",
      "42/42 [==============================] - 4s 97ms/step - loss: 0.0117 - accuracy: 0.9964\n",
      "Epoch 537/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0137 - accuracy: 0.9957\n",
      "Epoch 537: accuracy did not improve from 0.99766\n",
      "42/42 [==============================] - 4s 100ms/step - loss: 0.0137 - accuracy: 0.9957\n",
      "Epoch 538/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0087 - accuracy: 0.9975\n",
      "Epoch 538: accuracy did not improve from 0.99766\n",
      "42/42 [==============================] - 4s 94ms/step - loss: 0.0087 - accuracy: 0.9975\n",
      "Epoch 539/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0060 - accuracy: 0.9981\n",
      "Epoch 539: accuracy improved from 0.99766 to 0.99813, saving model to eq_solver.h5\n",
      "42/42 [==============================] - 4s 95ms/step - loss: 0.0060 - accuracy: 0.9981\n",
      "Epoch 540/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0118 - accuracy: 0.9968\n",
      "Epoch 540: accuracy did not improve from 0.99813\n",
      "42/42 [==============================] - 4s 92ms/step - loss: 0.0118 - accuracy: 0.9968\n",
      "Epoch 541/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0101 - accuracy: 0.9972\n",
      "Epoch 541: accuracy did not improve from 0.99813\n",
      "42/42 [==============================] - 4s 90ms/step - loss: 0.0101 - accuracy: 0.9972\n",
      "Epoch 542/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0109 - accuracy: 0.9966\n",
      "Epoch 542: accuracy did not improve from 0.99813\n",
      "42/42 [==============================] - 4s 95ms/step - loss: 0.0109 - accuracy: 0.9966\n",
      "Epoch 543/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0152 - accuracy: 0.9967\n",
      "Epoch 543: accuracy did not improve from 0.99813\n",
      "42/42 [==============================] - 4s 92ms/step - loss: 0.0152 - accuracy: 0.9967\n",
      "Epoch 544/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0127 - accuracy: 0.9958\n",
      "Epoch 544: accuracy did not improve from 0.99813\n",
      "42/42 [==============================] - 4s 91ms/step - loss: 0.0127 - accuracy: 0.9958\n",
      "Epoch 545/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0135 - accuracy: 0.9963\n",
      "Epoch 545: accuracy did not improve from 0.99813\n",
      "42/42 [==============================] - 4s 91ms/step - loss: 0.0135 - accuracy: 0.9963\n",
      "Epoch 546/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0115 - accuracy: 0.9962\n",
      "Epoch 546: accuracy did not improve from 0.99813\n",
      "42/42 [==============================] - 4s 92ms/step - loss: 0.0115 - accuracy: 0.9962\n",
      "Epoch 547/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0054 - accuracy: 0.9983\n",
      "Epoch 547: accuracy improved from 0.99813 to 0.99832, saving model to eq_solver.h5\n",
      "42/42 [==============================] - 4s 95ms/step - loss: 0.0054 - accuracy: 0.9983\n",
      "Epoch 548/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0063 - accuracy: 0.9984\n",
      "Epoch 548: accuracy improved from 0.99832 to 0.99841, saving model to eq_solver.h5\n",
      "42/42 [==============================] - 4s 92ms/step - loss: 0.0063 - accuracy: 0.9984\n",
      "Epoch 549/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0125 - accuracy: 0.9953\n",
      "Epoch 549: accuracy did not improve from 0.99841\n",
      "42/42 [==============================] - 4s 91ms/step - loss: 0.0125 - accuracy: 0.9953\n",
      "Epoch 550/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0129 - accuracy: 0.9966\n",
      "Epoch 550: accuracy did not improve from 0.99841\n",
      "42/42 [==============================] - 4s 93ms/step - loss: 0.0129 - accuracy: 0.9966\n",
      "Epoch 551/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0123 - accuracy: 0.9960\n",
      "Epoch 551: accuracy did not improve from 0.99841\n",
      "42/42 [==============================] - 4s 91ms/step - loss: 0.0123 - accuracy: 0.9960\n",
      "Epoch 552/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0103 - accuracy: 0.9970\n",
      "Epoch 552: accuracy did not improve from 0.99841\n",
      "42/42 [==============================] - 4s 90ms/step - loss: 0.0103 - accuracy: 0.9970\n",
      "Epoch 553/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0200 - accuracy: 0.9944\n",
      "Epoch 553: accuracy did not improve from 0.99841\n",
      "42/42 [==============================] - 4s 91ms/step - loss: 0.0200 - accuracy: 0.9944\n",
      "Epoch 554/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0116 - accuracy: 0.9966\n",
      "Epoch 554: accuracy did not improve from 0.99841\n",
      "42/42 [==============================] - 4s 92ms/step - loss: 0.0116 - accuracy: 0.9966\n",
      "Epoch 555/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0116 - accuracy: 0.9968\n",
      "Epoch 555: accuracy did not improve from 0.99841\n",
      "42/42 [==============================] - 4s 91ms/step - loss: 0.0116 - accuracy: 0.9968\n",
      "Epoch 556/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0132 - accuracy: 0.9959\n",
      "Epoch 556: accuracy did not improve from 0.99841\n",
      "42/42 [==============================] - 4s 92ms/step - loss: 0.0132 - accuracy: 0.9959\n",
      "Epoch 557/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0113 - accuracy: 0.9963\n",
      "Epoch 557: accuracy did not improve from 0.99841\n",
      "42/42 [==============================] - 4s 93ms/step - loss: 0.0113 - accuracy: 0.9963\n",
      "Epoch 558/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0090 - accuracy: 0.9977\n",
      "Epoch 558: accuracy did not improve from 0.99841\n",
      "42/42 [==============================] - 4s 91ms/step - loss: 0.0090 - accuracy: 0.9977\n",
      "Epoch 559/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0081 - accuracy: 0.9971\n",
      "Epoch 559: accuracy did not improve from 0.99841\n",
      "42/42 [==============================] - 4s 90ms/step - loss: 0.0081 - accuracy: 0.9971\n",
      "Epoch 560/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0070 - accuracy: 0.9976\n",
      "Epoch 560: accuracy did not improve from 0.99841\n",
      "42/42 [==============================] - 4s 93ms/step - loss: 0.0070 - accuracy: 0.9976\n",
      "Epoch 561/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0054 - accuracy: 0.9978\n",
      "Epoch 561: accuracy did not improve from 0.99841\n",
      "42/42 [==============================] - 4s 92ms/step - loss: 0.0054 - accuracy: 0.9978\n",
      "Epoch 562/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0069 - accuracy: 0.9975\n",
      "Epoch 562: accuracy did not improve from 0.99841\n",
      "42/42 [==============================] - 4s 94ms/step - loss: 0.0069 - accuracy: 0.9975\n",
      "Epoch 563/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0076 - accuracy: 0.9976\n",
      "Epoch 563: accuracy did not improve from 0.99841\n",
      "42/42 [==============================] - 4s 96ms/step - loss: 0.0076 - accuracy: 0.9976\n",
      "Epoch 564/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0163 - accuracy: 0.9958\n",
      "Epoch 564: accuracy did not improve from 0.99841\n",
      "42/42 [==============================] - 4s 90ms/step - loss: 0.0163 - accuracy: 0.9958\n",
      "Epoch 565/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0109 - accuracy: 0.9965\n",
      "Epoch 565: accuracy did not improve from 0.99841\n",
      "42/42 [==============================] - 4s 89ms/step - loss: 0.0109 - accuracy: 0.9965\n",
      "Epoch 566/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0140 - accuracy: 0.9964\n",
      "Epoch 566: accuracy did not improve from 0.99841\n",
      "42/42 [==============================] - 4s 92ms/step - loss: 0.0140 - accuracy: 0.9964\n",
      "Epoch 567/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0146 - accuracy: 0.9964\n",
      "Epoch 567: accuracy did not improve from 0.99841\n",
      "42/42 [==============================] - 4s 91ms/step - loss: 0.0146 - accuracy: 0.9964\n",
      "Epoch 568/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0138 - accuracy: 0.9957\n",
      "Epoch 568: accuracy did not improve from 0.99841\n",
      "42/42 [==============================] - 4s 92ms/step - loss: 0.0138 - accuracy: 0.9957\n",
      "Epoch 569/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0122 - accuracy: 0.9964\n",
      "Epoch 569: accuracy did not improve from 0.99841\n",
      "42/42 [==============================] - 4s 93ms/step - loss: 0.0122 - accuracy: 0.9964\n",
      "Epoch 570/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0142 - accuracy: 0.9966\n",
      "Epoch 570: accuracy did not improve from 0.99841\n",
      "42/42 [==============================] - 4s 94ms/step - loss: 0.0142 - accuracy: 0.9966\n",
      "Epoch 571/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0112 - accuracy: 0.9965\n",
      "Epoch 571: accuracy did not improve from 0.99841\n",
      "42/42 [==============================] - 4s 91ms/step - loss: 0.0112 - accuracy: 0.9965\n",
      "Epoch 572/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0083 - accuracy: 0.9974\n",
      "Epoch 572: accuracy did not improve from 0.99841\n",
      "42/42 [==============================] - 4s 91ms/step - loss: 0.0083 - accuracy: 0.9974\n",
      "Epoch 573/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0111 - accuracy: 0.9967\n",
      "Epoch 573: accuracy did not improve from 0.99841\n",
      "42/42 [==============================] - 4s 92ms/step - loss: 0.0111 - accuracy: 0.9967\n",
      "Epoch 574/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0143 - accuracy: 0.9963\n",
      "Epoch 574: accuracy did not improve from 0.99841\n",
      "42/42 [==============================] - 4s 88ms/step - loss: 0.0143 - accuracy: 0.9963\n",
      "Epoch 575/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0139 - accuracy: 0.9967\n",
      "Epoch 575: accuracy did not improve from 0.99841\n",
      "42/42 [==============================] - 4s 90ms/step - loss: 0.0139 - accuracy: 0.9967\n",
      "Epoch 576/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0064 - accuracy: 0.9981\n",
      "Epoch 576: accuracy did not improve from 0.99841\n",
      "42/42 [==============================] - 4s 90ms/step - loss: 0.0064 - accuracy: 0.9981\n",
      "Epoch 577/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0067 - accuracy: 0.9977\n",
      "Epoch 577: accuracy did not improve from 0.99841\n",
      "42/42 [==============================] - 4s 91ms/step - loss: 0.0067 - accuracy: 0.9977\n",
      "Epoch 578/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0075 - accuracy: 0.9978\n",
      "Epoch 578: accuracy did not improve from 0.99841\n",
      "42/42 [==============================] - 4s 92ms/step - loss: 0.0075 - accuracy: 0.9978\n",
      "Epoch 579/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0101 - accuracy: 0.9969\n",
      "Epoch 579: accuracy did not improve from 0.99841\n",
      "42/42 [==============================] - 4s 91ms/step - loss: 0.0101 - accuracy: 0.9969\n",
      "Epoch 580/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0065 - accuracy: 0.9978\n",
      "Epoch 580: accuracy did not improve from 0.99841\n",
      "42/42 [==============================] - 4s 92ms/step - loss: 0.0065 - accuracy: 0.9978\n",
      "Epoch 581/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0092 - accuracy: 0.9970\n",
      "Epoch 581: accuracy did not improve from 0.99841\n",
      "42/42 [==============================] - 4s 90ms/step - loss: 0.0092 - accuracy: 0.9970\n",
      "Epoch 582/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0120 - accuracy: 0.9958\n",
      "Epoch 582: accuracy did not improve from 0.99841\n",
      "42/42 [==============================] - 4s 91ms/step - loss: 0.0120 - accuracy: 0.9958\n",
      "Epoch 583/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0137 - accuracy: 0.9961\n",
      "Epoch 583: accuracy did not improve from 0.99841\n",
      "42/42 [==============================] - 4s 91ms/step - loss: 0.0137 - accuracy: 0.9961\n",
      "Epoch 584/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0149 - accuracy: 0.9958\n",
      "Epoch 584: accuracy did not improve from 0.99841\n",
      "42/42 [==============================] - 4s 92ms/step - loss: 0.0149 - accuracy: 0.9958\n",
      "Epoch 585/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0094 - accuracy: 0.9975\n",
      "Epoch 585: accuracy did not improve from 0.99841\n",
      "42/42 [==============================] - 4s 92ms/step - loss: 0.0094 - accuracy: 0.9975\n",
      "Epoch 586/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0093 - accuracy: 0.9968\n",
      "Epoch 586: accuracy did not improve from 0.99841\n",
      "42/42 [==============================] - 4s 92ms/step - loss: 0.0093 - accuracy: 0.9968\n",
      "Epoch 587/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0088 - accuracy: 0.9966\n",
      "Epoch 587: accuracy did not improve from 0.99841\n",
      "42/42 [==============================] - 4s 93ms/step - loss: 0.0088 - accuracy: 0.9966\n",
      "Epoch 588/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0133 - accuracy: 0.9970\n",
      "Epoch 588: accuracy did not improve from 0.99841\n",
      "42/42 [==============================] - 4s 94ms/step - loss: 0.0133 - accuracy: 0.9970\n",
      "Epoch 589/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0146 - accuracy: 0.9960\n",
      "Epoch 589: accuracy did not improve from 0.99841\n",
      "42/42 [==============================] - 4s 91ms/step - loss: 0.0146 - accuracy: 0.9960\n",
      "Epoch 590/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0096 - accuracy: 0.9969\n",
      "Epoch 590: accuracy did not improve from 0.99841\n",
      "42/42 [==============================] - 4s 92ms/step - loss: 0.0096 - accuracy: 0.9969\n",
      "Epoch 591/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0096 - accuracy: 0.9964\n",
      "Epoch 591: accuracy did not improve from 0.99841\n",
      "42/42 [==============================] - 4s 92ms/step - loss: 0.0096 - accuracy: 0.9964\n",
      "Epoch 592/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0079 - accuracy: 0.9978\n",
      "Epoch 592: accuracy did not improve from 0.99841\n",
      "42/42 [==============================] - 4s 91ms/step - loss: 0.0079 - accuracy: 0.9978\n",
      "Epoch 593/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0061 - accuracy: 0.9983\n",
      "Epoch 593: accuracy did not improve from 0.99841\n",
      "42/42 [==============================] - 4s 92ms/step - loss: 0.0061 - accuracy: 0.9983\n",
      "Epoch 594/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0086 - accuracy: 0.9975\n",
      "Epoch 594: accuracy did not improve from 0.99841\n",
      "42/42 [==============================] - 4s 95ms/step - loss: 0.0086 - accuracy: 0.9975\n",
      "Epoch 595/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0085 - accuracy: 0.9972\n",
      "Epoch 595: accuracy did not improve from 0.99841\n",
      "42/42 [==============================] - 4s 93ms/step - loss: 0.0085 - accuracy: 0.9972\n",
      "Epoch 596/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0080 - accuracy: 0.9976\n",
      "Epoch 596: accuracy did not improve from 0.99841\n",
      "42/42 [==============================] - 4s 92ms/step - loss: 0.0080 - accuracy: 0.9976\n",
      "Epoch 597/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0091 - accuracy: 0.9974\n",
      "Epoch 597: accuracy did not improve from 0.99841\n",
      "42/42 [==============================] - 4s 90ms/step - loss: 0.0091 - accuracy: 0.9974\n",
      "Epoch 598/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0084 - accuracy: 0.9975\n",
      "Epoch 598: accuracy did not improve from 0.99841\n",
      "42/42 [==============================] - 4s 90ms/step - loss: 0.0084 - accuracy: 0.9975\n",
      "Epoch 599/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0117 - accuracy: 0.9970\n",
      "Epoch 599: accuracy did not improve from 0.99841\n",
      "42/42 [==============================] - 4s 93ms/step - loss: 0.0117 - accuracy: 0.9970\n",
      "Epoch 600/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0121 - accuracy: 0.9968\n",
      "Epoch 600: accuracy did not improve from 0.99841\n",
      "42/42 [==============================] - 4s 92ms/step - loss: 0.0121 - accuracy: 0.9968\n",
      "Epoch 601/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0105 - accuracy: 0.9962\n",
      "Epoch 601: accuracy did not improve from 0.99841\n",
      "42/42 [==============================] - 4s 93ms/step - loss: 0.0105 - accuracy: 0.9962\n",
      "Epoch 602/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0084 - accuracy: 0.9976\n",
      "Epoch 602: accuracy did not improve from 0.99841\n",
      "42/42 [==============================] - 4s 91ms/step - loss: 0.0084 - accuracy: 0.9976\n",
      "Epoch 603/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0080 - accuracy: 0.9975\n",
      "Epoch 603: accuracy did not improve from 0.99841\n",
      "42/42 [==============================] - 4s 92ms/step - loss: 0.0080 - accuracy: 0.9975\n",
      "Epoch 604/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0076 - accuracy: 0.9974\n",
      "Epoch 604: accuracy did not improve from 0.99841\n",
      "42/42 [==============================] - 4s 92ms/step - loss: 0.0076 - accuracy: 0.9974\n",
      "Epoch 605/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0130 - accuracy: 0.9962\n",
      "Epoch 605: accuracy did not improve from 0.99841\n",
      "42/42 [==============================] - 4s 91ms/step - loss: 0.0130 - accuracy: 0.9962\n",
      "Epoch 606/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0113 - accuracy: 0.9970\n",
      "Epoch 606: accuracy did not improve from 0.99841\n",
      "42/42 [==============================] - 4s 93ms/step - loss: 0.0113 - accuracy: 0.9970\n",
      "Epoch 607/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0071 - accuracy: 0.9979\n",
      "Epoch 607: accuracy did not improve from 0.99841\n",
      "42/42 [==============================] - 4s 90ms/step - loss: 0.0071 - accuracy: 0.9979\n",
      "Epoch 608/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0100 - accuracy: 0.9973\n",
      "Epoch 608: accuracy did not improve from 0.99841\n",
      "42/42 [==============================] - 4s 91ms/step - loss: 0.0100 - accuracy: 0.9973\n",
      "Epoch 609/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0109 - accuracy: 0.9976\n",
      "Epoch 609: accuracy did not improve from 0.99841\n",
      "42/42 [==============================] - 4s 92ms/step - loss: 0.0109 - accuracy: 0.9976\n",
      "Epoch 610/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0080 - accuracy: 0.9978\n",
      "Epoch 610: accuracy did not improve from 0.99841\n",
      "42/42 [==============================] - 4s 95ms/step - loss: 0.0080 - accuracy: 0.9978\n",
      "Epoch 611/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0089 - accuracy: 0.9974\n",
      "Epoch 611: accuracy did not improve from 0.99841\n",
      "42/42 [==============================] - 4s 94ms/step - loss: 0.0089 - accuracy: 0.9974\n",
      "Epoch 612/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0091 - accuracy: 0.9978\n",
      "Epoch 612: accuracy did not improve from 0.99841\n",
      "42/42 [==============================] - 4s 89ms/step - loss: 0.0091 - accuracy: 0.9978\n",
      "Epoch 613/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0109 - accuracy: 0.9970\n",
      "Epoch 613: accuracy did not improve from 0.99841\n",
      "42/42 [==============================] - 4s 92ms/step - loss: 0.0109 - accuracy: 0.9970\n",
      "Epoch 614/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0144 - accuracy: 0.9964\n",
      "Epoch 614: accuracy did not improve from 0.99841\n",
      "42/42 [==============================] - 4s 91ms/step - loss: 0.0144 - accuracy: 0.9964\n",
      "Epoch 615/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0086 - accuracy: 0.9977\n",
      "Epoch 615: accuracy did not improve from 0.99841\n",
      "42/42 [==============================] - 4s 90ms/step - loss: 0.0086 - accuracy: 0.9977\n",
      "Epoch 616/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0095 - accuracy: 0.9973\n",
      "Epoch 616: accuracy did not improve from 0.99841\n",
      "42/42 [==============================] - 4s 91ms/step - loss: 0.0095 - accuracy: 0.9973\n",
      "Epoch 617/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0117 - accuracy: 0.9970\n",
      "Epoch 617: accuracy did not improve from 0.99841\n",
      "42/42 [==============================] - 4s 92ms/step - loss: 0.0117 - accuracy: 0.9970\n",
      "Epoch 618/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0091 - accuracy: 0.9964\n",
      "Epoch 618: accuracy did not improve from 0.99841\n",
      "42/42 [==============================] - 4s 91ms/step - loss: 0.0091 - accuracy: 0.9964\n",
      "Epoch 619/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0108 - accuracy: 0.9966\n",
      "Epoch 619: accuracy did not improve from 0.99841\n",
      "42/42 [==============================] - 4s 91ms/step - loss: 0.0108 - accuracy: 0.9966\n",
      "Epoch 620/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0110 - accuracy: 0.9967\n",
      "Epoch 620: accuracy did not improve from 0.99841\n",
      "42/42 [==============================] - 4s 92ms/step - loss: 0.0110 - accuracy: 0.9967\n",
      "Epoch 621/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0094 - accuracy: 0.9969\n",
      "Epoch 621: accuracy did not improve from 0.99841\n",
      "42/42 [==============================] - 4s 92ms/step - loss: 0.0094 - accuracy: 0.9969\n",
      "Epoch 622/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0118 - accuracy: 0.9969\n",
      "Epoch 622: accuracy did not improve from 0.99841\n",
      "42/42 [==============================] - 4s 93ms/step - loss: 0.0118 - accuracy: 0.9969\n",
      "Epoch 623/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0089 - accuracy: 0.9974\n",
      "Epoch 623: accuracy did not improve from 0.99841\n",
      "42/42 [==============================] - 4s 92ms/step - loss: 0.0089 - accuracy: 0.9974\n",
      "Epoch 624/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0135 - accuracy: 0.9967\n",
      "Epoch 624: accuracy did not improve from 0.99841\n",
      "42/42 [==============================] - 4s 91ms/step - loss: 0.0135 - accuracy: 0.9967\n",
      "Epoch 625/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0125 - accuracy: 0.9967\n",
      "Epoch 625: accuracy did not improve from 0.99841\n",
      "42/42 [==============================] - 4s 90ms/step - loss: 0.0125 - accuracy: 0.9967\n",
      "Epoch 626/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0098 - accuracy: 0.9970\n",
      "Epoch 626: accuracy did not improve from 0.99841\n",
      "42/42 [==============================] - 4s 94ms/step - loss: 0.0098 - accuracy: 0.9970\n",
      "Epoch 627/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0143 - accuracy: 0.9962\n",
      "Epoch 627: accuracy did not improve from 0.99841\n",
      "42/42 [==============================] - 4s 92ms/step - loss: 0.0143 - accuracy: 0.9962\n",
      "Epoch 628/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0142 - accuracy: 0.9969\n",
      "Epoch 628: accuracy did not improve from 0.99841\n",
      "42/42 [==============================] - 4s 91ms/step - loss: 0.0142 - accuracy: 0.9969\n",
      "Epoch 629/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0071 - accuracy: 0.9978\n",
      "Epoch 629: accuracy did not improve from 0.99841\n",
      "42/42 [==============================] - 4s 91ms/step - loss: 0.0071 - accuracy: 0.9978\n",
      "Epoch 630/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0080 - accuracy: 0.9976\n",
      "Epoch 630: accuracy did not improve from 0.99841\n",
      "42/42 [==============================] - 4s 91ms/step - loss: 0.0080 - accuracy: 0.9976\n",
      "Epoch 631/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0102 - accuracy: 0.9968\n",
      "Epoch 631: accuracy did not improve from 0.99841\n",
      "42/42 [==============================] - 4s 90ms/step - loss: 0.0102 - accuracy: 0.9968\n",
      "Epoch 632/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0087 - accuracy: 0.9979\n",
      "Epoch 632: accuracy did not improve from 0.99841\n",
      "42/42 [==============================] - 4s 92ms/step - loss: 0.0087 - accuracy: 0.9979\n",
      "Epoch 633/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0158 - accuracy: 0.9969\n",
      "Epoch 633: accuracy did not improve from 0.99841\n",
      "42/42 [==============================] - 4s 92ms/step - loss: 0.0158 - accuracy: 0.9969\n",
      "Epoch 634/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0084 - accuracy: 0.9970\n",
      "Epoch 634: accuracy did not improve from 0.99841\n",
      "42/42 [==============================] - 4s 91ms/step - loss: 0.0084 - accuracy: 0.9970\n",
      "Epoch 635/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0062 - accuracy: 0.9979\n",
      "Epoch 635: accuracy did not improve from 0.99841\n",
      "42/42 [==============================] - 4s 89ms/step - loss: 0.0062 - accuracy: 0.9979\n",
      "Epoch 636/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0074 - accuracy: 0.9975\n",
      "Epoch 636: accuracy did not improve from 0.99841\n",
      "42/42 [==============================] - 4s 89ms/step - loss: 0.0074 - accuracy: 0.9975\n",
      "Epoch 637/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0089 - accuracy: 0.9973\n",
      "Epoch 637: accuracy did not improve from 0.99841\n",
      "42/42 [==============================] - 4s 90ms/step - loss: 0.0089 - accuracy: 0.9973\n",
      "Epoch 638/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0117 - accuracy: 0.9961\n",
      "Epoch 638: accuracy did not improve from 0.99841\n",
      "42/42 [==============================] - 4s 90ms/step - loss: 0.0117 - accuracy: 0.9961\n",
      "Epoch 639/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0088 - accuracy: 0.9970\n",
      "Epoch 639: accuracy did not improve from 0.99841\n",
      "42/42 [==============================] - 4s 91ms/step - loss: 0.0088 - accuracy: 0.9970\n",
      "Epoch 640/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0091 - accuracy: 0.9973\n",
      "Epoch 640: accuracy did not improve from 0.99841\n",
      "42/42 [==============================] - 4s 93ms/step - loss: 0.0091 - accuracy: 0.9973\n",
      "Epoch 641/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0060 - accuracy: 0.9982\n",
      "Epoch 641: accuracy did not improve from 0.99841\n",
      "42/42 [==============================] - 4s 90ms/step - loss: 0.0060 - accuracy: 0.9982\n",
      "Epoch 642/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0050 - accuracy: 0.9980\n",
      "Epoch 642: accuracy did not improve from 0.99841\n",
      "42/42 [==============================] - 4s 92ms/step - loss: 0.0050 - accuracy: 0.9980\n",
      "Epoch 643/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0074 - accuracy: 0.9975\n",
      "Epoch 643: accuracy did not improve from 0.99841\n",
      "42/42 [==============================] - 4s 91ms/step - loss: 0.0074 - accuracy: 0.9975\n",
      "Epoch 644/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0085 - accuracy: 0.9972\n",
      "Epoch 644: accuracy did not improve from 0.99841\n",
      "42/42 [==============================] - 4s 92ms/step - loss: 0.0085 - accuracy: 0.9972\n",
      "Epoch 645/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0100 - accuracy: 0.9974\n",
      "Epoch 645: accuracy did not improve from 0.99841\n",
      "42/42 [==============================] - 4s 91ms/step - loss: 0.0100 - accuracy: 0.9974\n",
      "Epoch 646/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0073 - accuracy: 0.9977\n",
      "Epoch 646: accuracy did not improve from 0.99841\n",
      "42/42 [==============================] - 4s 92ms/step - loss: 0.0073 - accuracy: 0.9977\n",
      "Epoch 647/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0078 - accuracy: 0.9973\n",
      "Epoch 647: accuracy did not improve from 0.99841\n",
      "42/42 [==============================] - 4s 94ms/step - loss: 0.0078 - accuracy: 0.9973\n",
      "Epoch 648/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0063 - accuracy: 0.9979\n",
      "Epoch 648: accuracy did not improve from 0.99841\n",
      "42/42 [==============================] - 4s 91ms/step - loss: 0.0063 - accuracy: 0.9979\n",
      "Epoch 649/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0070 - accuracy: 0.9976\n",
      "Epoch 649: accuracy did not improve from 0.99841\n",
      "42/42 [==============================] - 4s 92ms/step - loss: 0.0070 - accuracy: 0.9976\n",
      "Epoch 650/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0043 - accuracy: 0.9984\n",
      "Epoch 650: accuracy did not improve from 0.99841\n",
      "42/42 [==============================] - 4s 91ms/step - loss: 0.0043 - accuracy: 0.9984\n",
      "Epoch 651/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0106 - accuracy: 0.9975\n",
      "Epoch 651: accuracy did not improve from 0.99841\n",
      "42/42 [==============================] - 4s 91ms/step - loss: 0.0106 - accuracy: 0.9975\n",
      "Epoch 652/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0073 - accuracy: 0.9978\n",
      "Epoch 652: accuracy did not improve from 0.99841\n",
      "42/42 [==============================] - 4s 91ms/step - loss: 0.0073 - accuracy: 0.9978\n",
      "Epoch 653/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0069 - accuracy: 0.9979\n",
      "Epoch 653: accuracy did not improve from 0.99841\n",
      "42/42 [==============================] - 4s 91ms/step - loss: 0.0069 - accuracy: 0.9979\n",
      "Epoch 654/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0098 - accuracy: 0.9975\n",
      "Epoch 654: accuracy did not improve from 0.99841\n",
      "42/42 [==============================] - 4s 91ms/step - loss: 0.0098 - accuracy: 0.9975\n",
      "Epoch 655/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0128 - accuracy: 0.9971\n",
      "Epoch 655: accuracy did not improve from 0.99841\n",
      "42/42 [==============================] - 4s 92ms/step - loss: 0.0128 - accuracy: 0.9971\n",
      "Epoch 656/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0164 - accuracy: 0.9959\n",
      "Epoch 656: accuracy did not improve from 0.99841\n",
      "42/42 [==============================] - 4s 94ms/step - loss: 0.0164 - accuracy: 0.9959\n",
      "Epoch 657/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0127 - accuracy: 0.9964\n",
      "Epoch 657: accuracy did not improve from 0.99841\n",
      "42/42 [==============================] - 4s 93ms/step - loss: 0.0127 - accuracy: 0.9964\n",
      "Epoch 658/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0133 - accuracy: 0.9965\n",
      "Epoch 658: accuracy did not improve from 0.99841\n",
      "42/42 [==============================] - 4s 93ms/step - loss: 0.0133 - accuracy: 0.9965\n",
      "Epoch 659/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0134 - accuracy: 0.9967\n",
      "Epoch 659: accuracy did not improve from 0.99841\n",
      "42/42 [==============================] - 4s 93ms/step - loss: 0.0134 - accuracy: 0.9967\n",
      "Epoch 660/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0132 - accuracy: 0.9970\n",
      "Epoch 660: accuracy did not improve from 0.99841\n",
      "42/42 [==============================] - 4s 91ms/step - loss: 0.0132 - accuracy: 0.9970\n",
      "Epoch 661/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0193 - accuracy: 0.9956\n",
      "Epoch 661: accuracy did not improve from 0.99841\n",
      "42/42 [==============================] - 4s 90ms/step - loss: 0.0193 - accuracy: 0.9956\n",
      "Epoch 662/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0200 - accuracy: 0.9954\n",
      "Epoch 662: accuracy did not improve from 0.99841\n",
      "42/42 [==============================] - 4s 91ms/step - loss: 0.0200 - accuracy: 0.9954\n",
      "Epoch 663/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0135 - accuracy: 0.9970\n",
      "Epoch 663: accuracy did not improve from 0.99841\n",
      "42/42 [==============================] - 4s 91ms/step - loss: 0.0135 - accuracy: 0.9970\n",
      "Epoch 664/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0133 - accuracy: 0.9963\n",
      "Epoch 664: accuracy did not improve from 0.99841\n",
      "42/42 [==============================] - 4s 99ms/step - loss: 0.0133 - accuracy: 0.9963\n",
      "Epoch 665/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0136 - accuracy: 0.9957\n",
      "Epoch 665: accuracy did not improve from 0.99841\n",
      "42/42 [==============================] - 4s 96ms/step - loss: 0.0136 - accuracy: 0.9957\n",
      "Epoch 666/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0139 - accuracy: 0.9968\n",
      "Epoch 666: accuracy did not improve from 0.99841\n",
      "42/42 [==============================] - 4s 95ms/step - loss: 0.0139 - accuracy: 0.9968\n",
      "Epoch 667/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0107 - accuracy: 0.9959\n",
      "Epoch 667: accuracy did not improve from 0.99841\n",
      "42/42 [==============================] - 4s 93ms/step - loss: 0.0107 - accuracy: 0.9959\n",
      "Epoch 668/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0139 - accuracy: 0.9963\n",
      "Epoch 668: accuracy did not improve from 0.99841\n",
      "42/42 [==============================] - 4s 92ms/step - loss: 0.0139 - accuracy: 0.9963\n",
      "Epoch 669/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0073 - accuracy: 0.9969\n",
      "Epoch 669: accuracy did not improve from 0.99841\n",
      "42/42 [==============================] - 4s 92ms/step - loss: 0.0073 - accuracy: 0.9969\n",
      "Epoch 670/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0100 - accuracy: 0.9964\n",
      "Epoch 670: accuracy did not improve from 0.99841\n",
      "42/42 [==============================] - 4s 92ms/step - loss: 0.0100 - accuracy: 0.9964\n",
      "Epoch 671/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0150 - accuracy: 0.9964\n",
      "Epoch 671: accuracy did not improve from 0.99841\n",
      "42/42 [==============================] - 4s 92ms/step - loss: 0.0150 - accuracy: 0.9964\n",
      "Epoch 672/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0096 - accuracy: 0.9973\n",
      "Epoch 672: accuracy did not improve from 0.99841\n",
      "42/42 [==============================] - 4s 93ms/step - loss: 0.0096 - accuracy: 0.9973\n",
      "Epoch 673/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0160 - accuracy: 0.9964\n",
      "Epoch 673: accuracy did not improve from 0.99841\n",
      "42/42 [==============================] - 4s 92ms/step - loss: 0.0160 - accuracy: 0.9964\n",
      "Epoch 674/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0154 - accuracy: 0.9955\n",
      "Epoch 674: accuracy did not improve from 0.99841\n",
      "42/42 [==============================] - 4s 93ms/step - loss: 0.0154 - accuracy: 0.9955\n",
      "Epoch 675/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0131 - accuracy: 0.9963\n",
      "Epoch 675: accuracy did not improve from 0.99841\n",
      "42/42 [==============================] - 4s 93ms/step - loss: 0.0131 - accuracy: 0.9963\n",
      "Epoch 676/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0143 - accuracy: 0.9966\n",
      "Epoch 676: accuracy did not improve from 0.99841\n",
      "42/42 [==============================] - 4s 91ms/step - loss: 0.0143 - accuracy: 0.9966\n",
      "Epoch 677/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0064 - accuracy: 0.9979\n",
      "Epoch 677: accuracy did not improve from 0.99841\n",
      "42/42 [==============================] - 4s 92ms/step - loss: 0.0064 - accuracy: 0.9979\n",
      "Epoch 678/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0108 - accuracy: 0.9975\n",
      "Epoch 678: accuracy did not improve from 0.99841\n",
      "42/42 [==============================] - 4s 92ms/step - loss: 0.0108 - accuracy: 0.9975\n",
      "Epoch 679/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0058 - accuracy: 0.9985\n",
      "Epoch 679: accuracy improved from 0.99841 to 0.99850, saving model to eq_solver.h5\n",
      "42/42 [==============================] - 4s 93ms/step - loss: 0.0058 - accuracy: 0.9985\n",
      "Epoch 680/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0087 - accuracy: 0.9972\n",
      "Epoch 680: accuracy did not improve from 0.99850\n",
      "42/42 [==============================] - 4s 93ms/step - loss: 0.0087 - accuracy: 0.9972\n",
      "Epoch 681/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0100 - accuracy: 0.9969\n",
      "Epoch 681: accuracy did not improve from 0.99850\n",
      "42/42 [==============================] - 4s 92ms/step - loss: 0.0100 - accuracy: 0.9969\n",
      "Epoch 682/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0072 - accuracy: 0.9977\n",
      "Epoch 682: accuracy did not improve from 0.99850\n",
      "42/42 [==============================] - 4s 93ms/step - loss: 0.0072 - accuracy: 0.9977\n",
      "Epoch 683/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0064 - accuracy: 0.9978\n",
      "Epoch 683: accuracy did not improve from 0.99850\n",
      "42/42 [==============================] - 4s 92ms/step - loss: 0.0064 - accuracy: 0.9978\n",
      "Epoch 684/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0063 - accuracy: 0.9978\n",
      "Epoch 684: accuracy did not improve from 0.99850\n",
      "42/42 [==============================] - 4s 91ms/step - loss: 0.0063 - accuracy: 0.9978\n",
      "Epoch 685/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0107 - accuracy: 0.9967\n",
      "Epoch 685: accuracy did not improve from 0.99850\n",
      "42/42 [==============================] - 4s 91ms/step - loss: 0.0107 - accuracy: 0.9967\n",
      "Epoch 686/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0105 - accuracy: 0.9970\n",
      "Epoch 686: accuracy did not improve from 0.99850\n",
      "42/42 [==============================] - 4s 92ms/step - loss: 0.0105 - accuracy: 0.9970\n",
      "Epoch 687/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0083 - accuracy: 0.9973\n",
      "Epoch 687: accuracy did not improve from 0.99850\n",
      "42/42 [==============================] - 4s 91ms/step - loss: 0.0083 - accuracy: 0.9973\n",
      "Epoch 688/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0068 - accuracy: 0.9983\n",
      "Epoch 688: accuracy did not improve from 0.99850\n",
      "42/42 [==============================] - 4s 91ms/step - loss: 0.0068 - accuracy: 0.9983\n",
      "Epoch 689/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0095 - accuracy: 0.9973\n",
      "Epoch 689: accuracy did not improve from 0.99850\n",
      "42/42 [==============================] - 4s 92ms/step - loss: 0.0095 - accuracy: 0.9973\n",
      "Epoch 690/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0103 - accuracy: 0.9971\n",
      "Epoch 690: accuracy did not improve from 0.99850\n",
      "42/42 [==============================] - 4s 92ms/step - loss: 0.0103 - accuracy: 0.9971\n",
      "Epoch 691/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0081 - accuracy: 0.9972\n",
      "Epoch 691: accuracy did not improve from 0.99850\n",
      "42/42 [==============================] - 4s 100ms/step - loss: 0.0081 - accuracy: 0.9972\n",
      "Epoch 692/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0123 - accuracy: 0.9967\n",
      "Epoch 692: accuracy did not improve from 0.99850\n",
      "42/42 [==============================] - 4s 98ms/step - loss: 0.0123 - accuracy: 0.9967\n",
      "Epoch 693/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0060 - accuracy: 0.9978\n",
      "Epoch 693: accuracy did not improve from 0.99850\n",
      "42/42 [==============================] - 4s 99ms/step - loss: 0.0060 - accuracy: 0.9978\n",
      "Epoch 694/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0067 - accuracy: 0.9972\n",
      "Epoch 694: accuracy did not improve from 0.99850\n",
      "42/42 [==============================] - 4s 92ms/step - loss: 0.0067 - accuracy: 0.9972\n",
      "Epoch 695/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0095 - accuracy: 0.9963\n",
      "Epoch 695: accuracy did not improve from 0.99850\n",
      "42/42 [==============================] - 4s 93ms/step - loss: 0.0095 - accuracy: 0.9963\n",
      "Epoch 696/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0137 - accuracy: 0.9967\n",
      "Epoch 696: accuracy did not improve from 0.99850\n",
      "42/42 [==============================] - 4s 89ms/step - loss: 0.0137 - accuracy: 0.9967\n",
      "Epoch 697/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0127 - accuracy: 0.9966\n",
      "Epoch 697: accuracy did not improve from 0.99850\n",
      "42/42 [==============================] - 4s 88ms/step - loss: 0.0127 - accuracy: 0.9966\n",
      "Epoch 698/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0062 - accuracy: 0.9978\n",
      "Epoch 698: accuracy did not improve from 0.99850\n",
      "42/42 [==============================] - 4s 91ms/step - loss: 0.0062 - accuracy: 0.9978\n",
      "Epoch 699/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0110 - accuracy: 0.9964\n",
      "Epoch 699: accuracy did not improve from 0.99850\n",
      "42/42 [==============================] - 4s 91ms/step - loss: 0.0110 - accuracy: 0.9964\n",
      "Epoch 700/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0098 - accuracy: 0.9976\n",
      "Epoch 700: accuracy did not improve from 0.99850\n",
      "42/42 [==============================] - 4s 93ms/step - loss: 0.0098 - accuracy: 0.9976\n",
      "Epoch 701/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0072 - accuracy: 0.9976\n",
      "Epoch 701: accuracy did not improve from 0.99850\n",
      "42/42 [==============================] - 4s 90ms/step - loss: 0.0072 - accuracy: 0.9976\n",
      "Epoch 702/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0102 - accuracy: 0.9969\n",
      "Epoch 702: accuracy did not improve from 0.99850\n",
      "42/42 [==============================] - 4s 91ms/step - loss: 0.0102 - accuracy: 0.9969\n",
      "Epoch 703/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0076 - accuracy: 0.9976\n",
      "Epoch 703: accuracy did not improve from 0.99850\n",
      "42/42 [==============================] - 4s 91ms/step - loss: 0.0076 - accuracy: 0.9976\n",
      "Epoch 704/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0088 - accuracy: 0.9978\n",
      "Epoch 704: accuracy did not improve from 0.99850\n",
      "42/42 [==============================] - 4s 90ms/step - loss: 0.0088 - accuracy: 0.9978\n",
      "Epoch 705/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0166 - accuracy: 0.9959\n",
      "Epoch 705: accuracy did not improve from 0.99850\n",
      "42/42 [==============================] - 4s 93ms/step - loss: 0.0166 - accuracy: 0.9959\n",
      "Epoch 706/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0083 - accuracy: 0.9972\n",
      "Epoch 706: accuracy did not improve from 0.99850\n",
      "42/42 [==============================] - 4s 94ms/step - loss: 0.0083 - accuracy: 0.9972\n",
      "Epoch 707/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0094 - accuracy: 0.9971\n",
      "Epoch 707: accuracy did not improve from 0.99850\n",
      "42/42 [==============================] - 4s 90ms/step - loss: 0.0094 - accuracy: 0.9971\n",
      "Epoch 708/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0098 - accuracy: 0.9973\n",
      "Epoch 708: accuracy did not improve from 0.99850\n",
      "42/42 [==============================] - 4s 93ms/step - loss: 0.0098 - accuracy: 0.9973\n",
      "Epoch 709/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0073 - accuracy: 0.9977\n",
      "Epoch 709: accuracy did not improve from 0.99850\n",
      "42/42 [==============================] - 4s 90ms/step - loss: 0.0073 - accuracy: 0.9977\n",
      "Epoch 710/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0082 - accuracy: 0.9974\n",
      "Epoch 710: accuracy did not improve from 0.99850\n",
      "42/42 [==============================] - 4s 93ms/step - loss: 0.0082 - accuracy: 0.9974\n",
      "Epoch 711/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0095 - accuracy: 0.9970\n",
      "Epoch 711: accuracy did not improve from 0.99850\n",
      "42/42 [==============================] - 4s 92ms/step - loss: 0.0095 - accuracy: 0.9970\n",
      "Epoch 712/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0079 - accuracy: 0.9979\n",
      "Epoch 712: accuracy did not improve from 0.99850\n",
      "42/42 [==============================] - 4s 92ms/step - loss: 0.0079 - accuracy: 0.9979\n",
      "Epoch 713/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0051 - accuracy: 0.9986\n",
      "Epoch 713: accuracy improved from 0.99850 to 0.99860, saving model to eq_solver.h5\n",
      "42/42 [==============================] - 4s 92ms/step - loss: 0.0051 - accuracy: 0.9986\n",
      "Epoch 714/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0057 - accuracy: 0.9985\n",
      "Epoch 714: accuracy did not improve from 0.99860\n",
      "42/42 [==============================] - 4s 92ms/step - loss: 0.0057 - accuracy: 0.9985\n",
      "Epoch 715/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0066 - accuracy: 0.9981\n",
      "Epoch 715: accuracy did not improve from 0.99860\n",
      "42/42 [==============================] - 4s 93ms/step - loss: 0.0066 - accuracy: 0.9981\n",
      "Epoch 716/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0086 - accuracy: 0.9978\n",
      "Epoch 716: accuracy did not improve from 0.99860\n",
      "42/42 [==============================] - 4s 92ms/step - loss: 0.0086 - accuracy: 0.9978\n",
      "Epoch 717/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0069 - accuracy: 0.9978\n",
      "Epoch 717: accuracy did not improve from 0.99860\n",
      "42/42 [==============================] - 4s 92ms/step - loss: 0.0069 - accuracy: 0.9978\n",
      "Epoch 718/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0068 - accuracy: 0.9975\n",
      "Epoch 718: accuracy did not improve from 0.99860\n",
      "42/42 [==============================] - 4s 94ms/step - loss: 0.0068 - accuracy: 0.9975\n",
      "Epoch 719/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0082 - accuracy: 0.9976\n",
      "Epoch 719: accuracy did not improve from 0.99860\n",
      "42/42 [==============================] - 4s 93ms/step - loss: 0.0082 - accuracy: 0.9976\n",
      "Epoch 720/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0108 - accuracy: 0.9970\n",
      "Epoch 720: accuracy did not improve from 0.99860\n",
      "42/42 [==============================] - 4s 91ms/step - loss: 0.0108 - accuracy: 0.9970\n",
      "Epoch 721/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0152 - accuracy: 0.9960\n",
      "Epoch 721: accuracy did not improve from 0.99860\n",
      "42/42 [==============================] - 4s 95ms/step - loss: 0.0152 - accuracy: 0.9960\n",
      "Epoch 722/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0152 - accuracy: 0.9960\n",
      "Epoch 722: accuracy did not improve from 0.99860\n",
      "42/42 [==============================] - 4s 90ms/step - loss: 0.0152 - accuracy: 0.9960\n",
      "Epoch 723/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0096 - accuracy: 0.9971\n",
      "Epoch 723: accuracy did not improve from 0.99860\n",
      "42/42 [==============================] - 4s 92ms/step - loss: 0.0096 - accuracy: 0.9971\n",
      "Epoch 724/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0079 - accuracy: 0.9979\n",
      "Epoch 724: accuracy did not improve from 0.99860\n",
      "42/42 [==============================] - 4s 92ms/step - loss: 0.0079 - accuracy: 0.9979\n",
      "Epoch 725/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0070 - accuracy: 0.9979\n",
      "Epoch 725: accuracy did not improve from 0.99860\n",
      "42/42 [==============================] - 4s 91ms/step - loss: 0.0070 - accuracy: 0.9979\n",
      "Epoch 726/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0061 - accuracy: 0.9978\n",
      "Epoch 726: accuracy did not improve from 0.99860\n",
      "42/42 [==============================] - 4s 92ms/step - loss: 0.0061 - accuracy: 0.9978\n",
      "Epoch 727/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0091 - accuracy: 0.9971\n",
      "Epoch 727: accuracy did not improve from 0.99860\n",
      "42/42 [==============================] - 4s 90ms/step - loss: 0.0091 - accuracy: 0.9971\n",
      "Epoch 728/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0055 - accuracy: 0.9981\n",
      "Epoch 728: accuracy did not improve from 0.99860\n",
      "42/42 [==============================] - 4s 89ms/step - loss: 0.0055 - accuracy: 0.9981\n",
      "Epoch 729/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0066 - accuracy: 0.9980\n",
      "Epoch 729: accuracy did not improve from 0.99860\n",
      "42/42 [==============================] - 4s 90ms/step - loss: 0.0066 - accuracy: 0.9980\n",
      "Epoch 730/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0072 - accuracy: 0.9974\n",
      "Epoch 730: accuracy did not improve from 0.99860\n",
      "42/42 [==============================] - 4s 92ms/step - loss: 0.0072 - accuracy: 0.9974\n",
      "Epoch 731/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0065 - accuracy: 0.9975\n",
      "Epoch 731: accuracy did not improve from 0.99860\n",
      "42/42 [==============================] - 4s 91ms/step - loss: 0.0065 - accuracy: 0.9975\n",
      "Epoch 732/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0087 - accuracy: 0.9978\n",
      "Epoch 732: accuracy did not improve from 0.99860\n",
      "42/42 [==============================] - 4s 91ms/step - loss: 0.0087 - accuracy: 0.9978\n",
      "Epoch 733/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0133 - accuracy: 0.9970\n",
      "Epoch 733: accuracy did not improve from 0.99860\n",
      "42/42 [==============================] - 4s 92ms/step - loss: 0.0133 - accuracy: 0.9970\n",
      "Epoch 734/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0128 - accuracy: 0.9961\n",
      "Epoch 734: accuracy did not improve from 0.99860\n",
      "42/42 [==============================] - 4s 93ms/step - loss: 0.0128 - accuracy: 0.9961\n",
      "Epoch 735/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0084 - accuracy: 0.9972\n",
      "Epoch 735: accuracy did not improve from 0.99860\n",
      "42/42 [==============================] - 4s 91ms/step - loss: 0.0084 - accuracy: 0.9972\n",
      "Epoch 736/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0105 - accuracy: 0.9979\n",
      "Epoch 736: accuracy did not improve from 0.99860\n",
      "42/42 [==============================] - 4s 90ms/step - loss: 0.0105 - accuracy: 0.9979\n",
      "Epoch 737/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0117 - accuracy: 0.9973\n",
      "Epoch 737: accuracy did not improve from 0.99860\n",
      "42/42 [==============================] - 4s 95ms/step - loss: 0.0117 - accuracy: 0.9973\n",
      "Epoch 738/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0118 - accuracy: 0.9969\n",
      "Epoch 738: accuracy did not improve from 0.99860\n",
      "42/42 [==============================] - 4s 90ms/step - loss: 0.0118 - accuracy: 0.9969\n",
      "Epoch 739/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0094 - accuracy: 0.9977\n",
      "Epoch 739: accuracy did not improve from 0.99860\n",
      "42/42 [==============================] - 4s 93ms/step - loss: 0.0094 - accuracy: 0.9977\n",
      "Epoch 740/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0084 - accuracy: 0.9977\n",
      "Epoch 740: accuracy did not improve from 0.99860\n",
      "42/42 [==============================] - 4s 90ms/step - loss: 0.0084 - accuracy: 0.9977\n",
      "Epoch 741/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0047 - accuracy: 0.9981\n",
      "Epoch 741: accuracy did not improve from 0.99860\n",
      "42/42 [==============================] - 4s 92ms/step - loss: 0.0047 - accuracy: 0.9981\n",
      "Epoch 742/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0067 - accuracy: 0.9978\n",
      "Epoch 742: accuracy did not improve from 0.99860\n",
      "42/42 [==============================] - 4s 93ms/step - loss: 0.0067 - accuracy: 0.9978\n",
      "Epoch 743/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0149 - accuracy: 0.9965\n",
      "Epoch 743: accuracy did not improve from 0.99860\n",
      "42/42 [==============================] - 4s 91ms/step - loss: 0.0149 - accuracy: 0.9965\n",
      "Epoch 744/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0087 - accuracy: 0.9974\n",
      "Epoch 744: accuracy did not improve from 0.99860\n",
      "42/42 [==============================] - 4s 91ms/step - loss: 0.0087 - accuracy: 0.9974\n",
      "Epoch 745/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0153 - accuracy: 0.9965\n",
      "Epoch 745: accuracy did not improve from 0.99860\n",
      "42/42 [==============================] - 4s 92ms/step - loss: 0.0153 - accuracy: 0.9965\n",
      "Epoch 746/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0122 - accuracy: 0.9964\n",
      "Epoch 746: accuracy did not improve from 0.99860\n",
      "42/42 [==============================] - 4s 91ms/step - loss: 0.0122 - accuracy: 0.9964\n",
      "Epoch 747/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0148 - accuracy: 0.9961\n",
      "Epoch 747: accuracy did not improve from 0.99860\n",
      "42/42 [==============================] - 4s 90ms/step - loss: 0.0148 - accuracy: 0.9961\n",
      "Epoch 748/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0082 - accuracy: 0.9971\n",
      "Epoch 748: accuracy did not improve from 0.99860\n",
      "42/42 [==============================] - 4s 91ms/step - loss: 0.0082 - accuracy: 0.9971\n",
      "Epoch 749/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0088 - accuracy: 0.9978\n",
      "Epoch 749: accuracy did not improve from 0.99860\n",
      "42/42 [==============================] - 4s 91ms/step - loss: 0.0088 - accuracy: 0.9978\n",
      "Epoch 750/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0066 - accuracy: 0.9981\n",
      "Epoch 750: accuracy did not improve from 0.99860\n",
      "42/42 [==============================] - 4s 95ms/step - loss: 0.0066 - accuracy: 0.9981\n",
      "Epoch 751/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0075 - accuracy: 0.9978\n",
      "Epoch 751: accuracy did not improve from 0.99860\n",
      "42/42 [==============================] - 4s 94ms/step - loss: 0.0075 - accuracy: 0.9978\n",
      "Epoch 752/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0053 - accuracy: 0.9981\n",
      "Epoch 752: accuracy did not improve from 0.99860\n",
      "42/42 [==============================] - 4s 91ms/step - loss: 0.0053 - accuracy: 0.9981\n",
      "Epoch 753/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0116 - accuracy: 0.9967\n",
      "Epoch 753: accuracy did not improve from 0.99860\n",
      "42/42 [==============================] - 4s 96ms/step - loss: 0.0116 - accuracy: 0.9967\n",
      "Epoch 754/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0108 - accuracy: 0.9963\n",
      "Epoch 754: accuracy did not improve from 0.99860\n",
      "42/42 [==============================] - 4s 92ms/step - loss: 0.0108 - accuracy: 0.9963\n",
      "Epoch 755/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0084 - accuracy: 0.9971\n",
      "Epoch 755: accuracy did not improve from 0.99860\n",
      "42/42 [==============================] - 4s 93ms/step - loss: 0.0084 - accuracy: 0.9971\n",
      "Epoch 756/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0078 - accuracy: 0.9974\n",
      "Epoch 756: accuracy did not improve from 0.99860\n",
      "42/42 [==============================] - 4s 93ms/step - loss: 0.0078 - accuracy: 0.9974\n",
      "Epoch 757/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0083 - accuracy: 0.9976\n",
      "Epoch 757: accuracy did not improve from 0.99860\n",
      "42/42 [==============================] - 4s 93ms/step - loss: 0.0083 - accuracy: 0.9976\n",
      "Epoch 758/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0066 - accuracy: 0.9980\n",
      "Epoch 758: accuracy did not improve from 0.99860\n",
      "42/42 [==============================] - 4s 91ms/step - loss: 0.0066 - accuracy: 0.9980\n",
      "Epoch 759/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0061 - accuracy: 0.9980\n",
      "Epoch 759: accuracy did not improve from 0.99860\n",
      "42/42 [==============================] - 4s 90ms/step - loss: 0.0061 - accuracy: 0.9980\n",
      "Epoch 760/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0055 - accuracy: 0.9976\n",
      "Epoch 760: accuracy did not improve from 0.99860\n",
      "42/42 [==============================] - 4s 93ms/step - loss: 0.0055 - accuracy: 0.9976\n",
      "Epoch 761/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0070 - accuracy: 0.9975\n",
      "Epoch 761: accuracy did not improve from 0.99860\n",
      "42/42 [==============================] - 4s 90ms/step - loss: 0.0070 - accuracy: 0.9975\n",
      "Epoch 762/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0055 - accuracy: 0.9978\n",
      "Epoch 762: accuracy did not improve from 0.99860\n",
      "42/42 [==============================] - 4s 91ms/step - loss: 0.0055 - accuracy: 0.9978\n",
      "Epoch 763/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0079 - accuracy: 0.9979\n",
      "Epoch 763: accuracy did not improve from 0.99860\n",
      "42/42 [==============================] - 4s 89ms/step - loss: 0.0079 - accuracy: 0.9979\n",
      "Epoch 764/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0092 - accuracy: 0.9974\n",
      "Epoch 764: accuracy did not improve from 0.99860\n",
      "42/42 [==============================] - 4s 92ms/step - loss: 0.0092 - accuracy: 0.9974\n",
      "Epoch 765/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0103 - accuracy: 0.9965\n",
      "Epoch 765: accuracy did not improve from 0.99860\n",
      "42/42 [==============================] - 4s 92ms/step - loss: 0.0103 - accuracy: 0.9965\n",
      "Epoch 766/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0063 - accuracy: 0.9978\n",
      "Epoch 766: accuracy did not improve from 0.99860\n",
      "42/42 [==============================] - 4s 93ms/step - loss: 0.0063 - accuracy: 0.9978\n",
      "Epoch 767/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0105 - accuracy: 0.9967\n",
      "Epoch 767: accuracy did not improve from 0.99860\n",
      "42/42 [==============================] - 4s 91ms/step - loss: 0.0105 - accuracy: 0.9967\n",
      "Epoch 768/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0111 - accuracy: 0.9958\n",
      "Epoch 768: accuracy did not improve from 0.99860\n",
      "42/42 [==============================] - 4s 93ms/step - loss: 0.0111 - accuracy: 0.9958\n",
      "Epoch 769/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0067 - accuracy: 0.9982\n",
      "Epoch 769: accuracy did not improve from 0.99860\n",
      "42/42 [==============================] - 4s 92ms/step - loss: 0.0067 - accuracy: 0.9982\n",
      "Epoch 770/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0068 - accuracy: 0.9979\n",
      "Epoch 770: accuracy did not improve from 0.99860\n",
      "42/42 [==============================] - 4s 90ms/step - loss: 0.0068 - accuracy: 0.9979\n",
      "Epoch 771/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0110 - accuracy: 0.9966\n",
      "Epoch 771: accuracy did not improve from 0.99860\n",
      "42/42 [==============================] - 4s 91ms/step - loss: 0.0110 - accuracy: 0.9966\n",
      "Epoch 772/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0141 - accuracy: 0.9964\n",
      "Epoch 772: accuracy did not improve from 0.99860\n",
      "42/42 [==============================] - 4s 91ms/step - loss: 0.0141 - accuracy: 0.9964\n",
      "Epoch 773/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0094 - accuracy: 0.9971\n",
      "Epoch 773: accuracy did not improve from 0.99860\n",
      "42/42 [==============================] - 4s 92ms/step - loss: 0.0094 - accuracy: 0.9971\n",
      "Epoch 774/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0084 - accuracy: 0.9975\n",
      "Epoch 774: accuracy did not improve from 0.99860\n",
      "42/42 [==============================] - 4s 90ms/step - loss: 0.0084 - accuracy: 0.9975\n",
      "Epoch 775/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0117 - accuracy: 0.9973\n",
      "Epoch 775: accuracy did not improve from 0.99860\n",
      "42/42 [==============================] - 4s 90ms/step - loss: 0.0117 - accuracy: 0.9973\n",
      "Epoch 776/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0079 - accuracy: 0.9978\n",
      "Epoch 776: accuracy did not improve from 0.99860\n",
      "42/42 [==============================] - 4s 90ms/step - loss: 0.0079 - accuracy: 0.9978\n",
      "Epoch 777/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0054 - accuracy: 0.9983\n",
      "Epoch 777: accuracy did not improve from 0.99860\n",
      "42/42 [==============================] - 4s 90ms/step - loss: 0.0054 - accuracy: 0.9983\n",
      "Epoch 778/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0095 - accuracy: 0.9975\n",
      "Epoch 778: accuracy did not improve from 0.99860\n",
      "42/42 [==============================] - 4s 89ms/step - loss: 0.0095 - accuracy: 0.9975\n",
      "Epoch 779/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0102 - accuracy: 0.9974\n",
      "Epoch 779: accuracy did not improve from 0.99860\n",
      "42/42 [==============================] - 4s 89ms/step - loss: 0.0102 - accuracy: 0.9974\n",
      "Epoch 780/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0077 - accuracy: 0.9978\n",
      "Epoch 780: accuracy did not improve from 0.99860\n",
      "42/42 [==============================] - 4s 90ms/step - loss: 0.0077 - accuracy: 0.9978\n",
      "Epoch 781/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0101 - accuracy: 0.9967\n",
      "Epoch 781: accuracy did not improve from 0.99860\n",
      "42/42 [==============================] - 4s 91ms/step - loss: 0.0101 - accuracy: 0.9967\n",
      "Epoch 782/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0091 - accuracy: 0.9973\n",
      "Epoch 782: accuracy did not improve from 0.99860\n",
      "42/42 [==============================] - 4s 91ms/step - loss: 0.0091 - accuracy: 0.9973\n",
      "Epoch 783/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0070 - accuracy: 0.9976\n",
      "Epoch 783: accuracy did not improve from 0.99860\n",
      "42/42 [==============================] - 4s 90ms/step - loss: 0.0070 - accuracy: 0.9976\n",
      "Epoch 784/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0057 - accuracy: 0.9978\n",
      "Epoch 784: accuracy did not improve from 0.99860\n",
      "42/42 [==============================] - 4s 92ms/step - loss: 0.0057 - accuracy: 0.9978\n",
      "Epoch 785/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0081 - accuracy: 0.9980\n",
      "Epoch 785: accuracy did not improve from 0.99860\n",
      "42/42 [==============================] - 4s 93ms/step - loss: 0.0081 - accuracy: 0.9980\n",
      "Epoch 786/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0109 - accuracy: 0.9970\n",
      "Epoch 786: accuracy did not improve from 0.99860\n",
      "42/42 [==============================] - 4s 93ms/step - loss: 0.0109 - accuracy: 0.9970\n",
      "Epoch 787/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0090 - accuracy: 0.9971\n",
      "Epoch 787: accuracy did not improve from 0.99860\n",
      "42/42 [==============================] - 4s 92ms/step - loss: 0.0090 - accuracy: 0.9971\n",
      "Epoch 788/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0098 - accuracy: 0.9974\n",
      "Epoch 788: accuracy did not improve from 0.99860\n",
      "42/42 [==============================] - 4s 91ms/step - loss: 0.0098 - accuracy: 0.9974\n",
      "Epoch 789/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0063 - accuracy: 0.9978\n",
      "Epoch 789: accuracy did not improve from 0.99860\n",
      "42/42 [==============================] - 4s 94ms/step - loss: 0.0063 - accuracy: 0.9978\n",
      "Epoch 790/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0098 - accuracy: 0.9973\n",
      "Epoch 790: accuracy did not improve from 0.99860\n",
      "42/42 [==============================] - 4s 91ms/step - loss: 0.0098 - accuracy: 0.9973\n",
      "Epoch 791/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0065 - accuracy: 0.9975\n",
      "Epoch 791: accuracy did not improve from 0.99860\n",
      "42/42 [==============================] - 4s 90ms/step - loss: 0.0065 - accuracy: 0.9975\n",
      "Epoch 792/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0113 - accuracy: 0.9965\n",
      "Epoch 792: accuracy did not improve from 0.99860\n",
      "42/42 [==============================] - 4s 91ms/step - loss: 0.0113 - accuracy: 0.9965\n",
      "Epoch 793/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0099 - accuracy: 0.9976\n",
      "Epoch 793: accuracy did not improve from 0.99860\n",
      "42/42 [==============================] - 4s 97ms/step - loss: 0.0099 - accuracy: 0.9976\n",
      "Epoch 794/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0139 - accuracy: 0.9971\n",
      "Epoch 794: accuracy did not improve from 0.99860\n",
      "42/42 [==============================] - 5s 116ms/step - loss: 0.0139 - accuracy: 0.9971\n",
      "Epoch 795/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0081 - accuracy: 0.9980\n",
      "Epoch 795: accuracy did not improve from 0.99860\n",
      "42/42 [==============================] - 5s 118ms/step - loss: 0.0081 - accuracy: 0.9980\n",
      "Epoch 796/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0088 - accuracy: 0.9971\n",
      "Epoch 796: accuracy did not improve from 0.99860\n",
      "42/42 [==============================] - 5s 114ms/step - loss: 0.0088 - accuracy: 0.9971\n",
      "Epoch 797/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0089 - accuracy: 0.9982\n",
      "Epoch 797: accuracy did not improve from 0.99860\n",
      "42/42 [==============================] - 5s 107ms/step - loss: 0.0089 - accuracy: 0.9982\n",
      "Epoch 798/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0107 - accuracy: 0.9969\n",
      "Epoch 798: accuracy did not improve from 0.99860\n",
      "42/42 [==============================] - 4s 96ms/step - loss: 0.0107 - accuracy: 0.9969\n",
      "Epoch 799/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0071 - accuracy: 0.9978\n",
      "Epoch 799: accuracy did not improve from 0.99860\n",
      "42/42 [==============================] - 4s 97ms/step - loss: 0.0071 - accuracy: 0.9978\n",
      "Epoch 800/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0110 - accuracy: 0.9967\n",
      "Epoch 800: accuracy did not improve from 0.99860\n",
      "42/42 [==============================] - 4s 98ms/step - loss: 0.0110 - accuracy: 0.9967\n",
      "Epoch 801/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0163 - accuracy: 0.9958\n",
      "Epoch 801: accuracy did not improve from 0.99860\n",
      "42/42 [==============================] - 4s 96ms/step - loss: 0.0163 - accuracy: 0.9958\n",
      "Epoch 802/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0070 - accuracy: 0.9979\n",
      "Epoch 802: accuracy did not improve from 0.99860\n",
      "42/42 [==============================] - 4s 97ms/step - loss: 0.0070 - accuracy: 0.9979\n",
      "Epoch 803/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0086 - accuracy: 0.9975\n",
      "Epoch 803: accuracy did not improve from 0.99860\n",
      "42/42 [==============================] - 4s 96ms/step - loss: 0.0086 - accuracy: 0.9975\n",
      "Epoch 804/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0095 - accuracy: 0.9971\n",
      "Epoch 804: accuracy did not improve from 0.99860\n",
      "42/42 [==============================] - 4s 96ms/step - loss: 0.0095 - accuracy: 0.9971\n",
      "Epoch 805/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0113 - accuracy: 0.9973\n",
      "Epoch 805: accuracy did not improve from 0.99860\n",
      "42/42 [==============================] - 4s 95ms/step - loss: 0.0113 - accuracy: 0.9973\n",
      "Epoch 806/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0108 - accuracy: 0.9976\n",
      "Epoch 806: accuracy did not improve from 0.99860\n",
      "42/42 [==============================] - 4s 96ms/step - loss: 0.0108 - accuracy: 0.9976\n",
      "Epoch 807/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0075 - accuracy: 0.9971\n",
      "Epoch 807: accuracy did not improve from 0.99860\n",
      "42/42 [==============================] - 4s 94ms/step - loss: 0.0075 - accuracy: 0.9971\n",
      "Epoch 808/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0085 - accuracy: 0.9970\n",
      "Epoch 808: accuracy did not improve from 0.99860\n",
      "42/42 [==============================] - 4s 94ms/step - loss: 0.0085 - accuracy: 0.9970\n",
      "Epoch 809/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0083 - accuracy: 0.9976\n",
      "Epoch 809: accuracy did not improve from 0.99860\n",
      "42/42 [==============================] - 4s 97ms/step - loss: 0.0083 - accuracy: 0.9976\n",
      "Epoch 810/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0045 - accuracy: 0.9987\n",
      "Epoch 810: accuracy improved from 0.99860 to 0.99869, saving model to eq_solver.h5\n",
      "42/42 [==============================] - 4s 95ms/step - loss: 0.0045 - accuracy: 0.9987\n",
      "Epoch 811/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0099 - accuracy: 0.9977\n",
      "Epoch 811: accuracy did not improve from 0.99869\n",
      "42/42 [==============================] - 4s 97ms/step - loss: 0.0099 - accuracy: 0.9977\n",
      "Epoch 812/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0100 - accuracy: 0.9972\n",
      "Epoch 812: accuracy did not improve from 0.99869\n",
      "42/42 [==============================] - 4s 95ms/step - loss: 0.0100 - accuracy: 0.9972\n",
      "Epoch 813/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0124 - accuracy: 0.9970\n",
      "Epoch 813: accuracy did not improve from 0.99869\n",
      "42/42 [==============================] - 4s 95ms/step - loss: 0.0124 - accuracy: 0.9970\n",
      "Epoch 814/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0090 - accuracy: 0.9977\n",
      "Epoch 814: accuracy did not improve from 0.99869\n",
      "42/42 [==============================] - 4s 94ms/step - loss: 0.0090 - accuracy: 0.9977\n",
      "Epoch 815/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0120 - accuracy: 0.9969\n",
      "Epoch 815: accuracy did not improve from 0.99869\n",
      "42/42 [==============================] - 4s 95ms/step - loss: 0.0120 - accuracy: 0.9969\n",
      "Epoch 816/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0073 - accuracy: 0.9972\n",
      "Epoch 816: accuracy did not improve from 0.99869\n",
      "42/42 [==============================] - 4s 97ms/step - loss: 0.0073 - accuracy: 0.9972\n",
      "Epoch 817/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0092 - accuracy: 0.9978\n",
      "Epoch 817: accuracy did not improve from 0.99869\n",
      "42/42 [==============================] - 4s 96ms/step - loss: 0.0092 - accuracy: 0.9978\n",
      "Epoch 818/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0047 - accuracy: 0.9984\n",
      "Epoch 818: accuracy did not improve from 0.99869\n",
      "42/42 [==============================] - 4s 96ms/step - loss: 0.0047 - accuracy: 0.9984\n",
      "Epoch 819/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0053 - accuracy: 0.9983\n",
      "Epoch 819: accuracy did not improve from 0.99869\n",
      "42/42 [==============================] - 4s 95ms/step - loss: 0.0053 - accuracy: 0.9983\n",
      "Epoch 820/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0089 - accuracy: 0.9973\n",
      "Epoch 820: accuracy did not improve from 0.99869\n",
      "42/42 [==============================] - 4s 93ms/step - loss: 0.0089 - accuracy: 0.9973\n",
      "Epoch 821/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0077 - accuracy: 0.9978\n",
      "Epoch 821: accuracy did not improve from 0.99869\n",
      "42/42 [==============================] - 4s 94ms/step - loss: 0.0077 - accuracy: 0.9978\n",
      "Epoch 822/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0070 - accuracy: 0.9977\n",
      "Epoch 822: accuracy did not improve from 0.99869\n",
      "42/42 [==============================] - 4s 94ms/step - loss: 0.0070 - accuracy: 0.9977\n",
      "Epoch 823/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0045 - accuracy: 0.9987\n",
      "Epoch 823: accuracy did not improve from 0.99869\n",
      "42/42 [==============================] - 4s 95ms/step - loss: 0.0045 - accuracy: 0.9987\n",
      "Epoch 824/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0094 - accuracy: 0.9966\n",
      "Epoch 824: accuracy did not improve from 0.99869\n",
      "42/42 [==============================] - 4s 94ms/step - loss: 0.0094 - accuracy: 0.9966\n",
      "Epoch 825/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0109 - accuracy: 0.9974\n",
      "Epoch 825: accuracy did not improve from 0.99869\n",
      "42/42 [==============================] - 4s 97ms/step - loss: 0.0109 - accuracy: 0.9974\n",
      "Epoch 826/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0132 - accuracy: 0.9978\n",
      "Epoch 826: accuracy did not improve from 0.99869\n",
      "42/42 [==============================] - 4s 97ms/step - loss: 0.0132 - accuracy: 0.9978\n",
      "Epoch 827/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0096 - accuracy: 0.9974\n",
      "Epoch 827: accuracy did not improve from 0.99869\n",
      "42/42 [==============================] - 4s 94ms/step - loss: 0.0096 - accuracy: 0.9974\n",
      "Epoch 828/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0066 - accuracy: 0.9981\n",
      "Epoch 828: accuracy did not improve from 0.99869\n",
      "42/42 [==============================] - 4s 95ms/step - loss: 0.0066 - accuracy: 0.9981\n",
      "Epoch 829/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0107 - accuracy: 0.9972\n",
      "Epoch 829: accuracy did not improve from 0.99869\n",
      "42/42 [==============================] - 4s 95ms/step - loss: 0.0107 - accuracy: 0.9972\n",
      "Epoch 830/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0084 - accuracy: 0.9975\n",
      "Epoch 830: accuracy did not improve from 0.99869\n",
      "42/42 [==============================] - 4s 97ms/step - loss: 0.0084 - accuracy: 0.9975\n",
      "Epoch 831/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0110 - accuracy: 0.9975\n",
      "Epoch 831: accuracy did not improve from 0.99869\n",
      "42/42 [==============================] - 4s 97ms/step - loss: 0.0110 - accuracy: 0.9975\n",
      "Epoch 832/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0129 - accuracy: 0.9963\n",
      "Epoch 832: accuracy did not improve from 0.99869\n",
      "42/42 [==============================] - 4s 95ms/step - loss: 0.0129 - accuracy: 0.9963\n",
      "Epoch 833/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0111 - accuracy: 0.9965\n",
      "Epoch 833: accuracy did not improve from 0.99869\n",
      "42/42 [==============================] - 4s 96ms/step - loss: 0.0111 - accuracy: 0.9965\n",
      "Epoch 834/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0121 - accuracy: 0.9966\n",
      "Epoch 834: accuracy did not improve from 0.99869\n",
      "42/42 [==============================] - 4s 97ms/step - loss: 0.0121 - accuracy: 0.9966\n",
      "Epoch 835/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0089 - accuracy: 0.9979\n",
      "Epoch 835: accuracy did not improve from 0.99869\n",
      "42/42 [==============================] - 4s 95ms/step - loss: 0.0089 - accuracy: 0.9979\n",
      "Epoch 836/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0065 - accuracy: 0.9980\n",
      "Epoch 836: accuracy did not improve from 0.99869\n",
      "42/42 [==============================] - 4s 94ms/step - loss: 0.0065 - accuracy: 0.9980\n",
      "Epoch 837/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0044 - accuracy: 0.9988\n",
      "Epoch 837: accuracy improved from 0.99869 to 0.99878, saving model to eq_solver.h5\n",
      "42/42 [==============================] - 4s 94ms/step - loss: 0.0044 - accuracy: 0.9988\n",
      "Epoch 838/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0036 - accuracy: 0.9988\n",
      "Epoch 838: accuracy did not improve from 0.99878\n",
      "42/42 [==============================] - 4s 95ms/step - loss: 0.0036 - accuracy: 0.9988\n",
      "Epoch 839/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0079 - accuracy: 0.9978\n",
      "Epoch 839: accuracy did not improve from 0.99878\n",
      "42/42 [==============================] - 4s 95ms/step - loss: 0.0079 - accuracy: 0.9978\n",
      "Epoch 840/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0078 - accuracy: 0.9975\n",
      "Epoch 840: accuracy did not improve from 0.99878\n",
      "42/42 [==============================] - 4s 95ms/step - loss: 0.0078 - accuracy: 0.9975\n",
      "Epoch 841/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0099 - accuracy: 0.9973\n",
      "Epoch 841: accuracy did not improve from 0.99878\n",
      "42/42 [==============================] - 4s 95ms/step - loss: 0.0099 - accuracy: 0.9973\n",
      "Epoch 842/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0080 - accuracy: 0.9981\n",
      "Epoch 842: accuracy did not improve from 0.99878\n",
      "42/42 [==============================] - 4s 95ms/step - loss: 0.0080 - accuracy: 0.9981\n",
      "Epoch 843/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0075 - accuracy: 0.9977\n",
      "Epoch 843: accuracy did not improve from 0.99878\n",
      "42/42 [==============================] - 4s 95ms/step - loss: 0.0075 - accuracy: 0.9977\n",
      "Epoch 844/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0081 - accuracy: 0.9978\n",
      "Epoch 844: accuracy did not improve from 0.99878\n",
      "42/42 [==============================] - 4s 93ms/step - loss: 0.0081 - accuracy: 0.9978\n",
      "Epoch 845/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0090 - accuracy: 0.9979\n",
      "Epoch 845: accuracy did not improve from 0.99878\n",
      "42/42 [==============================] - 4s 98ms/step - loss: 0.0090 - accuracy: 0.9979\n",
      "Epoch 846/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0054 - accuracy: 0.9982\n",
      "Epoch 846: accuracy did not improve from 0.99878\n",
      "42/42 [==============================] - 4s 96ms/step - loss: 0.0054 - accuracy: 0.9982\n",
      "Epoch 847/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0079 - accuracy: 0.9976\n",
      "Epoch 847: accuracy did not improve from 0.99878\n",
      "42/42 [==============================] - 4s 97ms/step - loss: 0.0079 - accuracy: 0.9976\n",
      "Epoch 848/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0078 - accuracy: 0.9978\n",
      "Epoch 848: accuracy did not improve from 0.99878\n",
      "42/42 [==============================] - 4s 95ms/step - loss: 0.0078 - accuracy: 0.9978\n",
      "Epoch 849/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0070 - accuracy: 0.9972\n",
      "Epoch 849: accuracy did not improve from 0.99878\n",
      "42/42 [==============================] - 4s 95ms/step - loss: 0.0070 - accuracy: 0.9972\n",
      "Epoch 850/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0063 - accuracy: 0.9983\n",
      "Epoch 850: accuracy did not improve from 0.99878\n",
      "42/42 [==============================] - 4s 94ms/step - loss: 0.0063 - accuracy: 0.9983\n",
      "Epoch 851/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0078 - accuracy: 0.9977\n",
      "Epoch 851: accuracy did not improve from 0.99878\n",
      "42/42 [==============================] - 4s 97ms/step - loss: 0.0078 - accuracy: 0.9977\n",
      "Epoch 852/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0054 - accuracy: 0.9982\n",
      "Epoch 852: accuracy did not improve from 0.99878\n",
      "42/42 [==============================] - 4s 94ms/step - loss: 0.0054 - accuracy: 0.9982\n",
      "Epoch 853/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0085 - accuracy: 0.9974\n",
      "Epoch 853: accuracy did not improve from 0.99878\n",
      "42/42 [==============================] - 4s 96ms/step - loss: 0.0085 - accuracy: 0.9974\n",
      "Epoch 854/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0088 - accuracy: 0.9973\n",
      "Epoch 854: accuracy did not improve from 0.99878\n",
      "42/42 [==============================] - 4s 96ms/step - loss: 0.0088 - accuracy: 0.9973\n",
      "Epoch 855/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0141 - accuracy: 0.9965\n",
      "Epoch 855: accuracy did not improve from 0.99878\n",
      "42/42 [==============================] - 4s 97ms/step - loss: 0.0141 - accuracy: 0.9965\n",
      "Epoch 856/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0096 - accuracy: 0.9977\n",
      "Epoch 856: accuracy did not improve from 0.99878\n",
      "42/42 [==============================] - 4s 95ms/step - loss: 0.0096 - accuracy: 0.9977\n",
      "Epoch 857/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0094 - accuracy: 0.9970\n",
      "Epoch 857: accuracy did not improve from 0.99878\n",
      "42/42 [==============================] - 4s 103ms/step - loss: 0.0094 - accuracy: 0.9970\n",
      "Epoch 858/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0108 - accuracy: 0.9972\n",
      "Epoch 858: accuracy did not improve from 0.99878\n",
      "42/42 [==============================] - 4s 100ms/step - loss: 0.0108 - accuracy: 0.9972\n",
      "Epoch 859/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0112 - accuracy: 0.9972\n",
      "Epoch 859: accuracy did not improve from 0.99878\n",
      "42/42 [==============================] - 4s 96ms/step - loss: 0.0112 - accuracy: 0.9972\n",
      "Epoch 860/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0095 - accuracy: 0.9976\n",
      "Epoch 860: accuracy did not improve from 0.99878\n",
      "42/42 [==============================] - 4s 94ms/step - loss: 0.0095 - accuracy: 0.9976\n",
      "Epoch 861/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0100 - accuracy: 0.9964\n",
      "Epoch 861: accuracy did not improve from 0.99878\n",
      "42/42 [==============================] - 4s 97ms/step - loss: 0.0100 - accuracy: 0.9964\n",
      "Epoch 862/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0078 - accuracy: 0.9977\n",
      "Epoch 862: accuracy did not improve from 0.99878\n",
      "42/42 [==============================] - 4s 96ms/step - loss: 0.0078 - accuracy: 0.9977\n",
      "Epoch 863/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0145 - accuracy: 0.9971\n",
      "Epoch 863: accuracy did not improve from 0.99878\n",
      "42/42 [==============================] - 4s 95ms/step - loss: 0.0145 - accuracy: 0.9971\n",
      "Epoch 864/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0127 - accuracy: 0.9974\n",
      "Epoch 864: accuracy did not improve from 0.99878\n",
      "42/42 [==============================] - 4s 95ms/step - loss: 0.0127 - accuracy: 0.9974\n",
      "Epoch 865/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0093 - accuracy: 0.9971\n",
      "Epoch 865: accuracy did not improve from 0.99878\n",
      "42/42 [==============================] - 4s 97ms/step - loss: 0.0093 - accuracy: 0.9971\n",
      "Epoch 866/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0163 - accuracy: 0.9972\n",
      "Epoch 866: accuracy did not improve from 0.99878\n",
      "42/42 [==============================] - 4s 96ms/step - loss: 0.0163 - accuracy: 0.9972\n",
      "Epoch 867/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0142 - accuracy: 0.9964\n",
      "Epoch 867: accuracy did not improve from 0.99878\n",
      "42/42 [==============================] - 4s 95ms/step - loss: 0.0142 - accuracy: 0.9964\n",
      "Epoch 868/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0085 - accuracy: 0.9978\n",
      "Epoch 868: accuracy did not improve from 0.99878\n",
      "42/42 [==============================] - 4s 97ms/step - loss: 0.0085 - accuracy: 0.9978\n",
      "Epoch 869/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0125 - accuracy: 0.9973\n",
      "Epoch 869: accuracy did not improve from 0.99878\n",
      "42/42 [==============================] - 4s 95ms/step - loss: 0.0125 - accuracy: 0.9973\n",
      "Epoch 870/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0081 - accuracy: 0.9975\n",
      "Epoch 870: accuracy did not improve from 0.99878\n",
      "42/42 [==============================] - 4s 97ms/step - loss: 0.0081 - accuracy: 0.9975\n",
      "Epoch 871/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0080 - accuracy: 0.9976\n",
      "Epoch 871: accuracy did not improve from 0.99878\n",
      "42/42 [==============================] - 4s 94ms/step - loss: 0.0080 - accuracy: 0.9976\n",
      "Epoch 872/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0129 - accuracy: 0.9971\n",
      "Epoch 872: accuracy did not improve from 0.99878\n",
      "42/42 [==============================] - 4s 93ms/step - loss: 0.0129 - accuracy: 0.9971\n",
      "Epoch 873/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0059 - accuracy: 0.9978\n",
      "Epoch 873: accuracy did not improve from 0.99878\n",
      "42/42 [==============================] - 4s 99ms/step - loss: 0.0059 - accuracy: 0.9978\n",
      "Epoch 874/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0078 - accuracy: 0.9983\n",
      "Epoch 874: accuracy did not improve from 0.99878\n",
      "42/42 [==============================] - 4s 95ms/step - loss: 0.0078 - accuracy: 0.9983\n",
      "Epoch 875/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0060 - accuracy: 0.9981\n",
      "Epoch 875: accuracy did not improve from 0.99878\n",
      "42/42 [==============================] - 4s 96ms/step - loss: 0.0060 - accuracy: 0.9981\n",
      "Epoch 876/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0079 - accuracy: 0.9976\n",
      "Epoch 876: accuracy did not improve from 0.99878\n",
      "42/42 [==============================] - 4s 100ms/step - loss: 0.0079 - accuracy: 0.9976\n",
      "Epoch 877/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0113 - accuracy: 0.9967\n",
      "Epoch 877: accuracy did not improve from 0.99878\n",
      "42/42 [==============================] - 4s 98ms/step - loss: 0.0113 - accuracy: 0.9967\n",
      "Epoch 878/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0101 - accuracy: 0.9976\n",
      "Epoch 878: accuracy did not improve from 0.99878\n",
      "42/42 [==============================] - 4s 95ms/step - loss: 0.0101 - accuracy: 0.9976\n",
      "Epoch 879/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0052 - accuracy: 0.9982\n",
      "Epoch 879: accuracy did not improve from 0.99878\n",
      "42/42 [==============================] - 4s 93ms/step - loss: 0.0052 - accuracy: 0.9982\n",
      "Epoch 880/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0114 - accuracy: 0.9964\n",
      "Epoch 880: accuracy did not improve from 0.99878\n",
      "42/42 [==============================] - 4s 94ms/step - loss: 0.0114 - accuracy: 0.9964\n",
      "Epoch 881/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0096 - accuracy: 0.9970\n",
      "Epoch 881: accuracy did not improve from 0.99878\n",
      "42/42 [==============================] - 4s 95ms/step - loss: 0.0096 - accuracy: 0.9970\n",
      "Epoch 882/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0062 - accuracy: 0.9985\n",
      "Epoch 882: accuracy did not improve from 0.99878\n",
      "42/42 [==============================] - 4s 97ms/step - loss: 0.0062 - accuracy: 0.9985\n",
      "Epoch 883/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0065 - accuracy: 0.9979\n",
      "Epoch 883: accuracy did not improve from 0.99878\n",
      "42/42 [==============================] - 4s 94ms/step - loss: 0.0065 - accuracy: 0.9979\n",
      "Epoch 884/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0059 - accuracy: 0.9984\n",
      "Epoch 884: accuracy did not improve from 0.99878\n",
      "42/42 [==============================] - 4s 95ms/step - loss: 0.0059 - accuracy: 0.9984\n",
      "Epoch 885/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0124 - accuracy: 0.9969\n",
      "Epoch 885: accuracy did not improve from 0.99878\n",
      "42/42 [==============================] - 4s 97ms/step - loss: 0.0124 - accuracy: 0.9969\n",
      "Epoch 886/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0118 - accuracy: 0.9973\n",
      "Epoch 886: accuracy did not improve from 0.99878\n",
      "42/42 [==============================] - 4s 93ms/step - loss: 0.0118 - accuracy: 0.9973\n",
      "Epoch 887/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0066 - accuracy: 0.9978\n",
      "Epoch 887: accuracy did not improve from 0.99878\n",
      "42/42 [==============================] - 4s 96ms/step - loss: 0.0066 - accuracy: 0.9978\n",
      "Epoch 888/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0113 - accuracy: 0.9978\n",
      "Epoch 888: accuracy did not improve from 0.99878\n",
      "42/42 [==============================] - 4s 95ms/step - loss: 0.0113 - accuracy: 0.9978\n",
      "Epoch 889/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0103 - accuracy: 0.9971\n",
      "Epoch 889: accuracy did not improve from 0.99878\n",
      "42/42 [==============================] - 4s 97ms/step - loss: 0.0103 - accuracy: 0.9971\n",
      "Epoch 890/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0108 - accuracy: 0.9976\n",
      "Epoch 890: accuracy did not improve from 0.99878\n",
      "42/42 [==============================] - 4s 95ms/step - loss: 0.0108 - accuracy: 0.9976\n",
      "Epoch 891/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0095 - accuracy: 0.9970\n",
      "Epoch 891: accuracy did not improve from 0.99878\n",
      "42/42 [==============================] - 4s 98ms/step - loss: 0.0095 - accuracy: 0.9970\n",
      "Epoch 892/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0073 - accuracy: 0.9978\n",
      "Epoch 892: accuracy did not improve from 0.99878\n",
      "42/42 [==============================] - 4s 97ms/step - loss: 0.0073 - accuracy: 0.9978\n",
      "Epoch 893/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0072 - accuracy: 0.9978\n",
      "Epoch 893: accuracy did not improve from 0.99878\n",
      "42/42 [==============================] - 4s 97ms/step - loss: 0.0072 - accuracy: 0.9978\n",
      "Epoch 894/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0089 - accuracy: 0.9978\n",
      "Epoch 894: accuracy did not improve from 0.99878\n",
      "42/42 [==============================] - 4s 95ms/step - loss: 0.0089 - accuracy: 0.9978\n",
      "Epoch 895/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0049 - accuracy: 0.9985\n",
      "Epoch 895: accuracy did not improve from 0.99878\n",
      "42/42 [==============================] - 4s 96ms/step - loss: 0.0049 - accuracy: 0.9985\n",
      "Epoch 896/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0107 - accuracy: 0.9972\n",
      "Epoch 896: accuracy did not improve from 0.99878\n",
      "42/42 [==============================] - 4s 97ms/step - loss: 0.0107 - accuracy: 0.9972\n",
      "Epoch 897/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0128 - accuracy: 0.9972\n",
      "Epoch 897: accuracy did not improve from 0.99878\n",
      "42/42 [==============================] - 4s 95ms/step - loss: 0.0128 - accuracy: 0.9972\n",
      "Epoch 898/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0077 - accuracy: 0.9975\n",
      "Epoch 898: accuracy did not improve from 0.99878\n",
      "42/42 [==============================] - 4s 95ms/step - loss: 0.0077 - accuracy: 0.9975\n",
      "Epoch 899/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0064 - accuracy: 0.9984\n",
      "Epoch 899: accuracy did not improve from 0.99878\n",
      "42/42 [==============================] - 4s 96ms/step - loss: 0.0064 - accuracy: 0.9984\n",
      "Epoch 900/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0066 - accuracy: 0.9978\n",
      "Epoch 900: accuracy did not improve from 0.99878\n",
      "42/42 [==============================] - 4s 95ms/step - loss: 0.0066 - accuracy: 0.9978\n",
      "Epoch 901/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0085 - accuracy: 0.9978\n",
      "Epoch 901: accuracy did not improve from 0.99878\n",
      "42/42 [==============================] - 4s 100ms/step - loss: 0.0085 - accuracy: 0.9978\n",
      "Epoch 902/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0095 - accuracy: 0.9976\n",
      "Epoch 902: accuracy did not improve from 0.99878\n",
      "42/42 [==============================] - 4s 97ms/step - loss: 0.0095 - accuracy: 0.9976\n",
      "Epoch 903/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0066 - accuracy: 0.9984\n",
      "Epoch 903: accuracy did not improve from 0.99878\n",
      "42/42 [==============================] - 4s 99ms/step - loss: 0.0066 - accuracy: 0.9984\n",
      "Epoch 904/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0053 - accuracy: 0.9982\n",
      "Epoch 904: accuracy did not improve from 0.99878\n",
      "42/42 [==============================] - 4s 97ms/step - loss: 0.0053 - accuracy: 0.9982\n",
      "Epoch 905/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0069 - accuracy: 0.9981\n",
      "Epoch 905: accuracy did not improve from 0.99878\n",
      "42/42 [==============================] - 4s 105ms/step - loss: 0.0069 - accuracy: 0.9981\n",
      "Epoch 906/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0048 - accuracy: 0.9985\n",
      "Epoch 906: accuracy did not improve from 0.99878\n",
      "42/42 [==============================] - 4s 99ms/step - loss: 0.0048 - accuracy: 0.9985\n",
      "Epoch 907/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0075 - accuracy: 0.9981\n",
      "Epoch 907: accuracy did not improve from 0.99878\n",
      "42/42 [==============================] - 4s 98ms/step - loss: 0.0075 - accuracy: 0.9981\n",
      "Epoch 908/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0078 - accuracy: 0.9978\n",
      "Epoch 908: accuracy did not improve from 0.99878\n",
      "42/42 [==============================] - 4s 96ms/step - loss: 0.0078 - accuracy: 0.9978\n",
      "Epoch 909/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0079 - accuracy: 0.9978\n",
      "Epoch 909: accuracy did not improve from 0.99878\n",
      "42/42 [==============================] - 4s 95ms/step - loss: 0.0079 - accuracy: 0.9978\n",
      "Epoch 910/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0119 - accuracy: 0.9974\n",
      "Epoch 910: accuracy did not improve from 0.99878\n",
      "42/42 [==============================] - 4s 96ms/step - loss: 0.0119 - accuracy: 0.9974\n",
      "Epoch 911/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0051 - accuracy: 0.9987\n",
      "Epoch 911: accuracy did not improve from 0.99878\n",
      "42/42 [==============================] - 4s 95ms/step - loss: 0.0051 - accuracy: 0.9987\n",
      "Epoch 912/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0067 - accuracy: 0.9981\n",
      "Epoch 912: accuracy did not improve from 0.99878\n",
      "42/42 [==============================] - 4s 96ms/step - loss: 0.0067 - accuracy: 0.9981\n",
      "Epoch 913/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0074 - accuracy: 0.9978\n",
      "Epoch 913: accuracy did not improve from 0.99878\n",
      "42/42 [==============================] - 4s 96ms/step - loss: 0.0074 - accuracy: 0.9978\n",
      "Epoch 914/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0111 - accuracy: 0.9977\n",
      "Epoch 914: accuracy did not improve from 0.99878\n",
      "42/42 [==============================] - 4s 96ms/step - loss: 0.0111 - accuracy: 0.9977\n",
      "Epoch 915/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0116 - accuracy: 0.9969\n",
      "Epoch 915: accuracy did not improve from 0.99878\n",
      "42/42 [==============================] - 4s 99ms/step - loss: 0.0116 - accuracy: 0.9969\n",
      "Epoch 916/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0074 - accuracy: 0.9978\n",
      "Epoch 916: accuracy did not improve from 0.99878\n",
      "42/42 [==============================] - 4s 95ms/step - loss: 0.0074 - accuracy: 0.9978\n",
      "Epoch 917/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0080 - accuracy: 0.9978\n",
      "Epoch 917: accuracy did not improve from 0.99878\n",
      "42/42 [==============================] - 4s 96ms/step - loss: 0.0080 - accuracy: 0.9978\n",
      "Epoch 918/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0073 - accuracy: 0.9982\n",
      "Epoch 918: accuracy did not improve from 0.99878\n",
      "42/42 [==============================] - 4s 96ms/step - loss: 0.0073 - accuracy: 0.9982\n",
      "Epoch 919/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0146 - accuracy: 0.9964\n",
      "Epoch 919: accuracy did not improve from 0.99878\n",
      "42/42 [==============================] - 4s 98ms/step - loss: 0.0146 - accuracy: 0.9964\n",
      "Epoch 920/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0091 - accuracy: 0.9976\n",
      "Epoch 920: accuracy did not improve from 0.99878\n",
      "42/42 [==============================] - 4s 96ms/step - loss: 0.0091 - accuracy: 0.9976\n",
      "Epoch 921/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0117 - accuracy: 0.9972\n",
      "Epoch 921: accuracy did not improve from 0.99878\n",
      "42/42 [==============================] - 4s 97ms/step - loss: 0.0117 - accuracy: 0.9972\n",
      "Epoch 922/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0086 - accuracy: 0.9976\n",
      "Epoch 922: accuracy did not improve from 0.99878\n",
      "42/42 [==============================] - 4s 97ms/step - loss: 0.0086 - accuracy: 0.9976\n",
      "Epoch 923/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0074 - accuracy: 0.9980\n",
      "Epoch 923: accuracy did not improve from 0.99878\n",
      "42/42 [==============================] - 4s 98ms/step - loss: 0.0074 - accuracy: 0.9980\n",
      "Epoch 924/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0067 - accuracy: 0.9979\n",
      "Epoch 924: accuracy did not improve from 0.99878\n",
      "42/42 [==============================] - 4s 96ms/step - loss: 0.0067 - accuracy: 0.9979\n",
      "Epoch 925/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0074 - accuracy: 0.9977\n",
      "Epoch 925: accuracy did not improve from 0.99878\n",
      "42/42 [==============================] - 4s 95ms/step - loss: 0.0074 - accuracy: 0.9977\n",
      "Epoch 926/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0063 - accuracy: 0.9979\n",
      "Epoch 926: accuracy did not improve from 0.99878\n",
      "42/42 [==============================] - 4s 97ms/step - loss: 0.0063 - accuracy: 0.9979\n",
      "Epoch 927/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0067 - accuracy: 0.9975\n",
      "Epoch 927: accuracy did not improve from 0.99878\n",
      "42/42 [==============================] - 4s 95ms/step - loss: 0.0067 - accuracy: 0.9975\n",
      "Epoch 928/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0051 - accuracy: 0.9979\n",
      "Epoch 928: accuracy did not improve from 0.99878\n",
      "42/42 [==============================] - 4s 95ms/step - loss: 0.0051 - accuracy: 0.9979\n",
      "Epoch 929/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0081 - accuracy: 0.9979\n",
      "Epoch 929: accuracy did not improve from 0.99878\n",
      "42/42 [==============================] - 4s 98ms/step - loss: 0.0081 - accuracy: 0.9979\n",
      "Epoch 930/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0054 - accuracy: 0.9982\n",
      "Epoch 930: accuracy did not improve from 0.99878\n",
      "42/42 [==============================] - 4s 97ms/step - loss: 0.0054 - accuracy: 0.9982\n",
      "Epoch 931/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0069 - accuracy: 0.9977\n",
      "Epoch 931: accuracy did not improve from 0.99878\n",
      "42/42 [==============================] - 4s 97ms/step - loss: 0.0069 - accuracy: 0.9977\n",
      "Epoch 932/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0110 - accuracy: 0.9971\n",
      "Epoch 932: accuracy did not improve from 0.99878\n",
      "42/42 [==============================] - 4s 96ms/step - loss: 0.0110 - accuracy: 0.9971\n",
      "Epoch 933/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0079 - accuracy: 0.9977\n",
      "Epoch 933: accuracy did not improve from 0.99878\n",
      "42/42 [==============================] - 4s 95ms/step - loss: 0.0079 - accuracy: 0.9977\n",
      "Epoch 934/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0081 - accuracy: 0.9971\n",
      "Epoch 934: accuracy did not improve from 0.99878\n",
      "42/42 [==============================] - 4s 95ms/step - loss: 0.0081 - accuracy: 0.9971\n",
      "Epoch 935/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0071 - accuracy: 0.9978\n",
      "Epoch 935: accuracy did not improve from 0.99878\n",
      "42/42 [==============================] - 4s 94ms/step - loss: 0.0071 - accuracy: 0.9978\n",
      "Epoch 936/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0094 - accuracy: 0.9978\n",
      "Epoch 936: accuracy did not improve from 0.99878\n",
      "42/42 [==============================] - 4s 97ms/step - loss: 0.0094 - accuracy: 0.9978\n",
      "Epoch 937/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0102 - accuracy: 0.9974\n",
      "Epoch 937: accuracy did not improve from 0.99878\n",
      "42/42 [==============================] - 4s 95ms/step - loss: 0.0102 - accuracy: 0.9974\n",
      "Epoch 938/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0076 - accuracy: 0.9977\n",
      "Epoch 938: accuracy did not improve from 0.99878\n",
      "42/42 [==============================] - 4s 96ms/step - loss: 0.0076 - accuracy: 0.9977\n",
      "Epoch 939/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0104 - accuracy: 0.9972\n",
      "Epoch 939: accuracy did not improve from 0.99878\n",
      "42/42 [==============================] - 4s 96ms/step - loss: 0.0104 - accuracy: 0.9972\n",
      "Epoch 940/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0113 - accuracy: 0.9979\n",
      "Epoch 940: accuracy did not improve from 0.99878\n",
      "42/42 [==============================] - 4s 96ms/step - loss: 0.0113 - accuracy: 0.9979\n",
      "Epoch 941/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0086 - accuracy: 0.9978\n",
      "Epoch 941: accuracy did not improve from 0.99878\n",
      "42/42 [==============================] - 4s 98ms/step - loss: 0.0086 - accuracy: 0.9978\n",
      "Epoch 942/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0103 - accuracy: 0.9979\n",
      "Epoch 942: accuracy did not improve from 0.99878\n",
      "42/42 [==============================] - 4s 96ms/step - loss: 0.0103 - accuracy: 0.9979\n",
      "Epoch 943/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0088 - accuracy: 0.9964\n",
      "Epoch 943: accuracy did not improve from 0.99878\n",
      "42/42 [==============================] - 4s 97ms/step - loss: 0.0088 - accuracy: 0.9964\n",
      "Epoch 944/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0098 - accuracy: 0.9976\n",
      "Epoch 944: accuracy did not improve from 0.99878\n",
      "42/42 [==============================] - 4s 97ms/step - loss: 0.0098 - accuracy: 0.9976\n",
      "Epoch 945/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0056 - accuracy: 0.9979\n",
      "Epoch 945: accuracy did not improve from 0.99878\n",
      "42/42 [==============================] - 4s 96ms/step - loss: 0.0056 - accuracy: 0.9979\n",
      "Epoch 946/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0063 - accuracy: 0.9980\n",
      "Epoch 946: accuracy did not improve from 0.99878\n",
      "42/42 [==============================] - 4s 94ms/step - loss: 0.0063 - accuracy: 0.9980\n",
      "Epoch 947/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0145 - accuracy: 0.9978\n",
      "Epoch 947: accuracy did not improve from 0.99878\n",
      "42/42 [==============================] - 4s 97ms/step - loss: 0.0145 - accuracy: 0.9978\n",
      "Epoch 948/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0095 - accuracy: 0.9973\n",
      "Epoch 948: accuracy did not improve from 0.99878\n",
      "42/42 [==============================] - 4s 96ms/step - loss: 0.0095 - accuracy: 0.9973\n",
      "Epoch 949/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0048 - accuracy: 0.9986\n",
      "Epoch 949: accuracy did not improve from 0.99878\n",
      "42/42 [==============================] - 4s 96ms/step - loss: 0.0048 - accuracy: 0.9986\n",
      "Epoch 950/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0127 - accuracy: 0.9971\n",
      "Epoch 950: accuracy did not improve from 0.99878\n",
      "42/42 [==============================] - 4s 101ms/step - loss: 0.0127 - accuracy: 0.9971\n",
      "Epoch 951/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0116 - accuracy: 0.9974\n",
      "Epoch 951: accuracy did not improve from 0.99878\n",
      "42/42 [==============================] - 4s 103ms/step - loss: 0.0116 - accuracy: 0.9974\n",
      "Epoch 952/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0089 - accuracy: 0.9979\n",
      "Epoch 952: accuracy did not improve from 0.99878\n",
      "42/42 [==============================] - 4s 105ms/step - loss: 0.0089 - accuracy: 0.9979\n",
      "Epoch 953/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0174 - accuracy: 0.9961\n",
      "Epoch 953: accuracy did not improve from 0.99878\n",
      "42/42 [==============================] - 4s 103ms/step - loss: 0.0174 - accuracy: 0.9961\n",
      "Epoch 954/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0063 - accuracy: 0.9978\n",
      "Epoch 954: accuracy did not improve from 0.99878\n",
      "42/42 [==============================] - 4s 103ms/step - loss: 0.0063 - accuracy: 0.9978\n",
      "Epoch 955/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0086 - accuracy: 0.9978\n",
      "Epoch 955: accuracy did not improve from 0.99878\n",
      "42/42 [==============================] - 4s 100ms/step - loss: 0.0086 - accuracy: 0.9978\n",
      "Epoch 956/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0042 - accuracy: 0.9987\n",
      "Epoch 956: accuracy did not improve from 0.99878\n",
      "42/42 [==============================] - 4s 102ms/step - loss: 0.0042 - accuracy: 0.9987\n",
      "Epoch 957/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0056 - accuracy: 0.9984\n",
      "Epoch 957: accuracy did not improve from 0.99878\n",
      "42/42 [==============================] - 4s 101ms/step - loss: 0.0056 - accuracy: 0.9984\n",
      "Epoch 958/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0062 - accuracy: 0.9983\n",
      "Epoch 958: accuracy did not improve from 0.99878\n",
      "42/42 [==============================] - 4s 102ms/step - loss: 0.0062 - accuracy: 0.9983\n",
      "Epoch 959/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0059 - accuracy: 0.9984\n",
      "Epoch 959: accuracy did not improve from 0.99878\n",
      "42/42 [==============================] - 4s 104ms/step - loss: 0.0059 - accuracy: 0.9984\n",
      "Epoch 960/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0120 - accuracy: 0.9965\n",
      "Epoch 960: accuracy did not improve from 0.99878\n",
      "42/42 [==============================] - 4s 103ms/step - loss: 0.0120 - accuracy: 0.9965\n",
      "Epoch 961/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0129 - accuracy: 0.9978\n",
      "Epoch 961: accuracy did not improve from 0.99878\n",
      "42/42 [==============================] - 4s 100ms/step - loss: 0.0129 - accuracy: 0.9978\n",
      "Epoch 962/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0078 - accuracy: 0.9978\n",
      "Epoch 962: accuracy did not improve from 0.99878\n",
      "42/42 [==============================] - 4s 102ms/step - loss: 0.0078 - accuracy: 0.9978\n",
      "Epoch 963/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0130 - accuracy: 0.9973\n",
      "Epoch 963: accuracy did not improve from 0.99878\n",
      "42/42 [==============================] - 4s 101ms/step - loss: 0.0130 - accuracy: 0.9973\n",
      "Epoch 964/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0092 - accuracy: 0.9974\n",
      "Epoch 964: accuracy did not improve from 0.99878\n",
      "42/42 [==============================] - 4s 100ms/step - loss: 0.0092 - accuracy: 0.9974\n",
      "Epoch 965/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0063 - accuracy: 0.9978\n",
      "Epoch 965: accuracy did not improve from 0.99878\n",
      "42/42 [==============================] - 4s 102ms/step - loss: 0.0063 - accuracy: 0.9978\n",
      "Epoch 966/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0103 - accuracy: 0.9974\n",
      "Epoch 966: accuracy did not improve from 0.99878\n",
      "42/42 [==============================] - 4s 103ms/step - loss: 0.0103 - accuracy: 0.9974\n",
      "Epoch 967/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0089 - accuracy: 0.9982\n",
      "Epoch 967: accuracy did not improve from 0.99878\n",
      "42/42 [==============================] - 4s 101ms/step - loss: 0.0089 - accuracy: 0.9982\n",
      "Epoch 968/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0094 - accuracy: 0.9975\n",
      "Epoch 968: accuracy did not improve from 0.99878\n",
      "42/42 [==============================] - 4s 102ms/step - loss: 0.0094 - accuracy: 0.9975\n",
      "Epoch 969/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0090 - accuracy: 0.9971\n",
      "Epoch 969: accuracy did not improve from 0.99878\n",
      "42/42 [==============================] - 4s 101ms/step - loss: 0.0090 - accuracy: 0.9971\n",
      "Epoch 970/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0092 - accuracy: 0.9973\n",
      "Epoch 970: accuracy did not improve from 0.99878\n",
      "42/42 [==============================] - 4s 102ms/step - loss: 0.0092 - accuracy: 0.9973\n",
      "Epoch 971/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0083 - accuracy: 0.9973\n",
      "Epoch 971: accuracy did not improve from 0.99878\n",
      "42/42 [==============================] - 4s 101ms/step - loss: 0.0083 - accuracy: 0.9973\n",
      "Epoch 972/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0052 - accuracy: 0.9986\n",
      "Epoch 972: accuracy did not improve from 0.99878\n",
      "42/42 [==============================] - 4s 105ms/step - loss: 0.0052 - accuracy: 0.9986\n",
      "Epoch 973/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0075 - accuracy: 0.9980\n",
      "Epoch 973: accuracy did not improve from 0.99878\n",
      "42/42 [==============================] - 4s 106ms/step - loss: 0.0075 - accuracy: 0.9980\n",
      "Epoch 974/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0088 - accuracy: 0.9978\n",
      "Epoch 974: accuracy did not improve from 0.99878\n",
      "42/42 [==============================] - 4s 102ms/step - loss: 0.0088 - accuracy: 0.9978\n",
      "Epoch 975/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0102 - accuracy: 0.9975\n",
      "Epoch 975: accuracy did not improve from 0.99878\n",
      "42/42 [==============================] - 4s 99ms/step - loss: 0.0102 - accuracy: 0.9975\n",
      "Epoch 976/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0080 - accuracy: 0.9977\n",
      "Epoch 976: accuracy did not improve from 0.99878\n",
      "42/42 [==============================] - 4s 103ms/step - loss: 0.0080 - accuracy: 0.9977\n",
      "Epoch 977/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0052 - accuracy: 0.9981\n",
      "Epoch 977: accuracy did not improve from 0.99878\n",
      "42/42 [==============================] - 4s 103ms/step - loss: 0.0052 - accuracy: 0.9981\n",
      "Epoch 978/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0047 - accuracy: 0.9983\n",
      "Epoch 978: accuracy did not improve from 0.99878\n",
      "42/42 [==============================] - 4s 101ms/step - loss: 0.0047 - accuracy: 0.9983\n",
      "Epoch 979/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0074 - accuracy: 0.9981\n",
      "Epoch 979: accuracy did not improve from 0.99878\n",
      "42/42 [==============================] - 4s 103ms/step - loss: 0.0074 - accuracy: 0.9981\n",
      "Epoch 980/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0078 - accuracy: 0.9975\n",
      "Epoch 980: accuracy did not improve from 0.99878\n",
      "42/42 [==============================] - 4s 99ms/step - loss: 0.0078 - accuracy: 0.9975\n",
      "Epoch 981/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0088 - accuracy: 0.9978\n",
      "Epoch 981: accuracy did not improve from 0.99878\n",
      "42/42 [==============================] - 4s 95ms/step - loss: 0.0088 - accuracy: 0.9978\n",
      "Epoch 982/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0047 - accuracy: 0.9983\n",
      "Epoch 982: accuracy did not improve from 0.99878\n",
      "42/42 [==============================] - 4s 94ms/step - loss: 0.0047 - accuracy: 0.9983\n",
      "Epoch 983/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0069 - accuracy: 0.9985\n",
      "Epoch 983: accuracy did not improve from 0.99878\n",
      "42/42 [==============================] - 4s 96ms/step - loss: 0.0069 - accuracy: 0.9985\n",
      "Epoch 984/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0064 - accuracy: 0.9987\n",
      "Epoch 984: accuracy did not improve from 0.99878\n",
      "42/42 [==============================] - 4s 95ms/step - loss: 0.0064 - accuracy: 0.9987\n",
      "Epoch 985/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0055 - accuracy: 0.9982\n",
      "Epoch 985: accuracy did not improve from 0.99878\n",
      "42/42 [==============================] - 4s 95ms/step - loss: 0.0055 - accuracy: 0.9982\n",
      "Epoch 986/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0101 - accuracy: 0.9977\n",
      "Epoch 986: accuracy did not improve from 0.99878\n",
      "42/42 [==============================] - 4s 97ms/step - loss: 0.0101 - accuracy: 0.9977\n",
      "Epoch 987/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0089 - accuracy: 0.9967\n",
      "Epoch 987: accuracy did not improve from 0.99878\n",
      "42/42 [==============================] - 4s 95ms/step - loss: 0.0089 - accuracy: 0.9967\n",
      "Epoch 988/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0056 - accuracy: 0.9981\n",
      "Epoch 988: accuracy did not improve from 0.99878\n",
      "42/42 [==============================] - 4s 94ms/step - loss: 0.0056 - accuracy: 0.9981\n",
      "Epoch 989/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0082 - accuracy: 0.9979\n",
      "Epoch 989: accuracy did not improve from 0.99878\n",
      "42/42 [==============================] - 4s 99ms/step - loss: 0.0082 - accuracy: 0.9979\n",
      "Epoch 990/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0055 - accuracy: 0.9982\n",
      "Epoch 990: accuracy did not improve from 0.99878\n",
      "42/42 [==============================] - 4s 98ms/step - loss: 0.0055 - accuracy: 0.9982\n",
      "Epoch 991/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0065 - accuracy: 0.9982\n",
      "Epoch 991: accuracy did not improve from 0.99878\n",
      "42/42 [==============================] - 4s 94ms/step - loss: 0.0065 - accuracy: 0.9982\n",
      "Epoch 992/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0033 - accuracy: 0.9988\n",
      "Epoch 992: accuracy did not improve from 0.99878\n",
      "42/42 [==============================] - 4s 94ms/step - loss: 0.0033 - accuracy: 0.9988\n",
      "Epoch 993/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0057 - accuracy: 0.9987\n",
      "Epoch 993: accuracy did not improve from 0.99878\n",
      "42/42 [==============================] - 4s 94ms/step - loss: 0.0057 - accuracy: 0.9987\n",
      "Epoch 994/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0090 - accuracy: 0.9984\n",
      "Epoch 994: accuracy did not improve from 0.99878\n",
      "42/42 [==============================] - 4s 98ms/step - loss: 0.0090 - accuracy: 0.9984\n",
      "Epoch 995/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0088 - accuracy: 0.9977\n",
      "Epoch 995: accuracy did not improve from 0.99878\n",
      "42/42 [==============================] - 4s 96ms/step - loss: 0.0088 - accuracy: 0.9977\n",
      "Epoch 996/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0146 - accuracy: 0.9973\n",
      "Epoch 996: accuracy did not improve from 0.99878\n",
      "42/42 [==============================] - 4s 94ms/step - loss: 0.0146 - accuracy: 0.9973\n",
      "Epoch 997/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0111 - accuracy: 0.9971\n",
      "Epoch 997: accuracy did not improve from 0.99878\n",
      "42/42 [==============================] - 4s 95ms/step - loss: 0.0111 - accuracy: 0.9971\n",
      "Epoch 998/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0074 - accuracy: 0.9977\n",
      "Epoch 998: accuracy did not improve from 0.99878\n",
      "42/42 [==============================] - 4s 94ms/step - loss: 0.0074 - accuracy: 0.9977\n",
      "Epoch 999/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0096 - accuracy: 0.9972\n",
      "Epoch 999: accuracy did not improve from 0.99878\n",
      "42/42 [==============================] - 4s 94ms/step - loss: 0.0096 - accuracy: 0.9972\n",
      "Epoch 1000/1000\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0054 - accuracy: 0.9978\n",
      "Epoch 1000: accuracy did not improve from 0.99878\n",
      "42/42 [==============================] - 4s 94ms/step - loss: 0.0054 - accuracy: 0.9978\n"
     ]
    }
   ],
   "source": [
    "checkpoint = ModelCheckpoint(\"eq_solver.h5\", monitor='accuracy', verbose=1, save_best_only=True)\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=Adam(learning_rate=0.001), metrics=['accuracy'])\n",
    "nn = model.fit(X_train, cat, epochs=1000, batch_size=256, shuffle=True, verbose=1, callbacks=[checkpoint]).history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9d13ae24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x22050f3b580>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(nn['loss'])\n",
    "plt.plot(nn['accuracy'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss and accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train_loss', 'train_accuracy'], loc='upper right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6c063544",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(\"eq_solver.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ea494126",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x220582def80>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "img = cv2.imread('./22.png',cv2.IMREAD_GRAYSCALE)\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e9aadd48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x22048775a80>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "img = cv2.imread('./eq.PNG',cv2.IMREAD_GRAYSCALE)\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "795eb1b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n",
      "4\n",
      "[[20, 88, 20, 29], [22, 69, 29, 9], [27, 22, 30, 37], [85, 60, 20, 17], [121, 23, 21, 35], [139, 85, 25, 25]]\n"
     ]
    }
   ],
   "source": [
    "if img is not None:\n",
    "    img=~img\n",
    "    _,thresh=cv2.threshold(img,127,255,cv2.THRESH_BINARY)\n",
    "    ctrs,_=cv2.findContours(thresh,cv2.RETR_EXTERNAL,cv2.CHAIN_APPROX_NONE)\n",
    "    cnt=sorted(ctrs, key=lambda ctr: cv2.boundingRect(ctr)[0])\n",
    "    w=int(28)\n",
    "    h=int(28)\n",
    "    train_data=[]\n",
    "    print(len(cnt))\n",
    "    rects=[]\n",
    "    for c in cnt :\n",
    "        x,y,w,h= cv2.boundingRect(c)\n",
    "        rect=[x,y,w,h]\n",
    "        rects.append(rect)\n",
    "    bool_rect=[]\n",
    "    for r in rects:\n",
    "        l=[]\n",
    "        for rec in rects:\n",
    "            flag=0\n",
    "            if rec!=r:\n",
    "                if r[0]<(rec[0]+rec[2]+10) and rec[0]<(r[0]+r[2]+10) and r[1]<(rec[1]+rec[3]+10) and rec[1]<(r[1]+r[3]+10):\n",
    "                    flag=1\n",
    "                l.append(flag)\n",
    "            if rec==r:\n",
    "                l.append(0)\n",
    "        bool_rect.append(l)\n",
    "    dump_rect=[]\n",
    "    for i in range(0,len(cnt)):\n",
    "        for j in range(0,len(cnt)):\n",
    "            if bool_rect[i][j]==1:\n",
    "                area1=rects[i][2]*rects[i][3]\n",
    "                area2=rects[j][2]*rects[j][3]\n",
    "                if(area1==min(area1,area2)):\n",
    "                    dump_rect.append(rects[i])\n",
    "    print(len(dump_rect)) \n",
    "    final_rect=[i for i in rects if i not in dump_rect]\n",
    "    print(final_rect)\n",
    "    for r in final_rect:\n",
    "        x=r[0]\n",
    "        y=r[1]\n",
    "        w=r[2]\n",
    "        h=r[3]\n",
    "        im_crop =thresh[y:y+h+10,x:x+w+10]\n",
    "        im_resize = cv2.resize(im_crop,(28,28))\n",
    "        im_resize=np.reshape(im_resize,(28,28,1))\n",
    "        train_data.append(im_resize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b276777f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "Your Equation : x=5.10\n"
     ]
    }
   ],
   "source": [
    "equation=''\n",
    "\n",
    "for i in range(len(train_data)):\n",
    "    \n",
    "    train_data[i]=np.array(train_data[i])\n",
    "    train_data[i]=train_data[i].reshape(1,28,28,1)\n",
    "    result=np.argmax(model.predict(train_data[i]), axis=-1)\n",
    "        \n",
    "    for j in range(10) :\n",
    "        if result[0] == j :\n",
    "            equation = equation + str(j)\n",
    "    \n",
    "    if result[0] == 10 :\n",
    "        equation = equation + \"+\"\n",
    "    if result[0] == 11 :\n",
    "        equation = equation + \"-\"\n",
    "    if result[0] == 12 :\n",
    "        equation = equation + \"*\"\n",
    "    if result[0] == 13 :\n",
    "        equation = equation + \"/\"\n",
    "    if result[0] == 14 :\n",
    "        equation = equation + \"=\"\n",
    "    if result[0] == 15 :\n",
    "        equation = equation + \".\"\n",
    "    if result[0] == 16 :\n",
    "        equation = equation + \"x\"\n",
    "    if result[0] == 17 :\n",
    "        equation = equation + \"y\"      \n",
    "    if result[0] == 18 :\n",
    "        equation = equation + \"z\"\n",
    "    \n",
    "print(\"Your Equation :\", equation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "26dddf09",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Arithmetic solver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ead5c220",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "from time import time\n",
    "from torchvision import datasets, transforms\n",
    "from torch import nn, optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b881b620",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from PIL import Image, ImageFilter, ImageEnhance\n",
    "from cv2 import boundingRect\n",
    "\n",
    "# Function for extracting symbols from an image featuring a white background and black colored symbols\n",
    "def extract_symbols(image_file, verbose = False):\n",
    "    # Load image and invert colors\n",
    "    img_gray = cv2.imread(image_file, cv2.IMREAD_GRAYSCALE)\n",
    "    img_gray = np.asarray(img_gray)\n",
    "    img_gray = (255 - img_gray)\n",
    "    orig_img = img_gray.copy() # clone for extracting symbols later\n",
    "\n",
    "    # Create dilated image to capture potential loose points (such as dots in division symbol)\n",
    "    kernel = np.ones((10,1))\n",
    "    img_gray = cv2.dilate(img_gray,kernel,iterations = 5)\n",
    "    \n",
    "    #find contours\n",
    "    contours, hierarchy = cv2.findContours(img_gray, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    b_boxes = []\n",
    "    for c in range(len(contours)):\n",
    "\n",
    "        # Only take contours of symbols without parents to avoid duplicate outlines\n",
    "        if (hierarchy[0][c][3] == -1):\n",
    "            brect = boundingRect(contours[c])\n",
    "            b_boxes.append(brect)\n",
    "            cv2.rectangle(img_gray, brect, (255,255,255), 2)\n",
    "\n",
    "    # Sort bounding boxes after appearance in x-direction\n",
    "    b_boxes = sorted(b_boxes)\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Original image:\")\n",
    "        plt.figure(figsize=(10,10))\n",
    "        plt.imshow((255 - orig_img), cmap='gray')\n",
    "        plt.show()\n",
    "        print(\"All bounding boxes of all symbols:\")\n",
    "        plt.figure(figsize=(10,10))\n",
    "        plt.imshow((255 - img_gray), cmap='gray')\n",
    "        plt.show()\n",
    "        print(\"Extracted symbols:\")\n",
    "    \n",
    "    # b_boxes are formatted as [x_pos, y_pos, width, height]\n",
    "    #print(b_boxes)\n",
    "\n",
    "    # Get all symbols as separate images\n",
    "    symbols = []\n",
    "    for box in b_boxes:\n",
    "        x,y,w,h = box\n",
    "        \n",
    "        # To ensure that all points in division sign is included:\n",
    "        # expand bounding box in y-direction if a symbol is wider\n",
    "        # than it is tall\n",
    "        \n",
    "        #if w > h:\n",
    "        #    h = w\n",
    "        #    y -= int(h/2)\n",
    "        \n",
    "        symbol_img = orig_img[y : y + h, x : x + w]\n",
    "        symbols.append(symbol_img)\n",
    "        \n",
    "    if verbose:\n",
    "        plt.figure(figsize=(15,5))\n",
    "        for x in range(len(b_boxes)):\n",
    "            plt.subplot(1,len(b_boxes), x+1)\n",
    "            plt.imshow((255 - symbols[x]), cmap='gray')\n",
    "        plt.show()\n",
    "        \n",
    "    return symbols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f7b145f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for reformatting extracted symbol images to match the MNIST data\n",
    "def format_image(img_arr, verbose = False, blur_strength = 1, norm = True):\n",
    "    #\n",
    "    # === PADDING ===\n",
    "    # to ensure that the symbols appear in the middle of the image\n",
    "    # with a slight margin similar to the MNIST digits\n",
    "    #\n",
    "    symbol_image = Image.fromarray(img_arr)\n",
    "    \n",
    "    if symbol_image.width > symbol_image.height:\n",
    "        x_padding = int(symbol_image.width / 8)\n",
    "        y_padding = symbol_image.width + x_padding - symbol_image.height\n",
    "\n",
    "    else:\n",
    "        y_padding = int(symbol_image.height / 8)\n",
    "        x_padding = symbol_image.height + y_padding - symbol_image.width\n",
    "    \n",
    "    x_offset = int(x_padding / 2)\n",
    "    y_offset = int(y_padding / 2)\n",
    "    \n",
    "    padded_img = Image.new(symbol_image.mode, (symbol_image.width + x_padding, symbol_image.height + y_padding), 0)\n",
    "    padded_img.paste(symbol_image, (x_offset, y_offset))\n",
    "    \n",
    "    #\n",
    "    # === PRE-PROCESSING ===\n",
    "    #\n",
    "    \n",
    "    # Gaussian blur to ensure that thin symbols still appear after downsizing\n",
    "    padded_img = padded_img.filter(ImageFilter.GaussianBlur(radius = blur_strength))\n",
    "    \n",
    "    # Contrast to increase visibility of symbols after blurring\n",
    "    padded_img = ImageEnhance.Contrast(padded_img).enhance(blur_strength)\n",
    "    \n",
    "    # Downsize symbols to 28x28 embedding\n",
    "    padded_img = np.asarray(padded_img.resize((28,28)))\n",
    "    \n",
    "    # Normalize embedding from values between 0 and 255 to values between 0 and 1\n",
    "    if norm:\n",
    "        padded_img = padded_img / 255\n",
    "    \n",
    "    # Return embedding as single 28x28 float tensor\n",
    "    return torch.as_tensor(padded_img).float()\n",
    "\n",
    "# Function for taking a 28x28 tensor embedding and predicting the symbol it contains\n",
    "def predict_image(img_arr):\n",
    "    prediction = predict_symbol(img_arr.reshape(1, 784))\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8308188d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pahadian\\AppData\\Local\\Temp\\ipykernel_6808\\3844256768.py:42: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  C:\\cb\\pytorch_1000000000000\\work\\torch\\csrc\\utils\\tensor_numpy.cpp:178.)\n",
      "  return torch.as_tensor(padded_img).float()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset/add/ symbols: torch.Size([1212, 28, 28])\n",
      "dataset/sub/ symbols: torch.Size([655, 28, 28])\n",
      "dataset/mul/ symbols: torch.Size([577, 28, 28])\n",
      "dataset/div/ symbols: torch.Size([618, 28, 28])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1800x500 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Define paths to math symbols\n",
    "path_to_math_add = \"dataset/add/\"\n",
    "path_to_math_sub = \"dataset/sub/\"\n",
    "path_to_math_mul = \"dataset/mul/\"\n",
    "path_to_math_div = \"dataset/div/\"\n",
    "\n",
    "def format_dataset_path(path):\n",
    "    for (root,dirs,files) in os.walk(path):\n",
    "        formatted_syms = torch.empty(len(files),28,28)\n",
    "        for sym in range(len(files)):\n",
    "            symbol = extract_symbols(path + files[sym])\n",
    "            formatted_symbol = format_image(symbol[0], blur_strength = 2, norm = False)\n",
    "            formatted_syms[sym] = formatted_symbol\n",
    "    print(path, \"symbols:\", formatted_syms.shape)\n",
    "    \n",
    "    return formatted_syms.byte()\n",
    "\n",
    "# Reformat each symbol set to match MNIST dataset\n",
    "\n",
    "formatted_adds = format_dataset_path(path_to_math_add)\n",
    "formatted_subs = format_dataset_path(path_to_math_sub)\n",
    "formatted_muls = format_dataset_path(path_to_math_mul)\n",
    "formatted_divs = format_dataset_path(path_to_math_div)\n",
    "\n",
    "new_form_syms = torch.cat((formatted_adds, formatted_subs, formatted_muls, formatted_divs), 0)\n",
    "new_form_labels = torch.cat((torch.ones(len(formatted_adds)) * 10,\n",
    "                           torch.ones(len(formatted_subs)) * 11,\n",
    "                           torch.ones(len(formatted_muls)) * 12,\n",
    "                           torch.ones(len(formatted_divs)) * 13), 0)\n",
    "\n",
    "plt.figure(figsize=(18,5))\n",
    "\n",
    "#print(formatted_adds[0])\n",
    "plt.subplot(2,2,1)\n",
    "plt.imshow(formatted_adds[0], cmap=\"gray\")\n",
    "\n",
    "#print(formatted_subs[0])\n",
    "plt.subplot(2,2,2)\n",
    "plt.imshow(formatted_subs[0], cmap=\"gray\")\n",
    "\n",
    "#print(formatted_muls[0])\n",
    "plt.subplot(2,2,3)\n",
    "plt.imshow(formatted_muls[0], cmap=\"gray\")\n",
    "\n",
    "#print(formatted_divs[0])\n",
    "plt.subplot(2,2,4)\n",
    "plt.imshow(formatted_divs[0], cmap=\"gray\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c0270b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Responsible for recognizing mathematical symbols, trained on MNIST dataset along with handwritten math symbols from Sagyam Thapa's kaggle dataset https://www.kaggle.com/sagyamthapa/handwritten-math-symbols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e4f15088",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([63062, 28, 28])\n",
      "torch.Size([63062])\n",
      "Sequential(\n",
      "  (0): Linear(in_features=784, out_features=128, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (3): ReLU()\n",
      "  (4): Linear(in_features=64, out_features=14, bias=True)\n",
      "  (5): LogSoftmax(dim=1)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Symbol recognition model definitions\n",
    "\n",
    "transform = transforms.Compose(([transforms.ToTensor()]))\n",
    "\n",
    "trainset = datasets.MNIST('./', download=True, train=True, transform=transform)\n",
    "valset = datasets.MNIST('./', download=True, train=False, transform=transform)\n",
    "\n",
    "#print(trainset.train_data[0])\n",
    "\n",
    "trainset.data = torch.cat((trainset.data, new_form_syms))\n",
    "trainset.targets = torch.cat((trainset.targets, new_form_labels.long()))\n",
    "\n",
    "print(trainset.data.shape)\n",
    "print(trainset.targets.shape)\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "valloader = torch.utils.data.DataLoader(valset, batch_size=64, shuffle=True)\n",
    "\n",
    "input_size = 784\n",
    "hidden_sizes = [128, 64]\n",
    "output_size = 14\n",
    "\n",
    "sym_recog_model = nn.Sequential(nn.Linear(input_size, hidden_sizes[0]),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(hidden_sizes[0], hidden_sizes[1]),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(hidden_sizes[1], output_size),\n",
    "                      nn.LogSoftmax(dim=1))\n",
    "print(sym_recog_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "016985f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Note: you may need to restart the kernel to use updated packages.Retrieving notices: ...working... done\n",
      "Collecting package metadata (current_repodata.json): ...working... done\n",
      "Solving environment: ...working... failed with initial frozen solve. Retrying with flexible solve.\n",
      "Collecting package metadata (repodata.json): ...working... done\n",
      "Solving environment: ...working... done\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: C:\\Users\\pahadian\\AppData\\Local\\anaconda3\n",
      "\n",
      "  added / updated specs:\n",
      "    - pytorch==1.12.1\n",
      "    - torchaudio==0.12.1\n",
      "    - torchvision==0.13.1\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    certifi-2023.7.22          |  py310haa95532_0         154 KB\n",
      "    cudatoolkit-11.3.1         |       h59b6b97_2       545.3 MB\n",
      "    openssl-1.1.1w             |       h2bbff1b_0         5.5 MB\n",
      "    pytorch-1.12.1             |py3.10_cuda11.3_cudnn8_0        1.20 GB  pytorch\n",
      "    pytorch-mutex-1.0          |             cuda           3 KB  pytorch\n",
      "    torchaudio-0.12.1          |      py310_cu113         3.7 MB  pytorch\n",
      "    torchvision-0.13.1         |      py310_cu113         7.4 MB  pytorch\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:        1.74 GB\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "  cudatoolkit        pkgs/main/win-64::cudatoolkit-11.3.1-h59b6b97_2 \n",
      "  pytorch-mutex      pytorch/noarch::pytorch-mutex-1.0-cuda \n",
      "  torchaudio         pytorch/win-64::torchaudio-0.12.1-py310_cu113 \n",
      "  torchvision        pytorch/win-64::torchvision-0.13.1-py310_cu113 \n",
      "\n",
      "The following packages will be UPDATED:\n",
      "\n",
      "  ca-certificates                     2023.01.10-haa95532_0 --> 2023.08.22-haa95532_0 \n",
      "  certifi                         2022.12.7-py310haa95532_0 --> 2023.7.22-py310haa95532_0 \n",
      "  openssl                                 1.1.1t-h2bbff1b_0 --> 1.1.1w-h2bbff1b_0 \n",
      "\n",
      "The following packages will be SUPERSEDED by a higher-priority channel:\n",
      "\n",
      "  pytorch            pkgs/main::pytorch-1.12.1-cpu_py310h5~ --> pytorch::pytorch-1.12.1-py3.10_cuda11.3_cudnn8_0 \n",
      "\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages\n",
      "\n",
      "torchvision-0.13.1   | 7.4 MB    |            |   0% \n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   |            |   0% \u001b[A\n",
      "\n",
      "\n",
      "pytorch-mutex-1.0    | 3 KB      |            |   0% \u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "openssl-1.1.1w       | 5.5 MB    |            |   0% \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "torchaudio-0.12.1    | 3.7 MB    |            |   0% \u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "certifi-2023.7.22    | 154 KB    |            |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  |            |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "openssl-1.1.1w       | 5.5 MB    |            |   0% \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch-mutex-1.0    | 3 KB      | ########## | 100% \u001b[A\u001b[A\n",
      "torchvision-0.13.1   | 7.4 MB    |            |   0% \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "torchaudio-0.12.1    | 3.7 MB    |            |   0% \u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch-mutex-1.0    | 3 KB      | ########## | 100% \u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "openssl-1.1.1w       | 5.5 MB    | 8          |   9% \u001b[A\u001b[A\u001b[A\n",
      "torchvision-0.13.1   | 7.4 MB    | 3          |   3% \n",
      "\n",
      "\n",
      "\n",
      "openssl-1.1.1w       | 5.5 MB    | ##3        |  23% \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "torchaudio-0.12.1    | 3.7 MB    | 4          |   4% \u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "certifi-2023.7.22    | 154 KB    | #          |  10% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   |            |   0% \u001b[A\n",
      "torchvision-0.13.1   | 7.4 MB    | 6          |   7% \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "torchaudio-0.12.1    | 3.7 MB    | 7          |   8% \u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "certifi-2023.7.22    | 154 KB    | ########3  |  83% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "openssl-1.1.1w       | 5.5 MB    | ###6       |  37% \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   |            |   0% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "certifi-2023.7.22    | 154 KB    | ########## | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "torchaudio-0.12.1    | 3.7 MB    | #4         |  14% \u001b[A\u001b[A\u001b[A\u001b[A\n",
      "torchvision-0.13.1   | 7.4 MB    | #1         |  12% \n",
      "\n",
      "\n",
      "\n",
      "openssl-1.1.1w       | 5.5 MB    | #####5     |  56% \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  |            |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "torchaudio-0.12.1    | 3.7 MB    | ##3        |  24% \u001b[A\u001b[A\u001b[A\u001b[A\n",
      "torchvision-0.13.1   | 7.4 MB    | #6         |  16% \n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   |            |   0% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  |            |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "torchaudio-0.12.1    | 3.7 MB    | ###3       |  33% \u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   |            |   0% \u001b[A\n",
      "torchvision-0.13.1   | 7.4 MB    | ##2        |  23% \n",
      "\n",
      "\n",
      "\n",
      "openssl-1.1.1w       | 5.5 MB    | #######    |  70% \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "openssl-1.1.1w       | 5.5 MB    | #########7 |  97% \u001b[A\u001b[A\u001b[A\n",
      "torchvision-0.13.1   | 7.4 MB    | ##6        |  26% \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  |            |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "torchaudio-0.12.1    | 3.7 MB    | ####2      |  42% \u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   |            |   0% \u001b[A\n",
      "torchvision-0.13.1   | 7.4 MB    | ##9        |  29% \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  |            |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "torchaudio-0.12.1    | 3.7 MB    | ####8      |  49% \u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   |            |   0% \u001b[A\n",
      "torchvision-0.13.1   | 7.4 MB    | ###3       |  33% \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  |            |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "openssl-1.1.1w       | 5.5 MB    | ########## | 100% \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   |            |   0% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "torchaudio-0.12.1    | 3.7 MB    | #####7     |  57% \u001b[A\u001b[A\u001b[A\u001b[A\n",
      "torchvision-0.13.1   | 7.4 MB    | ###7       |  37% \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  |            |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   |            |   0% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "torchaudio-0.12.1    | 3.7 MB    | ######3    |  64% \u001b[A\u001b[A\u001b[A\u001b[A\n",
      "torchvision-0.13.1   | 7.4 MB    | ####       |  41% \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  |            |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   |            |   0% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "torchaudio-0.12.1    | 3.7 MB    | #######2   |  72% \u001b[A\u001b[A\u001b[A\u001b[A\n",
      "torchvision-0.13.1   | 7.4 MB    | ####5      |  45% \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  |            |   1% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   |            |   0% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "torchaudio-0.12.1    | 3.7 MB    | #######8   |  79% \u001b[A\u001b[A\u001b[A\u001b[A\n",
      "torchvision-0.13.1   | 7.4 MB    | ####8      |  49% \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  |            |   1% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   |            |   0% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  |            |   1% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "torchaudio-0.12.1    | 3.7 MB    | ########5  |  85% \u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   |            |   0% \u001b[A\n",
      "torchvision-0.13.1   | 7.4 MB    | #####2     |  52% \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  |            |   1% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "torchaudio-0.12.1    | 3.7 MB    | #########1 |  92% \u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   |            |   0% \u001b[A\n",
      "torchvision-0.13.1   | 7.4 MB    | #####5     |  56% \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  |            |   1% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   |            |   0% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "torchaudio-0.12.1    | 3.7 MB    | #########7 |  97% \u001b[A\u001b[A\u001b[A\u001b[A\n",
      "torchvision-0.13.1   | 7.4 MB    | #####8     |  59% \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  |            |   1% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   |            |   0% \u001b[A\n",
      "torchvision-0.13.1   | 7.4 MB    | ######1    |  62% \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | 1          |   1% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   |            |   0% \u001b[A\n",
      "torchvision-0.13.1   | 7.4 MB    | ######4    |  65% \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | 1          |   1% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   |            |   0% \u001b[A\n",
      "torchvision-0.13.1   | 7.4 MB    | ######8    |  69% \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | 1          |   1% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   |            |   0% \u001b[A\n",
      "torchvision-0.13.1   | 7.4 MB    | #######2   |  72% \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | 1          |   1% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   |            |   0% \u001b[A\n",
      "torchvision-0.13.1   | 7.4 MB    | #######6   |  76% \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | 1          |   1% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   |            |   0% \u001b[A\n",
      "torchvision-0.13.1   | 7.4 MB    | #######9   |  80% \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | 1          |   2% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   |            |   0% \u001b[A\n",
      "torchvision-0.13.1   | 7.4 MB    | ########2  |  82% \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | 1          |   2% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   |            |   0% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "torchaudio-0.12.1    | 3.7 MB    | ########## | 100% \u001b[A\u001b[A\u001b[A\u001b[A\n",
      "torchvision-0.13.1   | 7.4 MB    | ########5  |  85% \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | 1          |   2% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   |            |   0% \u001b[A\n",
      "torchvision-0.13.1   | 7.4 MB    | ########7  |  87% \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | 1          |   2% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   |            |   0% \u001b[A\n",
      "torchvision-0.13.1   | 7.4 MB    | ########9  |  89% \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | 1          |   2% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   |            |   1% \u001b[A\n",
      "torchvision-0.13.1   | 7.4 MB    | #########1 |  92% \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | 1          |   2% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   |            |   1% \u001b[A\n",
      "torchvision-0.13.1   | 7.4 MB    | #########4 |  94% \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | 2          |   2% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   |            |   1% \u001b[A\n",
      "torchvision-0.13.1   | 7.4 MB    | #########7 |  97% \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | 2          |   2% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   |            |   1% \u001b[A\n",
      "torchvision-0.13.1   | 7.4 MB    | #########9 | 100% \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | 2          |   2% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   |            |   1% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | 2          |   2% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   |            |   1% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | 2          |   2% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   |            |   1% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   |            |   1% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | 2          |   3% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   |            |   1% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | 2          |   3% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | 2          |   3% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   |            |   1% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   |            |   1% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | 3          |   3% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | 3          |   3% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   |            |   1% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | 3          |   3% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   |            |   1% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   |            |   1% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | 3          |   4% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   |            |   1% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | 3          |   4% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | 3          |   4% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "torchvision-0.13.1   | 7.4 MB    | ########## | 100% \n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   |            |   1% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | 4          |   4% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | 1          |   1% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | 4          |   4% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | 1          |   1% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | 4          |   5% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | 1          |   1% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | 4          |   5% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | 1          |   1% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | 1          |   1% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | 5          |   5% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | 5          |   5% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | 1          |   1% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | 1          |   1% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | 5          |   5% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | 1          |   1% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | 5          |   6% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | 1          |   1% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | 5          |   6% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | 1          |   1% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | 5          |   6% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | 1          |   1% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | 6          |   6% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | 1          |   1% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | 1          |   1% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | 6          |   6% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | 1          |   2% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | 6          |   6% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | 6          |   7% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | 1          |   2% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | 1          |   2% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | 6          |   7% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | 1          |   2% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | 6          |   7% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | 1          |   2% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | 6          |   7% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | 1          |   2% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | 7          |   7% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | 1          |   2% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | 7          |   7% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | 1          |   2% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | 7          |   7% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | 1          |   2% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | 7          |   8% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | 1          |   2% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | 7          |   8% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | 1          |   2% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | 7          |   8% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | 2          |   2% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | 8          |   8% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | 2          |   2% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | 8          |   8% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | 2          |   2% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | 8          |   8% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | 2          |   2% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | 8          |   8% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | 2          |   2% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | 8          |   9% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | 2          |   2% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | 8          |   9% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | 2          |   2% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | 8          |   9% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | 2          |   2% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | 8          |   9% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | 2          |   2% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | 9          |   9% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | 2          |   2% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | 9          |   9% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | 2          |   2% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | 2          |   3% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | 9          |   9% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | 9          |   9% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | 2          |   3% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | 9          |  10% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | 2          |   3% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | 9          |  10% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | 2          |   3% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | 9          |  10% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | 2          |   3% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | 9          |  10% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | 2          |   3% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | 9          |  10% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | 2          |   3% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | 9          |  10% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | 2          |   3% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | #          |  10% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | 2          |   3% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | #          |  10% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | 2          |   3% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | #          |  10% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | #          |  10% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | 2          |   3% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | #          |  11% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | 2          |   3% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | #          |  11% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | 2          |   3% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | #          |  11% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | 2          |   3% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | #          |  11% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | 3          |   3% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | #          |  11% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | 3          |   3% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | #1         |  11% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | 3          |   3% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | #1         |  11% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | #1         |  11% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | #1         |  11% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | 3          |   3% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | #1         |  12% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | 3          |   3% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | #1         |  12% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | 3          |   3% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | #1         |  12% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | 3          |   3% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | 3          |   3% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | #1         |  12% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | 3          |   3% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | #2         |  12% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | #2         |  12% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | 3          |   3% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | #2         |  12% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | 3          |   3% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | #2         |  12% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | 3          |   3% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | #2         |  13% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | 3          |   3% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | 3          |   4% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | #2         |  13% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | #3         |  13% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | 3          |   4% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | #3         |  13% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | 3          |   4% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | #3         |  13% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | 3          |   4% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | #3         |  14% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | 3          |   4% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | #3         |  14% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | 3          |   4% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | #3         |  14% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | 3          |   4% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | 3          |   4% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | #4         |  14% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | 3          |   4% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | #4         |  14% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | #4         |  14% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | 3          |   4% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | #4         |  14% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | 3          |   4% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | 3          |   4% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | #4         |  15% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | 3          |   4% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | #4         |  15% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | 4          |   4% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | #4         |  15% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | 4          |   4% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | #5         |  15% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | 4          |   4% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | #5         |  15% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | 4          |   4% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | #5         |  15% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | 4          |   4% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | #5         |  15% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | 4          |   4% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | #5         |  16% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | 4          |   4% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | 4          |   4% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | #5         |  16% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | 4          |   4% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | #6         |  16% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | 4          |   4% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | #6         |  16% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | 4          |   4% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | #6         |  16% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | 4          |   4% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | #6         |  16% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | 4          |   4% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | #6         |  17% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | 4          |   5% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | #6         |  17% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | 4          |   5% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | #6         |  17% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | 4          |   5% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | 4          |   5% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | 4          |   5% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | #6         |  17% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | 4          |   5% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | #7         |  17% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | 4          |   5% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | #7         |  17% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | 4          |   5% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | #7         |  17% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | 4          |   5% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | #7         |  17% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | 4          |   5% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | #7         |  18% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | 4          |   5% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | #7         |  18% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | 5          |   5% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | #7         |  18% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | 5          |   5% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | #7         |  18% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | 5          |   5% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | #8         |  18% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | 5          |   5% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | #8         |  18% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | 5          |   5% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | #8         |  18% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | 5          |   5% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | #8         |  18% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | 5          |   5% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | #8         |  19% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | #8         |  19% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | 5          |   5% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | #8         |  19% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | 5          |   5% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | #9         |  19% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | 5          |   5% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | #9         |  19% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | 5          |   5% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | #9         |  19% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | #9         |  19% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | 5          |   6% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | #9         |  19% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | 5          |   6% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | 5          |   6% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | #9         |  20% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | 5          |   6% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | #9         |  20% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | 5          |   6% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | #9         |  20% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | 5          |   6% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | #9         |  20% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ##         |  20% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | 5          |   6% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ##         |  20% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | 5          |   6% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ##         |  20% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | 5          |   6% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ##         |  20% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | 5          |   6% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ##         |  21% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | 6          |   6% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ##         |  21% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | 6          |   6% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ##1        |  21% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | 6          |   6% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ##1        |  21% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | 6          |   6% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ##1        |  21% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | 6          |   6% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ##1        |  22% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ##1        |  22% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | 6          |   6% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ##1        |  22% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | 6          |   6% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | 6          |   6% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ##2        |  22% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | 6          |   7% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ##2        |  22% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | 6          |   7% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ##2        |  23% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | 6          |   7% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ##2        |  23% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | 6          |   7% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ##2        |  23% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | 6          |   7% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ##3        |  23% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | 6          |   7% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ##3        |  23% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | 6          |   7% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ##3        |  23% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | 6          |   7% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ##3        |  24% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | 7          |   7% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | 7          |   7% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | 7          |   7% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ##3        |  24% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ##4        |  24% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | 7          |   7% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | 7          |   7% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ##4        |  24% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | 7          |   7% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ##4        |  25% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | 7          |   7% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ##4        |  25% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | 7          |   7% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ##5        |  25% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | 7          |   8% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ##5        |  25% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | 7          |   8% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ##5        |  25% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | 7          |   8% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ##5        |  26% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | 7          |   8% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ##5        |  26% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | 7          |   8% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ##5        |  26% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | 7          |   8% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ##5        |  26% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ##6        |  26% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ##6        |  26% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | 7          |   8% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | 7          |   8% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | 7          |   8% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ##6        |  26% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ##6        |  27% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | 8          |   8% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ##6        |  27% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ##6        |  27% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | 8          |   8% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | 8          |   8% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ##7        |  27% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | 8          |   8% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ##7        |  27% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | 8          |   8% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ##7        |  27% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | 8          |   8% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ##7        |  28% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | 8          |   8% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "  current version: 23.3.1\n",
      "  latest version: 23.10.0\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base -c defaults conda\n",
      "\n",
      "Or to minimize the number of packages updated during conda update use\n",
      "\n",
      "     conda install conda=23.10.0\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ##7        |  28% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | 8          |   8% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ##7        |  28% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | 8          |   8% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ##8        |  28% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | 8          |   8% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ##8        |  28% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | 8          |   8% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ##8        |  28% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | 8          |   9% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ##8        |  29% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | 8          |   9% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ##8        |  29% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ##8        |  29% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | 8          |   9% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | 8          |   9% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | 8          |   9% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ##9        |  29% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | 8          |   9% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ##9        |  29% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | 8          |   9% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ##9        |  30% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ##9        |  30% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ##9        |  30% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | 8          |   9% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ##9        |  30% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | 8          |   9% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ###        |  30% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | 8          |   9% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ###        |  30% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | 8          |   9% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ###        |  30% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | 9          |   9% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ###        |  30% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | 9          |   9% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ###        |  31% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | 9          |   9% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ###        |  31% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | 9          |   9% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ###        |  31% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | 9          |   9% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ###        |  31% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | 9          |   9% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ###1       |  31% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | 9          |   9% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ###1       |  31% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ###1       |  31% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | 9          |   9% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ###1       |  31% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | 9          |   9% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ###1       |  32% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | 9          |   9% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ###1       |  32% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | 9          |   9% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ###1       |  32% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | 9          |  10% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ###1       |  32% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | 9          |  10% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | 9          |  10% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ###2       |  32% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | 9          |  10% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ###2       |  32% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | 9          |  10% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ###2       |  32% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | 9          |  10% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ###2       |  32% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | 9          |  10% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ###2       |  32% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | 9          |  10% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ###2       |  32% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | 9          |  10% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ###2       |  33% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | 9          |  10% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ###2       |  33% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | 9          |  10% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ###2       |  33% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | 9          |  10% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ###2       |  33% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | 9          |  10% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #          |  10% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #          |  10% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ###2       |  33% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #          |  10% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ###2       |  33% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #          |  10% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ###3       |  33% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #          |  10% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ###3       |  33% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #          |  10% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ###3       |  33% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #          |  10% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ###3       |  33% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #          |  10% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ###3       |  33% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #          |  11% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ###3       |  34% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #          |  11% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ###3       |  34% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #          |  11% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #          |  11% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ###3       |  34% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ###3       |  34% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #          |  11% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #          |  11% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ###3       |  34% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #          |  11% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ###4       |  34% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #          |  11% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ###4       |  34% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #          |  11% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ###4       |  34% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #1         |  11% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ###4       |  34% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ###4       |  35% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #1         |  11% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ###4       |  35% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #1         |  11% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ###4       |  35% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #1         |  11% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ###4       |  35% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #1         |  11% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ###5       |  35% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #1         |  11% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ###5       |  35% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #1         |  12% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ###5       |  35% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #1         |  12% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #1         |  12% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ###5       |  35% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #1         |  12% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ###5       |  36% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #1         |  12% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ###5       |  36% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #1         |  12% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ###5       |  36% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #1         |  12% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ###5       |  36% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #1         |  12% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ###6       |  36% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #2         |  12% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ###6       |  36% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #2         |  12% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ###6       |  36% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #2         |  12% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ###6       |  36% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #2         |  12% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ###6       |  36% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #2         |  12% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ###6       |  37% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #2         |  12% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ###6       |  37% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #2         |  12% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ###6       |  37% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #2         |  12% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ###6       |  37% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #2         |  13% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ###7       |  37% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #2         |  13% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ###7       |  37% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #2         |  13% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ###7       |  37% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #2         |  13% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ###7       |  37% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #2         |  13% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #2         |  13% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ###7       |  37% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ###7       |  38% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #2         |  13% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #3         |  13% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ###7       |  38% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #3         |  13% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ###7       |  38% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #3         |  13% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ###8       |  38% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #3         |  13% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ###8       |  38% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #3         |  13% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ###8       |  38% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #3         |  13% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ###8       |  38% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #3         |  13% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ###8       |  39% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ###8       |  39% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #3         |  13% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ###8       |  39% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #3         |  14% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ###8       |  39% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #3         |  14% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ###9       |  39% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #3         |  14% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ###9       |  39% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #3         |  14% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ###9       |  39% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #3         |  14% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ###9       |  39% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #3         |  14% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ###9       |  40% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ###9       |  40% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #3         |  14% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ###9       |  40% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #4         |  14% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ###9       |  40% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #4         |  14% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ###9       |  40% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #4         |  14% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #4         |  14% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ###9       |  40% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ####       |  40% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #4         |  14% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ####       |  40% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #4         |  14% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ####       |  40% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #4         |  14% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ####       |  40% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #4         |  14% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ####       |  40% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #4         |  15% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ####       |  41% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #4         |  15% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ####       |  41% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #4         |  15% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #4         |  15% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ####       |  41% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #4         |  15% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ####1      |  41% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #4         |  15% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #4         |  15% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ####1      |  41% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #4         |  15% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ####1      |  41% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #4         |  15% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ####1      |  41% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #4         |  15% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ####1      |  41% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #5         |  15% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ####1      |  42% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #5         |  15% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ####1      |  42% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #5         |  15% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ####1      |  42% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #5         |  15% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ####1      |  42% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #5         |  15% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ####2      |  42% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ####2      |  42% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #5         |  15% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ####2      |  42% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ####2      |  43% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #5         |  15% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ####2      |  43% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #5         |  15% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ####2      |  43% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #5         |  15% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ####2      |  43% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #5         |  16% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ####3      |  43% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #5         |  16% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ####3      |  43% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #5         |  16% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ####3      |  43% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #5         |  16% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ####3      |  43% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #5         |  16% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ####3      |  44% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #5         |  16% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ####3      |  44% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #5         |  16% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ####3      |  44% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #5         |  16% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ####3      |  44% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ####4      |  44% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #5         |  16% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ####4      |  44% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #6         |  16% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ####4      |  44% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #6         |  16% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ####4      |  44% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #6         |  16% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ####4      |  45% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #6         |  16% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ####4      |  45% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #6         |  16% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ####4      |  45% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #6         |  16% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ####5      |  45% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #6         |  16% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ####5      |  45% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #6         |  16% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ####5      |  45% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #6         |  17% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ####5      |  46% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #6         |  17% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ####5      |  46% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #6         |  17% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ####5      |  46% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #6         |  17% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ####5      |  46% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #6         |  17% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ####6      |  46% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #6         |  17% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ####6      |  46% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #6         |  17% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ####6      |  46% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #6         |  17% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ####6      |  47% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #7         |  17% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ####6      |  47% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #7         |  17% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ####6      |  47% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #7         |  17% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ####7      |  47% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #7         |  17% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ####7      |  47% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #7         |  17% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ####7      |  47% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #7         |  17% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ####7      |  48% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #7         |  17% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ####7      |  48% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #7         |  18% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ####7      |  48% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ####8      |  48% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #7         |  18% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ####8      |  48% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #7         |  18% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ####8      |  48% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #7         |  18% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ####8      |  49% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #7         |  18% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ####8      |  49% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #7         |  18% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ####8      |  49% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #8         |  18% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #8         |  18% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ####8      |  49% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #8         |  18% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ####9      |  49% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #8         |  18% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ####9      |  49% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #8         |  18% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #8         |  18% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ####9      |  50% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #8         |  18% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ####9      |  50% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #8         |  18% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | #####      |  50% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #8         |  19% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | #####      |  50% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #8         |  19% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | #####      |  50% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #8         |  19% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | #####      |  51% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #8         |  19% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | #####      |  51% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | #####      |  51% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #8         |  19% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #8         |  19% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | #####1     |  51% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | #####1     |  51% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #8         |  19% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | #####1     |  51% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #9         |  19% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #9         |  19% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | #####1     |  52% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #9         |  19% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | #####1     |  52% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #9         |  19% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | #####1     |  52% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #9         |  19% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | #####2     |  52% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #9         |  19% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | #####2     |  52% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #9         |  19% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | #####2     |  53% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #9         |  19% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | #####2     |  53% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #9         |  20% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | #####2     |  53% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #9         |  20% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | #####2     |  53% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #9         |  20% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | #####3     |  53% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #9         |  20% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | #####3     |  53% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #9         |  20% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | #####3     |  53% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #9         |  20% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | #####3     |  54% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #9         |  20% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | #####3     |  54% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #9         |  20% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | #####3     |  54% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ##         |  20% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | #####3     |  54% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ##         |  20% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | #####4     |  54% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | #####4     |  54% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ##         |  20% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | #####4     |  54% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ##         |  20% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | #####4     |  55% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ##         |  20% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | #####4     |  55% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ##         |  20% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | #####4     |  55% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ##         |  20% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | #####4     |  55% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ##         |  21% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | #####5     |  55% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | #####5     |  55% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ##         |  21% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ##         |  21% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ##         |  21% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ##         |  21% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | #####5     |  55% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ##         |  21% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | #####5     |  55% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ##         |  21% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | #####5     |  56% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ##         |  21% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | #####5     |  56% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ##         |  21% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | #####5     |  56% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ##1        |  21% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | #####6     |  56% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ##1        |  21% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | #####6     |  56% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ##1        |  21% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | #####6     |  56% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ##1        |  21% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | #####6     |  57% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ##1        |  21% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | #####6     |  57% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ##1        |  21% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | #####6     |  57% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ##1        |  21% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ##1        |  22% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | #####7     |  57% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ##1        |  22% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | #####7     |  57% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ##1        |  22% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | #####7     |  58% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ##1        |  22% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | #####7     |  58% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ##1        |  22% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | #####7     |  58% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ##1        |  22% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | #####8     |  58% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ##1        |  22% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | #####8     |  58% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ##1        |  22% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | #####8     |  58% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ##2        |  22% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | #####8     |  59% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ##2        |  22% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | #####8     |  59% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ##2        |  22% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ##2        |  22% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | #####8     |  59% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ##2        |  22% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | #####9     |  59% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | #####9     |  60% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ##2        |  22% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ##2        |  23% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | #####9     |  60% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | #####9     |  60% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ##2        |  23% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ######     |  60% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ##2        |  23% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ######     |  60% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ##2        |  23% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ######     |  60% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ##2        |  23% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ######     |  61% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ##2        |  23% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ######     |  61% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ##2        |  23% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ######     |  61% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ##3        |  23% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ######1    |  61% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ##3        |  23% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ######1    |  61% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ##3        |  23% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ######1    |  61% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ##3        |  23% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ######1    |  61% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ##3        |  23% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ######1    |  62% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ##3        |  23% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ######1    |  62% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ##3        |  23% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ######1    |  62% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ##3        |  23% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ######2    |  62% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ##3        |  24% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ######2    |  62% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ######2    |  62% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ##3        |  24% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ######2    |  62% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ##3        |  24% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ######2    |  63% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ##3        |  24% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ######2    |  63% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ######2    |  63% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ##3        |  24% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ######2    |  63% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ##3        |  24% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ##4        |  24% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ######3    |  63% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ######3    |  63% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ##4        |  24% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ######3    |  63% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ##4        |  24% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ######3    |  64% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ##4        |  24% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ######3    |  64% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ##4        |  24% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ######3    |  64% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ##4        |  24% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ######3    |  64% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ##4        |  24% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ######4    |  64% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ##4        |  24% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ######4    |  64% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ##4        |  25% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ######4    |  64% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ##4        |  25% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ######4    |  64% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ##4        |  25% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ######4    |  65% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ##4        |  25% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ######4    |  65% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ##4        |  25% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ######5    |  65% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ##4        |  25% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ##4        |  25% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ######5    |  65% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ##5        |  25% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ######5    |  65% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ##5        |  25% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ######5    |  66% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ##5        |  25% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ######5    |  66% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ##5        |  25% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ######5    |  66% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ##5        |  25% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ######5    |  66% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ##5        |  25% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ######6    |  66% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ######6    |  66% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ##5        |  25% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ######6    |  66% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ##5        |  26% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ######6    |  66% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ##5        |  26% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ######6    |  67% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ##5        |  26% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ##5        |  26% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ######6    |  67% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ######6    |  67% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ##5        |  26% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ##5        |  26% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ######6    |  67% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ######7    |  67% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ##5        |  26% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ######7    |  67% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ##5        |  26% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ######7    |  67% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ######7    |  68% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ######7    |  68% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ##6        |  26% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ######7    |  68% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ##6        |  26% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ######8    |  68% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ##6        |  26% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ######8    |  68% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ##6        |  26% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ##6        |  26% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ######8    |  68% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ##6        |  26% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ######8    |  69% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ##6        |  26% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ######8    |  69% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ##6        |  27% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ######9    |  69% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ######9    |  69% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ##6        |  27% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ######9    |  69% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ##6        |  27% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ######9    |  70% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ##6        |  27% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ##6        |  27% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ######9    |  70% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ##7        |  27% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | #######    |  70% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ##7        |  27% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | #######    |  70% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ##7        |  27% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | #######    |  70% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ##7        |  27% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | #######    |  71% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ##7        |  27% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | #######    |  71% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ##7        |  27% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | #######1   |  71% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ##7        |  28% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | #######1   |  71% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ##7        |  28% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | #######1   |  72% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ##7        |  28% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | #######1   |  72% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | #######1   |  72% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ##7        |  28% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ##7        |  28% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | #######2   |  72% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | #######2   |  72% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ##8        |  28% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | #######2   |  72% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ##8        |  28% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | #######2   |  72% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ##8        |  28% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | #######2   |  73% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ##8        |  28% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ##8        |  28% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | #######2   |  73% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ##8        |  28% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | #######2   |  73% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ##8        |  28% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | #######2   |  73% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | #######3   |  73% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ##8        |  28% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | #######3   |  73% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ##8        |  29% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | #######3   |  73% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ##8        |  29% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ##8        |  29% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | #######3   |  74% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ##8        |  29% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | #######3   |  74% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ##8        |  29% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | #######3   |  74% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ##8        |  29% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | #######3   |  74% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ##8        |  29% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | #######4   |  74% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ##8        |  29% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | #######4   |  74% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ##9        |  29% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ##9        |  29% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | #######4   |  74% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | #######4   |  74% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ##9        |  29% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ##9        |  29% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | #######4   |  75% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ##9        |  29% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | #######4   |  75% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | #######4   |  75% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ##9        |  29% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | #######4   |  75% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ##9        |  29% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | #######5   |  75% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ##9        |  29% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ##9        |  30% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | #######5   |  75% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ##9        |  30% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | #######5   |  75% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ##9        |  30% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | #######5   |  76% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | #######5   |  76% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ##9        |  30% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | #######5   |  76% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ##9        |  30% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ##9        |  30% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | #######5   |  76% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ###        |  30% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | #######6   |  76% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ###        |  30% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | #######6   |  76% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ###        |  30% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | #######6   |  76% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ###        |  30% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | #######6   |  76% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ###        |  30% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | #######6   |  77% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ###        |  30% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | #######6   |  77% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ###        |  30% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | #######6   |  77% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ###        |  31% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | #######7   |  77% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ###        |  31% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | #######7   |  77% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ###        |  31% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | #######7   |  78% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ###        |  31% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | #######7   |  78% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ###        |  31% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | #######7   |  78% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ###        |  31% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | #######8   |  78% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ###        |  31% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | #######8   |  78% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ###1       |  31% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | #######8   |  78% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ###1       |  31% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | #######8   |  79% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ###1       |  31% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | #######8   |  79% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ###1       |  31% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ###1       |  31% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | #######9   |  79% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ###1       |  32% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | #######9   |  79% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ###1       |  32% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ###1       |  32% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | #######9   |  79% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ###1       |  32% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | #######9   |  80% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ###1       |  32% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ###1       |  32% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ###2       |  32% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | #######9   |  80% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ###2       |  32% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ###2       |  32% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | #######9   |  80% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ########   |  80% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ###2       |  32% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ########   |  80% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ###2       |  32% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ########   |  81% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ###2       |  33% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ########   |  81% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ########   |  81% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ###2       |  33% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ########   |  81% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ###2       |  33% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ########1  |  81% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ###2       |  33% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ########1  |  81% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ###2       |  33% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ########1  |  81% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ###2       |  33% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ########1  |  81% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ###3       |  33% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ########1  |  82% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ########1  |  82% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ###3       |  33% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ########1  |  82% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ###3       |  33% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ########1  |  82% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ###3       |  33% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ########2  |  82% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ###3       |  33% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ########2  |  82% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ###3       |  33% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ########2  |  82% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ###3       |  33% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ########2  |  82% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ###3       |  34% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ########2  |  83% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ###3       |  34% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ########2  |  83% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ###3       |  34% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ########2  |  83% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ###3       |  34% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ########3  |  83% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ########3  |  83% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ###3       |  34% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ########3  |  83% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ###3       |  34% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ########3  |  83% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ###4       |  34% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ########3  |  84% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ###4       |  34% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ########3  |  84% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ###4       |  34% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ########3  |  84% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ###4       |  34% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ###4       |  34% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ########3  |  84% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ###4       |  34% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ########4  |  84% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ########4  |  84% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ###4       |  34% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ###4       |  35% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ########4  |  85% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ###4       |  35% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ########4  |  85% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ###4       |  35% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ########4  |  85% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ###4       |  35% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ########4  |  85% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ###4       |  35% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ########5  |  85% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ###4       |  35% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ########5  |  85% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ###4       |  35% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ########5  |  85% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ###4       |  35% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ########5  |  85% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ###5       |  35% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ########5  |  86% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ###5       |  35% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ########5  |  86% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ###5       |  35% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ########5  |  86% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ########6  |  86% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ###5       |  35% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ########6  |  86% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ###5       |  36% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ########6  |  87% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ###5       |  36% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ########6  |  87% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ###5       |  36% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ########6  |  87% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ###5       |  36% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ########7  |  87% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ###5       |  36% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ########7  |  87% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ###5       |  36% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ########7  |  87% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ###6       |  36% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ########7  |  87% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ###6       |  36% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ########7  |  88% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ###6       |  36% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ########7  |  88% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ###6       |  36% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ########8  |  88% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ###6       |  36% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ########8  |  88% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ###6       |  37% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ########8  |  88% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ###6       |  37% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ########8  |  89% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ###6       |  37% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ###6       |  37% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ########8  |  89% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ###6       |  37% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ########8  |  89% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ###6       |  37% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ########9  |  89% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ###7       |  37% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ########9  |  89% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ########9  |  89% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ###7       |  37% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ###7       |  37% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ########9  |  90% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ########9  |  90% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ###7       |  37% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ########9  |  90% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ###7       |  37% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | #########  |  90% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ###7       |  38% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | #########  |  90% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ###7       |  38% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | #########  |  90% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ###7       |  38% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ###7       |  38% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | #########  |  91% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | #########  |  91% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ###7       |  38% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | #########1 |  91% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ###8       |  38% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | #########1 |  91% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ###8       |  38% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | #########1 |  91% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ###8       |  38% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | #########1 |  92% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ###8       |  38% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ###8       |  38% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | #########1 |  92% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ###8       |  38% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | #########1 |  92% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ###8       |  39% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | #########2 |  92% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ###8       |  39% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | #########2 |  92% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ###8       |  39% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | #########2 |  92% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | #########2 |  92% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | #########2 |  93% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ###8       |  39% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | #########2 |  93% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ###8       |  39% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | #########3 |  93% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ###8       |  39% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | #########3 |  93% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ###9       |  39% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | #########3 |  93% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ###9       |  39% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | #########3 |  94% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ###9       |  39% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | #########3 |  94% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ###9       |  39% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | #########3 |  94% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ###9       |  39% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | #########4 |  94% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ###9       |  39% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | #########4 |  94% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ###9       |  40% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | #########4 |  94% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ###9       |  40% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | #########4 |  95% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ###9       |  40% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | #########4 |  95% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | #########4 |  95% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ###9       |  40% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | #########5 |  95% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ###9       |  40% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | #########5 |  95% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ####       |  40% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | #########5 |  95% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ####       |  40% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | #########5 |  96% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ####       |  40% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | #########5 |  96% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ####       |  40% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | #########5 |  96% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ####       |  40% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | #########6 |  96% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ####       |  40% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | #########6 |  96% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ####       |  41% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | #########6 |  96% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ####       |  41% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | #########6 |  96% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ####       |  41% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | #########6 |  97% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | #########6 |  97% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ####       |  41% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | #########6 |  97% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ####       |  41% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | #########7 |  97% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ####1      |  41% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | #########7 |  97% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ####1      |  41% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | #########7 |  97% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ####1      |  41% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | #########7 |  97% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ####1      |  41% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | #########7 |  98% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | #########7 |  98% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | #########7 |  98% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ####1      |  41% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | #########8 |  98% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ####1      |  42% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | #########8 |  98% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ####1      |  42% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | #########8 |  98% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ####1      |  42% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | #########8 |  99% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ####1      |  42% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | #########8 |  99% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ####1      |  42% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | #########8 |  99% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ####1      |  42% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | #########8 |  99% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ####2      |  42% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | #########9 |  99% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ####2      |  42% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | #########9 |  99% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ####2      |  42% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | #########9 | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ####2      |  42% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | #########9 | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ####2      |  42% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | #########9 | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ####2      |  42% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | #########9 | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ####2      |  43% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ####2      |  43% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ####2      |  43% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ####2      |  43% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ####2      |  43% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ####3      |  43% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ####3      |  43% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ####3      |  43% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ####3      |  43% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ####3      |  44% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ####3      |  44% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ####3      |  44% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ####3      |  44% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ####4      |  44% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ####4      |  44% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ####4      |  44% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ####4      |  44% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ####4      |  44% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ####4      |  45% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ####4      |  45% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ####4      |  45% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ####4      |  45% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ####5      |  45% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ####5      |  45% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ####5      |  45% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ####5      |  46% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ####5      |  46% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ####5      |  46% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ####5      |  46% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ####5      |  46% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ####6      |  46% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ####6      |  46% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ####6      |  46% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ####6      |  47% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ####6      |  47% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ####6      |  47% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ####6      |  47% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ####7      |  47% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ####7      |  47% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ####7      |  47% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ####7      |  47% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ####7      |  47% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ####7      |  48% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ####7      |  48% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ####8      |  48% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cudatoolkit-11.3.1   | 545.3 MB  | ########## | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ####8      |  48% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ####8      |  48% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ####8      |  49% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ####8      |  49% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ####8      |  49% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ####8      |  49% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ####9      |  49% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ####9      |  49% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ####9      |  50% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ####9      |  50% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ####9      |  50% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #####      |  50% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #####      |  50% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #####      |  50% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #####      |  50% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #####      |  51% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #####      |  51% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #####      |  51% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #####      |  51% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #####1     |  51% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #####1     |  51% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #####1     |  51% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #####1     |  52% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #####1     |  52% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #####1     |  52% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #####2     |  52% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #####2     |  52% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #####2     |  52% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #####2     |  52% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #####2     |  53% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #####2     |  53% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #####2     |  53% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #####3     |  53% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #####3     |  53% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #####3     |  53% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #####3     |  53% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #####3     |  54% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #####3     |  54% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #####3     |  54% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #####3     |  54% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #####4     |  54% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #####4     |  54% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #####4     |  54% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #####4     |  54% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #####4     |  55% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #####4     |  55% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #####4     |  55% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #####4     |  55% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #####4     |  55% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #####5     |  55% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #####5     |  55% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #####5     |  55% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #####5     |  55% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #####5     |  56% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #####5     |  56% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #####5     |  56% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #####6     |  56% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #####6     |  56% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #####6     |  56% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #####6     |  56% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #####6     |  57% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #####6     |  57% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #####6     |  57% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #####7     |  57% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #####7     |  57% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #####7     |  57% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #####7     |  58% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #####7     |  58% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #####7     |  58% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #####8     |  58% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #####8     |  58% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #####8     |  58% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #####8     |  58% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #####8     |  59% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #####8     |  59% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #####8     |  59% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #####8     |  59% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #####9     |  59% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #####9     |  59% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #####9     |  60% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #####9     |  60% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #####9     |  60% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ######     |  60% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ######     |  60% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ######     |  60% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ######     |  61% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ######     |  61% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ######     |  61% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ######1    |  61% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ######1    |  61% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ######1    |  62% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ######1    |  62% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ######1    |  62% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ######2    |  62% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ######2    |  62% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ######2    |  63% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ######2    |  63% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ######2    |  63% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ######3    |  63% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ######3    |  63% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ######3    |  64% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ######3    |  64% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ######4    |  64% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ######4    |  64% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ######4    |  64% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ######4    |  65% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ######4    |  65% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ######4    |  65% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ######5    |  65% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ######5    |  65% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ######5    |  65% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ######5    |  66% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ######5    |  66% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ######5    |  66% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ######6    |  66% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ######6    |  66% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ######6    |  66% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ######6    |  67% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ######6    |  67% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ######7    |  67% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ######7    |  67% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ######7    |  68% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ######7    |  68% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ######8    |  68% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ######8    |  68% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ######8    |  69% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ######8    |  69% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ######8    |  69% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ######8    |  69% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ######9    |  69% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ######9    |  69% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ######9    |  69% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ######9    |  69% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ######9    |  70% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ######9    |  70% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ######9    |  70% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ######9    |  70% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #######    |  70% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #######    |  70% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #######    |  70% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #######    |  70% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #######    |  71% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #######    |  71% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #######    |  71% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #######    |  71% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #######1   |  71% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #######1   |  71% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #######1   |  71% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #######1   |  71% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #######1   |  72% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #######1   |  72% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #######1   |  72% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #######1   |  72% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #######2   |  72% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #######2   |  72% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #######2   |  72% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #######2   |  73% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #######2   |  73% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #######2   |  73% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #######2   |  73% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #######2   |  73% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #######3   |  73% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #######3   |  73% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #######3   |  73% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #######3   |  73% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #######3   |  74% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #######3   |  74% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #######3   |  74% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #######3   |  74% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #######4   |  74% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #######4   |  74% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #######4   |  74% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #######4   |  74% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #######4   |  74% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #######4   |  74% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #######4   |  75% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #######4   |  75% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #######4   |  75% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #######4   |  75% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #######5   |  75% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #######5   |  75% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #######5   |  75% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #######5   |  75% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #######5   |  75% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #######5   |  76% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #######5   |  76% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #######5   |  76% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #######5   |  76% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #######5   |  76% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #######6   |  76% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #######6   |  76% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #######6   |  76% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #######6   |  76% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #######6   |  76% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #######6   |  77% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #######6   |  77% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #######6   |  77% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #######6   |  77% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #######7   |  77% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #######7   |  77% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #######7   |  77% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #######7   |  77% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #######7   |  77% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #######7   |  77% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #######7   |  78% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #######7   |  78% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #######7   |  78% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #######7   |  78% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #######7   |  78% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #######8   |  78% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #######8   |  78% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #######8   |  78% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #######8   |  78% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #######8   |  79% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #######8   |  79% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #######8   |  79% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #######8   |  79% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #######8   |  79% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #######9   |  79% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #######9   |  79% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #######9   |  79% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #######9   |  80% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #######9   |  80% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #######9   |  80% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #######9   |  80% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ########   |  80% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ########   |  80% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ########   |  80% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ########   |  80% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ########   |  81% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ########   |  81% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ########   |  81% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ########   |  81% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ########   |  81% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ########1  |  81% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ########1  |  81% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ########1  |  81% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ########1  |  81% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ########1  |  81% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ########1  |  81% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ########1  |  81% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ########1  |  82% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ########1  |  82% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ########1  |  82% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ########1  |  82% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ########2  |  82% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ########2  |  82% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ########2  |  82% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ########2  |  82% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ########2  |  83% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ########2  |  83% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ########2  |  83% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ########2  |  83% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ########3  |  83% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ########3  |  83% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ########3  |  83% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ########3  |  83% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ########3  |  84% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ########3  |  84% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ########3  |  84% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ########3  |  84% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ########4  |  84% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ########4  |  84% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ########4  |  84% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ########4  |  84% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ########4  |  84% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ########4  |  85% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ########4  |  85% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ########4  |  85% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ########4  |  85% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ########4  |  85% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ########5  |  85% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ########5  |  85% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ########5  |  85% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ########5  |  85% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ########5  |  85% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ########5  |  85% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ########5  |  86% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ########5  |  86% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ########5  |  86% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ########5  |  86% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ########6  |  86% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ########6  |  86% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ########6  |  86% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ########6  |  86% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ########6  |  86% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ########6  |  87% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ########6  |  87% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ########6  |  87% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ########6  |  87% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ########6  |  87% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ########7  |  87% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ########7  |  87% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ########7  |  87% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ########7  |  87% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ########7  |  88% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ########7  |  88% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ########7  |  88% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ########7  |  88% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ########8  |  88% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ########8  |  88% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ########8  |  88% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ########8  |  88% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ########8  |  89% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ########8  |  89% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ########8  |  89% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ########8  |  89% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ########9  |  89% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ########9  |  89% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ########9  |  89% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ########9  |  89% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ########9  |  90% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ########9  |  90% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ########9  |  90% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ########9  |  90% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #########  |  90% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #########  |  90% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #########  |  90% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #########  |  90% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #########  |  90% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #########  |  91% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #########  |  91% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #########  |  91% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #########  |  91% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #########1 |  91% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #########1 |  91% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #########1 |  91% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #########1 |  91% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #########1 |  92% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #########1 |  92% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #########1 |  92% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #########1 |  92% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #########2 |  92% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #########2 |  92% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #########2 |  92% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #########2 |  92% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #########2 |  93% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #########2 |  93% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #########2 |  93% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #########3 |  93% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #########3 |  93% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #########3 |  93% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #########3 |  94% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #########3 |  94% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #########3 |  94% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #########3 |  94% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #########4 |  94% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #########4 |  94% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #########4 |  94% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #########4 |  94% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #########4 |  94% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #########4 |  95% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #########4 |  95% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #########4 |  95% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #########4 |  95% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #########5 |  95% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #########5 |  95% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #########5 |  95% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #########5 |  95% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #########5 |  95% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #########5 |  96% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #########5 |  96% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #########5 |  96% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #########6 |  96% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #########6 |  96% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #########6 |  96% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #########6 |  96% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #########6 |  97% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #########6 |  97% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #########6 |  97% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #########6 |  97% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #########6 |  97% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #########7 |  97% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #########7 |  97% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #########7 |  97% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #########7 |  97% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #########7 |  98% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #########7 |  98% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #########7 |  98% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #########7 |  98% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #########7 |  98% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #########8 |  98% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #########8 |  98% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #########8 |  98% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #########8 |  98% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #########8 |  98% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #########8 |  99% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #########8 |  99% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #########9 |  99% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #########9 |  99% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #########9 |  99% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #########9 |  99% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #########9 |  99% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #########9 | 100% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #########9 | 100% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #########9 | 100% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | #########9 | 100% \u001b[A\n",
      "\n",
      "pytorch-1.12.1       | 1.20 GB   | ########## | 100% \u001b[A\n",
      "                                                     \n",
      "\n",
      "\n",
      "                                                     \u001b[A\n",
      "\n",
      "\n",
      "                                                     \u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "                                                     \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                                                     \u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                                                     \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                                                     \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "Preparing transaction: ...working... done\n",
      "Verifying transaction: ...working... done\n",
      "Executing transaction: ...working... done\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ccbfcc52",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.NLLLoss()\n",
    "images, labels = next(iter(trainloader))\n",
    "images = images.view(images.shape[0], -1)\n",
    "\n",
    "logps = sym_recog_model(images) #log probabilities\n",
    "loss = criterion(logps, labels) #calculate the NLL loss\n",
    "\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6e8a63ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Training loss: 1.064754771533409\n",
      "\n",
      "Training Time (in minutes) = 0.10856857299804687\n",
      "Epoch 2 - Training loss: 0.41980877589683746\n",
      "\n",
      "Training Time (in minutes) = 0.21256349086761475\n",
      "Epoch 3 - Training loss: 0.34051057814977476\n",
      "\n",
      "Training Time (in minutes) = 0.3473502556482951\n",
      "Epoch 4 - Training loss: 0.2900700738944461\n",
      "\n",
      "Training Time (in minutes) = 0.5027806560198466\n",
      "Epoch 5 - Training loss: 0.25073399742721425\n",
      "\n",
      "Training Time (in minutes) = 0.6341174006462097\n",
      "Epoch 6 - Training loss: 0.21730493573173426\n",
      "\n",
      "Training Time (in minutes) = 0.7676251610120137\n",
      "Epoch 7 - Training loss: 0.19161503037728717\n",
      "\n",
      "Training Time (in minutes) = 0.9023956735928853\n",
      "Epoch 8 - Training loss: 0.17093090766296545\n",
      "\n",
      "Training Time (in minutes) = 1.0228271762530008\n",
      "Epoch 9 - Training loss: 0.15296644705742538\n",
      "\n",
      "Training Time (in minutes) = 1.1468016227086386\n",
      "Epoch 10 - Training loss: 0.1376224057692241\n",
      "\n",
      "Training Time (in minutes) = 1.2646796425183615\n",
      "Epoch 11 - Training loss: 0.12435075149419916\n",
      "\n",
      "Training Time (in minutes) = 1.3845829725265504\n",
      "Epoch 12 - Training loss: 0.11384298354878447\n",
      "\n",
      "Training Time (in minutes) = 1.5056527455647786\n",
      "Epoch 13 - Training loss: 0.1042791070543139\n",
      "\n",
      "Training Time (in minutes) = 1.6239834507306417\n",
      "Epoch 14 - Training loss: 0.09588676185439705\n",
      "\n",
      "Training Time (in minutes) = 1.7412899692853292\n",
      "Epoch 15 - Training loss: 0.08863685693087446\n",
      "\n",
      "Training Time (in minutes) = 1.861879086494446\n"
     ]
    }
   ],
   "source": [
    "# ANN Training\n",
    "\n",
    "optimizer = optim.SGD(sym_recog_model.parameters(), lr=0.003, momentum=0.9)\n",
    "time0 = time()\n",
    "epochs = 15\n",
    "for e in range(epochs):\n",
    "    running_loss = 0\n",
    "    for images, labels in trainloader:\n",
    "        # Flatten MNIST images into a 784 long vector\n",
    "        images = images.view(images.shape[0], -1)\n",
    "    \n",
    "        # Training pass\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = sym_recog_model(images)\n",
    "        loss = criterion(output, labels)\n",
    "        \n",
    "        #This is where the model learns by backpropagating\n",
    "        loss.backward()\n",
    "        \n",
    "        #And optimizes its weights here\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    else:\n",
    "        print(\"Epoch {} - Training loss: {}\".format(e+1, running_loss/len(trainloader)))\n",
    "        print(\"\\nTraining Time (in minutes) =\",(time()-time0)/60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7264d8e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Digit = 1\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "images, labels = next(iter(valloader))\n",
    "\n",
    "img = images[0].view(1, 784)\n",
    "\n",
    "def predict_symbol(img):\n",
    "    with torch.no_grad():\n",
    "        logps = sym_recog_model(img)\n",
    "\n",
    "    ps = torch.exp(logps)\n",
    "    probab = list(ps.numpy()[0])\n",
    "    return probab.index(max(probab))\n",
    "\n",
    "# Check prediction\n",
    "print(\"Predicted Digit =\", predict_symbol(img))\n",
    "plt.imshow(img.reshape(28,28), cmap = 'gray')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
